{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from Vissim_env_class import environment\n",
    "from Actor_Critic_Class import ACAgent\n",
    "from MasterAC_Agent import MasterAC_Agent\n",
    "from MasterDQN_Agent import MasterDQN_Agent\n",
    "# Network Specific Libraries\n",
    "from Balance_Functions import balance_dictionary\n",
    "\n",
    "import numpy as np \n",
    "import pylab as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%load_ext tensorboard\n",
    "\n",
    "#%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 1800\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionary =\\\n",
    "{\\\n",
    "    # Controller Number 2 \n",
    "    0 : {'compatible_actions' : {   0 : [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0],\n",
    "                                    1 : [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
    "                                    2 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0] },\n",
    "         \n",
    "         'link' : [2, 40, 7, 38],\n",
    "         'lane' : ['2-1', '2-2', '2-3', '40-1', '7-1', '7-2', '7-3', '38-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "         \n",
    "         \n",
    "        },\n",
    "    # Controller Number 3\n",
    "    1 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 1],\n",
    "                                    1 : [1, 0, 0, 1, 0, 0, 0, 0],\n",
    "                                    2 : [0, 0, 1, 0, 0, 1, 0, 0] },\n",
    "         \n",
    "         'link' : [5, 48, 70, 46],\n",
    "         'lane' : ['5-1', '5-2', '5-3', '48-1', '70-1', '70-2', '70-3', '46-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 4\n",
    "    2 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    1 : [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    2 : [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    3 : [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "         \n",
    "         'link' : [73, 100, 84, 95],\n",
    "         'lane' : ['73-1', '73-2', '73-3', '100-1', '100-2', '100-3', '100-4',\\\n",
    "                  '84-1', '84-2', '84-3', '95-1', '95-2', '95-3', '95-4'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [14],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 5\n",
    "    3 : {'compatible_actions' : {   0 : [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    1 : [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    2 : [0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1]},\n",
    "         \n",
    "         'link' : [87, 36, 10, 34],\n",
    "         'lane' : ['87-1', '87-2', '87-3', '36-1', '10-1', '10-2', '10-3', '34-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [8],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 6 \n",
    "    4 : {'compatible_actions' : {   0 : [0, 1, 1, 0, 0],\n",
    "                                    1 : [1, 1, 0, 0, 0],\n",
    "                                    2 : [0, 0, 0, 1, 0]},\n",
    "         'link' : [8, 24, 13],\n",
    "         'lane' : ['8-1', '8-2', '24-1', '13-1', '13-2', '13-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 8\n",
    "    5 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 1, 0],\n",
    "                                    1 : [0, 1, 0, 1, 0, 1]},\n",
    "         'link' : [26, 23, 35],\n",
    "         'lane' : ['26-1', '23-1', '35-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [3],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "         \n",
    "        },\n",
    "    # Controller Number 9\n",
    "    6 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 1],\n",
    "                                    1 : [1, 0, 1, 0, 0, 0]},\n",
    "         'link' : [51, 92, 64, 19],\n",
    "         'lane' : ['51-1', '92-1', '92-2', '64-1', '19-1', '19-2'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Contoller Number 10\n",
    "    7 : {'compatible_actions' : {   0 : [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                                    1 : [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                    2 : [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]},\n",
    "         'link' : [18, 66, 16],\n",
    "         'lane' : ['18-1', '18-2', '18-3', '66-1', '16-1', '16-2', '16-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [7],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 12\n",
    "    8 : {'compatible_actions' : {   0 : [1, 0, 1, 0, 0, 0, 0],\n",
    "                                    1 : [0, 1, 0, 0, 0, 0, 0]},\n",
    "         'link' : [62, 45, 44],\n",
    "         'lane' : ['62-1', '45-1', '44-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [3],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller Number 13\n",
    "    9 : {'compatible_actions' : {   0 : [0, 1, 0, 1, 1, 0, 1, 0],\n",
    "                                    1 : [1, 0, 1, 0, 0, 1, 0, 1]},\n",
    "         'link' : [60, 43, 55, 58],\n",
    "         'lane' : ['60-1', '43-1', '55-1', '58-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "            \n",
    "        },\n",
    "    # Controller 15\n",
    "    10 : {'compatible_actions' : {  0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                    1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "         'link' : [32, 42, 30, 39],\n",
    "         'lane' : ['32-1', '42-1', '30-1', '39-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller 16\n",
    "    11 : {'compatible_actions' :  { 0 : [1, 0, 1, 0, 0, 1, 0, 1],\n",
    "                                    1 : [0, 1, 0, 1, 1, 0, 1, 0]},\n",
    "         'link' : [29, 50, 28, 47],\n",
    "         'lane' : ['29-1', '50-1', '28-1', '47-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [4],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        },\n",
    "    # Controller 17\n",
    "    12 : {'compatible_actions' :  { 0 : [1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                    1 : [0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1],\n",
    "                                    2 : [0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0],\n",
    "                                    3 : [0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0]},\n",
    "         'link' : [27, 22, 25, 77],\n",
    "         'lane' : ['27-1', '22-1', '22-2', '22-3', '25-1', '77-1', '77-2'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [7],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "            \n",
    "        },\n",
    "    # Controller 33 \n",
    "    13 : {'compatible_actions' :  { 0 : [1, 0, 0, 1, 0, 0, 1, 0, 0],\n",
    "                                    1 : [0, 0, 1, 1, 0, 1, 0, 0, 0],\n",
    "                                    2 : [0, 1, 0, 0, 1, 1, 0, 1, 1]},\n",
    "         'link' : [68, 71, 75],\n",
    "         'lane' : ['68-1', '68-2', '68-3', '71-1', '71-2', '75-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 8,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [6],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues'\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = environment(model_name, vissim_working_directory, sim_length, Balance_dictionary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.SCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.SCUs[0].state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "actions = dict()\n",
    "for i in range(len(env.SCUs)):\n",
    "    actions[i]=0\n",
    "    \n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Cyclic_Control():\n",
    "    def __init__(self,size):\n",
    "        self.action = 0\n",
    "        self.size = size\n",
    "        \n",
    "    def choose_action(self,state=None):\n",
    "        self.action = (self.action + 1) % self.size\n",
    "        return self.action\n",
    "CC = [] \n",
    "\n",
    "for idx, info in Balance_dictionary.items():\n",
    "        cycle_size = len(info['compatible_actions'])\n",
    "        CC.append(Cyclic_Control(cycle_size))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Training loop / simulation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_state = env.get_state()\n",
    "for idx, s in start_state.items():\n",
    "    actions[idx] = CC[idx].choose_action(s)\n",
    "    \n",
    "for _ in range(10000):\n",
    "    action_required, SARSDs = env.step(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            #print(sarsd)\n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = CC[idx].choose_action(ns)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Balance RL AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 1800\n",
    "agent_type = 'AC'\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionary = balance_dictionary(agent_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n"
     ]
    }
   ],
   "source": [
    "env = environment(model_name, vissim_working_directory, sim_length, Balance_dictionary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_1 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  960       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  630       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_2 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,307\n",
      "Trainable params: 12,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_3 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_4 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,416\n",
      "Trainable params: 11,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model2_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  256       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  168       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_5 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,055\n",
      "Trainable params: 11,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_6 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,373\n",
      "Trainable params: 11,373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  512       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  336       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_7 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,522\n",
      "Trainable params: 11,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  256       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  168       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_8 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,055\n",
      "Trainable params: 11,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_9 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_10  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model2_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_11  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  512       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  336       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_12  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,565\n",
      "Trainable params: 11,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_13  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,416\n",
      "Trainable params: 11,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Agent hyperparameters\n",
    "gamma = 0.85\n",
    "alpha = 0.0005\n",
    "value = 25\n",
    "entropy = 5000\n",
    "n_step_size = 11\n",
    "reduce_entropy_every = 1000\n",
    "entropy_threshold = 0.5\n",
    "timesteps_per_second = 1\n",
    "\n",
    "\n",
    "# for the monitoring only for AC\n",
    "horizon = 50\n",
    "n_sample = 10\n",
    "\n",
    "Balance_MultiAc_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Balance_dictionary, n_step_size, gamma, alpha, entropy, value, \\\n",
    "                timesteps_per_second = timesteps_per_second, verbose = True, horizon = 100, \\\n",
    "                n_sample = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Balance.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 1800 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links and Lanes.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 1.06 seconds.\n",
      "\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.4674e+03 - output_1_loss: -5.4941e+03 - output_2_loss: 26.7329\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4573e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 8.7875\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4571e+03 - output_1_loss: -3.4649e+03 - output_2_loss: 7.8728\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.9315e+03 - output_1_loss: -6.9315e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4351e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 30.4049\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4579e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 7.8736\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.4345e+03 - output_1_loss: -5.4942e+03 - output_2_loss: 59.6747\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.9315e+03 - output_1_loss: -6.9315e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4028e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 63.3638\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.1407e+03 - output_1_loss: -5.4775e+03 - output_2_loss: 1336.8276\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4657e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4578e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 8.3335\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.4838e+03 - output_1_loss: -5.4917e+03 - output_2_loss: 7.8809\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4579e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 7.6377\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3989e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 67.8662\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.3942e+03 - output_1_loss: -5.4950e+03 - output_2_loss: 100.7995\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.9315e+03 - output_1_loss: -6.9315e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.4669e+03 - output_1_loss: -5.4914e+03 - output_2_loss: 24.5284\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4657e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4366e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 29.7350\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.4647e+03 - output_1_loss: -5.4932e+03 - output_2_loss: 28.5185\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4127e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 53.6793\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.7591e+03 - output_1_loss: -5.4779e+03 - output_2_loss: 1718.8265\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4034e+03 - output_1_loss: -3.4631e+03 - output_2_loss: 59.6264\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4085e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 57.6951\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3064e+03 - output_1_loss: -5.4780e+03 - output_2_loss: 3171.5784\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4657e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4052e+03 - output_1_loss: -3.4649e+03 - output_2_loss: 59.6575\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4042e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 62.2442\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.4304e+03 - output_1_loss: -5.4893e+03 - output_2_loss: 58.8286\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3441e+03 - output_1_loss: -6.9336e+03 - output_2_loss: 3589.4758\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2592e+03 - output_1_loss: -5.4939e+03 - output_2_loss: 234.6320\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.3040e+03 - output_1_loss: -5.4959e+03 - output_2_loss: 191.8633\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.9315e+03 - output_1_loss: -6.9315e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4152e+03 - output_1_loss: -5.4903e+03 - output_2_loss: 3075.0439\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.4706e+03 - output_1_loss: -5.4926e+03 - output_2_loss: 21.9963\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 521.7256 - output_1_loss: -5.4806e+03 - output_2_loss: 6002.3423\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4657e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4103e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 55.3710\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4084e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 57.6409\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3051e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 162.1778\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.3335e+03 - output_1_loss: -5.4892e+03 - output_2_loss: 155.6323\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1819.0571 - output_1_loss: -6.9427e+03 - output_2_loss: 8761.7373\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3562e+03 - output_1_loss: -3.4647e+03 - output_2_loss: 108.4424\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3712e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 95.7029\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.4742e+03 - output_1_loss: -5.4930e+03 - output_2_loss: 18.7905\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2795e+03 - output_1_loss: -5.4951e+03 - output_2_loss: 215.6326\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.0717e+03 - output_1_loss: -5.4923e+03 - output_2_loss: 420.5308\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4132e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 53.0968\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1654e+03 - output_1_loss: -3.4638e+03 - output_2_loss: 298.4696\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.9315e+03 - output_1_loss: -6.9315e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.5359e+01 - output_1_loss: -5.5001e+03 - output_2_loss: 5444.6997\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2299e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 235.7737\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3948.8652 - output_1_loss: -5.4912e+03 - output_2_loss: 9440.1025\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4657e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4111e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 55.2020\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2091e+03 - output_1_loss: -5.4920e+03 - output_2_loss: 282.8970\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3405.9683 - output_1_loss: -6.9428e+03 - output_2_loss: 10348.7227\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.4390e+03 - output_1_loss: -5.4884e+03 - output_2_loss: 1049.4071\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3813e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 85.5684\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9990e+03 - output_1_loss: -3.4639e+03 - output_2_loss: 464.8617\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.3037e+03 - output_1_loss: -5.4956e+03 - output_2_loss: 191.9274\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7757.2300 - output_1_loss: -5.5053e+03 - output_2_loss: 13262.5225\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.1719e+03 - output_1_loss: -5.4964e+03 - output_2_loss: 324.4840\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4160e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 50.3849\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.9315e+03 - output_1_loss: -6.9315e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3507.9150 - output_1_loss: -6.9495e+03 - output_2_loss: 10457.4414\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.7211e+02 - output_1_loss: -5.5035e+03 - output_2_loss: 4931.3711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2630e+03 - output_1_loss: -3.4642e+03 - output_2_loss: 201.1702\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3864e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 80.3817\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4657e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4142e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 52.1730\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.1501e+03 - output_1_loss: -5.4938e+03 - output_2_loss: 343.6420\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.9373e+03 - output_1_loss: -5.4978e+03 - output_2_loss: 560.5225\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.0904e+03 - output_1_loss: -5.4883e+03 - output_2_loss: 1397.9475\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0328e+03 - output_1_loss: -3.4643e+03 - output_2_loss: 431.5408\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.9223e+03 - output_1_loss: -6.9323e+03 - output_2_loss: 9.9646\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -8.2387e+02 - output_1_loss: -5.5054e+03 - output_2_loss: 4681.5757\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.3465e+03 - output_1_loss: -5.4909e+03 - output_2_loss: 144.3887\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2227e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 244.1309\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6782.2891 - output_1_loss: -5.5110e+03 - output_2_loss: 12293.3057\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4657e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3402e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 126.9989\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4181e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 48.2067\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.9984e+03 - output_1_loss: -5.4957e+03 - output_2_loss: 497.2899\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4179.5610 - output_1_loss: -6.9522e+03 - output_2_loss: 11131.7812\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3930e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 73.6994\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0577e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 408.1644\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1802e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 287.3578\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.3095e+03 - output_1_loss: -5.5003e+03 - output_2_loss: 1190.7996\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4972e+03 - output_1_loss: -5.4961e+03 - output_2_loss: 1998.8519\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4234e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 42.9220\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.8560e+03 - output_1_loss: -6.9327e+03 - output_2_loss: 76.6538\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1507e+03 - output_1_loss: -5.5045e+03 - output_2_loss: 4353.7397\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2472e+03 - output_1_loss: -5.4946e+03 - output_2_loss: 247.3658\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4015e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 64.9801\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9720.1504 - output_1_loss: -5.5175e+03 - output_2_loss: 15237.6367\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4657e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 0.0000e+00\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7870e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 678.6085\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.6945e+03 - output_1_loss: -5.4977e+03 - output_2_loss: 803.1250\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5271.9473 - output_1_loss: -6.9546e+03 - output_2_loss: 12226.5596\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2584e+03 - output_1_loss: -5.5029e+03 - output_2_loss: 2244.5134\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.5662e+03 - output_1_loss: -5.4992e+03 - output_2_loss: 2932.9849\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4086e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 58.0641\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0889e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 377.4721\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.6563e+03 - output_1_loss: -6.9345e+03 - output_2_loss: 278.2399\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -7.2234e+02 - output_1_loss: -5.5057e+03 - output_2_loss: 4783.3950\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2028e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 264.7776\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4130e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 53.4109\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12117.8867 - output_1_loss: -5.5200e+03 - output_2_loss: 17637.9121\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.9639e+03 - output_1_loss: -5.4999e+03 - output_2_loss: 1535.9913\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.1961e+03 - output_1_loss: -5.4945e+03 - output_2_loss: 298.4611\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.6000e+03 - output_1_loss: -5.5059e+03 - output_2_loss: 3905.8779\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.5572e+03 - output_1_loss: -5.5037e+03 - output_2_loss: 3946.5056\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2334e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 233.9507\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14568.5195 - output_1_loss: -5.5222e+03 - output_2_loss: 20090.6855\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4596e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 6.4242\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4283e+03 - output_1_loss: -3.4609e+03 - output_2_loss: 1032.5601\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8042.8574 - output_1_loss: -6.9573e+03 - output_2_loss: 15000.1260\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4259e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 40.4475\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1303e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 336.3617\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.5402e+03 - output_1_loss: -6.9403e+03 - output_2_loss: 1400.1085\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6012e+02 - output_1_loss: -5.5067e+03 - output_2_loss: 5246.5918\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4284e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 37.8029\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17665.4180 - output_1_loss: -5.5230e+03 - output_2_loss: 23188.4121\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3310e+03 - output_1_loss: -5.5000e+03 - output_2_loss: 2168.9807\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11599.0605 - output_1_loss: -6.9601e+03 - output_2_loss: 18559.1270\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2065e+03 - output_1_loss: -5.4952e+03 - output_2_loss: 288.6704\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1155.4141 - output_1_loss: -5.5096e+03 - output_2_loss: 6664.9717\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 235.8999 - output_1_loss: -5.5076e+03 - output_2_loss: 5743.4604\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4419e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 24.3456\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4336e+03 - output_1_loss: -6.9224e+03 - output_2_loss: 3488.8076\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 545.9902 - output_1_loss: -5.5080e+03 - output_2_loss: 6053.9902\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2278e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 239.7099\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4495e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 16.5247\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4568e+03 - output_1_loss: -3.4647e+03 - output_2_loss: 7.8845\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.9537e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 1513.4128\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1857e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 280.9407\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15107.4707 - output_1_loss: -6.9628e+03 - output_2_loss: 22070.2207\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2201e+03 - output_1_loss: -5.4955e+03 - output_2_loss: 275.3521\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2595e+02 - output_1_loss: -6.9263e+03 - output_2_loss: 6400.3667\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4592e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 6.6802\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20184.8223 - output_1_loss: -5.5160e+03 - output_2_loss: 25700.7793\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4581e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 7.6130\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.9415e+02 - output_1_loss: -5.5064e+03 - output_2_loss: 4912.1997\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5808.1738 - output_1_loss: -5.5132e+03 - output_2_loss: 11321.4131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3849.2295 - output_1_loss: -5.5106e+03 - output_2_loss: 9359.8057\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4479e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 18.2404\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1827.2114 - output_1_loss: -5.5098e+03 - output_2_loss: 7336.9922\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2376e+03 - output_1_loss: -5.4955e+03 - output_2_loss: 257.8956\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2959e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 171.2751\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4011.9595 - output_1_loss: -6.9412e+03 - output_2_loss: 10953.1553\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.9811e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 1486.6107\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4630e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 2.8185\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2609e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 205.6325\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15022.5488 - output_1_loss: -6.9647e+03 - output_2_loss: 21987.2324\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22859.0449 - output_1_loss: -5.5160e+03 - output_2_loss: 28375.0801\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4586e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 7.2715\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4490.4844 - output_1_loss: -5.5138e+03 - output_2_loss: 10004.2637\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4329.7402 - output_1_loss: -5.4986e+03 - output_2_loss: 9828.3447\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7778.8208 - output_1_loss: -5.5150e+03 - output_2_loss: 13293.7812\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4435e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 22.8014\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3339.2749 - output_1_loss: -5.5114e+03 - output_2_loss: 8850.6904\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.0619e+03 - output_1_loss: -5.4971e+03 - output_2_loss: 435.2445\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3493e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 117.5455\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9096.7402 - output_1_loss: -6.9583e+03 - output_2_loss: 16055.0254\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.7763e+03 - output_1_loss: -3.4698e+03 - output_2_loss: 1693.5874\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4554e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 10.7114\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4591e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 6.9053\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3254e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 140.8640\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15328.8320 - output_1_loss: -6.9639e+03 - output_2_loss: 22292.7324\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3540.0879 - output_1_loss: -5.5065e+03 - output_2_loss: 9046.5928\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11950.5391 - output_1_loss: -5.5152e+03 - output_2_loss: 17465.7461\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5117.5112 - output_1_loss: -5.5131e+03 - output_2_loss: 10630.6455\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3591e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 107.8703\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15471.8730 - output_1_loss: -6.9667e+03 - output_2_loss: 22438.5430\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26216.3535 - output_1_loss: -5.5203e+03 - output_2_loss: 31736.6426\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.8296e+03 - output_1_loss: -3.4701e+03 - output_2_loss: 1640.5071\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5741.6045 - output_1_loss: -5.5115e+03 - output_2_loss: 11253.0781\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4372e+03 - output_1_loss: -3.4651e+03 - output_2_loss: 27.9491\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4595e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 6.4956\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3730e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 94.0414\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16116.8906 - output_1_loss: -6.9646e+03 - output_2_loss: 23081.4609\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.8135e+03 - output_1_loss: -5.4975e+03 - output_2_loss: 684.0123\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7106.7915 - output_1_loss: -5.5143e+03 - output_2_loss: 12621.0586\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3636e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 103.4216\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21989.9473 - output_1_loss: -6.9726e+03 - output_2_loss: 28962.5859\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 29771.7266 - output_1_loss: -5.5260e+03 - output_2_loss: 35297.7031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3841e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 81.9498\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3958.5059 - output_1_loss: -5.5094e+03 - output_2_loss: 9467.9180\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 18269.8789 - output_1_loss: -5.5191e+03 - output_2_loss: 23788.9453\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4481e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 18.2154\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2610e+03 - output_1_loss: -3.4676e+03 - output_2_loss: 206.6664\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0159e+03 - output_1_loss: -3.4697e+03 - output_2_loss: 1453.7469\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8538.7070 - output_1_loss: -5.5142e+03 - output_2_loss: 14052.8789\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4398e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 25.8525\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16696.2266 - output_1_loss: -6.9642e+03 - output_2_loss: 23660.4688\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.9608e+03 - output_1_loss: -5.4956e+03 - output_2_loss: 534.7779\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 29263.8242 - output_1_loss: -5.5145e+03 - output_2_loss: 34778.3633\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4240e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 42.5766\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9447.0566 - output_1_loss: -5.5169e+03 - output_2_loss: 14963.9541\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3957e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 71.0450\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 29387.8086 - output_1_loss: -6.9768e+03 - output_2_loss: 36364.6484\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 33750.1133 - output_1_loss: -5.5304e+03 - output_2_loss: 39280.5469\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1519e+03 - output_1_loss: -3.4693e+03 - output_2_loss: 1317.4025\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4220e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 44.2810\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2751.2183 - output_1_loss: -5.5079e+03 - output_2_loss: 8259.1162\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4058e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 60.8601\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2843e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 183.0662\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17044.8477 - output_1_loss: -6.9662e+03 - output_2_loss: 24011.0059\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3915e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 75.3990\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12355.9648 - output_1_loss: -5.5191e+03 - output_2_loss: 17875.1074\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3478e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 119.4200\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10588.2852 - output_1_loss: -5.5140e+03 - output_2_loss: 16102.2754\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.2742e+03 - output_1_loss: -3.4691e+03 - output_2_loss: 1194.8967\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.0644e+03 - output_1_loss: -5.4959e+03 - output_2_loss: 431.5412\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27823.9844 - output_1_loss: -5.5049e+03 - output_2_loss: 33328.9141\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 35011.0469 - output_1_loss: -6.9602e+03 - output_2_loss: 41971.2227\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 38029.5508 - output_1_loss: -5.5339e+03 - output_2_loss: 43563.4219\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14494.6113 - output_1_loss: -5.5210e+03 - output_2_loss: 20015.6328\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3870e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 79.7407\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3298e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 137.5389\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2013.5874 - output_1_loss: -5.5078e+03 - output_2_loss: 7521.3457\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4175e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 48.9233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4955e+03 - output_1_loss: -3.4687e+03 - output_2_loss: 973.2300\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2949e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 172.2894\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19686.0508 - output_1_loss: -6.9698e+03 - output_2_loss: 26655.8125\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.1246e+03 - output_1_loss: -5.4962e+03 - output_2_loss: 371.6681\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4020e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 64.7790\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17010.1445 - output_1_loss: -5.5214e+03 - output_2_loss: 22531.5098\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15850.9883 - output_1_loss: -5.5219e+03 - output_2_loss: 21372.8984\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3012e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 166.2437\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27939.9941 - output_1_loss: -5.5143e+03 - output_2_loss: 33454.3164\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8418e+03 - output_1_loss: -3.4683e+03 - output_2_loss: 626.5009\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.0941e+03 - output_1_loss: -5.4965e+03 - output_2_loss: 402.4124\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4091e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 57.5341\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 42970.1523 - output_1_loss: -6.9711e+03 - output_2_loss: 49941.2891\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 32089.7168 - output_1_loss: -5.5283e+03 - output_2_loss: 37618.0000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4071e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 58.1100\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2317.5225 - output_1_loss: -5.5094e+03 - output_2_loss: 7826.9214\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4202e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 46.3073\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3082e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 158.8514\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17852.8027 - output_1_loss: -6.9670e+03 - output_2_loss: 24819.8438\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25842.7109 - output_1_loss: -5.5273e+03 - output_2_loss: 31370.0234\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 51686.5078 - output_1_loss: -6.9723e+03 - output_2_loss: 58658.8516\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10738.3750 - output_1_loss: -5.5156e+03 - output_2_loss: 16253.9590\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3617e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 105.0666\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3364e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 130.7592\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28921.1562 - output_1_loss: -5.5187e+03 - output_2_loss: 34439.8086\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2047e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 262.6805\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3255e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 141.4536\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.1950e+03 - output_1_loss: -5.4958e+03 - output_2_loss: 300.7761\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4181e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 48.3435\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 34605.7344 - output_1_loss: -5.5293e+03 - output_2_loss: 40135.0781\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 35629.4062 - output_1_loss: -5.5340e+03 - output_2_loss: 41163.3594\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2844.7095 - output_1_loss: -5.5096e+03 - output_2_loss: 8354.3457\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3231e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 144.2394\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3205e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 146.6833\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3521e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 114.8333\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19350.8789 - output_1_loss: -6.9691e+03 - output_2_loss: 26320.0000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 30237.9375 - output_1_loss: -5.5222e+03 - output_2_loss: 35760.0977\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3483e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 118.4501\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 55169.8359 - output_1_loss: -6.9702e+03 - output_2_loss: 62140.0000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13659.1406 - output_1_loss: -5.5197e+03 - output_2_loss: 19178.8262\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 47919.2891 - output_1_loss: -5.5399e+03 - output_2_loss: 53459.1719\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3050e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 162.0065\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23119.4688 - output_1_loss: -6.9665e+03 - output_2_loss: 30085.9883\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2286e+03 - output_1_loss: -5.4955e+03 - output_2_loss: 266.9031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3663e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 100.7327\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3579e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 108.6809\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 39974.1523 - output_1_loss: -5.5287e+03 - output_2_loss: 45502.8594\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 430.6719 - output_1_loss: -5.5059e+03 - output_2_loss: 5936.6191\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3649e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 101.9838\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2881e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 179.3224\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3228e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 144.4907\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 59154.0664 - output_1_loss: -5.5392e+03 - output_2_loss: 64693.2539\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 29144.7891 - output_1_loss: -5.5220e+03 - output_2_loss: 34666.7461\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3831e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 83.5090\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28644.4844 - output_1_loss: -6.9646e+03 - output_2_loss: 35609.0586\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 61088.7734 - output_1_loss: -6.9737e+03 - output_2_loss: 68062.5234\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 46301.4375 - output_1_loss: -5.5267e+03 - output_2_loss: 51828.1836\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14075.9053 - output_1_loss: -5.5207e+03 - output_2_loss: 19596.6289\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3745e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 92.2968\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2968e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 170.6520\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 69872.5703 - output_1_loss: -5.5415e+03 - output_2_loss: 75414.1172\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2033e+03 - output_1_loss: -5.4960e+03 - output_2_loss: 292.7291\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28506.4238 - output_1_loss: -5.5225e+03 - output_2_loss: 34028.9609\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3652e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 100.5744\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3667e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 100.1493\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3456e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 121.3539\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.9620e+02 - output_1_loss: -5.5055e+03 - output_2_loss: 5109.3198\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3017e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 165.4913\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3869e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 79.8171\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0311e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 437.5071\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 36356.6133 - output_1_loss: -6.9713e+03 - output_2_loss: 43327.9414\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3766e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 90.0356\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3657e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 101.2304\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 85120.9609 - output_1_loss: -6.9845e+03 - output_2_loss: 92105.4688\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 35337.8242 - output_1_loss: -5.5201e+03 - output_2_loss: 40857.8789\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7989.0498 - output_1_loss: -5.5128e+03 - output_2_loss: 13501.8037\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3141e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 153.0346\n",
      "Train on 11 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 - 0s - loss: 79901.4141 - output_1_loss: -5.5467e+03 - output_2_loss: 85448.1016\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.1612e+03 - output_1_loss: -5.4961e+03 - output_2_loss: 334.8285\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21330.0039 - output_1_loss: -5.5181e+03 - output_2_loss: 26848.0625\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3603e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 106.5720\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 48672.8906 - output_1_loss: -6.9729e+03 - output_2_loss: 55645.8281\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.4480e+03 - output_1_loss: -5.5014e+03 - output_2_loss: 4053.3811\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28793.8320 - output_1_loss: -5.5170e+03 - output_2_loss: 34310.7812\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4032e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 63.2690\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8734e+03 - output_1_loss: -3.4691e+03 - output_2_loss: 595.6968\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2232e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 244.1068\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3902e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 76.4269\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2811e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 186.4331\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3746e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 92.2849\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 105691.7656 - output_1_loss: -6.9892e+03 - output_2_loss: 112680.9766\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8463.6084 - output_1_loss: -5.5145e+03 - output_2_loss: 13978.1396\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22624.3320 - output_1_loss: -5.5141e+03 - output_2_loss: 28138.4375\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 81563.7891 - output_1_loss: -5.5488e+03 - output_2_loss: 87112.5547\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.1079e+03 - output_1_loss: -5.4960e+03 - output_2_loss: 388.1377\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 18364.7715 - output_1_loss: -5.5154e+03 - output_2_loss: 23880.1230\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 43838.5664 - output_1_loss: -6.9639e+03 - output_2_loss: 50802.4766\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.5258e+03 - output_1_loss: -5.5005e+03 - output_2_loss: 3974.6853\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2935e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 173.9414\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4253e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 40.9420\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0319e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 436.3106\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2585e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 208.3877\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3699e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 97.0760\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3129e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 153.1692\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 131335.0781 - output_1_loss: -7.0004e+03 - output_2_loss: 138335.5156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9715.9355 - output_1_loss: -5.5172e+03 - output_2_loss: 15233.1523\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.5658e+02 - output_1_loss: -5.5041e+03 - output_2_loss: 5147.5645\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0260e+03 - output_1_loss: -3.4684e+03 - output_2_loss: 442.4652\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21867.6016 - output_1_loss: -5.5179e+03 - output_2_loss: 27385.5312\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 93102.3906 - output_1_loss: -5.5541e+03 - output_2_loss: 98656.4453\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.9934e+03 - output_1_loss: -5.4969e+03 - output_2_loss: 503.4916\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12239.7871 - output_1_loss: -5.5094e+03 - output_2_loss: 17749.1465\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8992e+03 - output_1_loss: -3.4687e+03 - output_2_loss: 569.5577\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 47097.4297 - output_1_loss: -6.9753e+03 - output_2_loss: 54072.6836\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3373e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 129.3754\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11488.8887 - output_1_loss: -5.5184e+03 - output_2_loss: 17007.2500\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2997e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 167.7603\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7070e+03 - output_1_loss: -3.4691e+03 - output_2_loss: 762.1323\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20751.6680 - output_1_loss: -5.5103e+03 - output_2_loss: 26262.0137\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2856e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 181.1121\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4105e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 56.0832\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7969e+03 - output_1_loss: -3.4691e+03 - output_2_loss: 672.2050\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 143171.7656 - output_1_loss: -6.9928e+03 - output_2_loss: 150164.5938\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 59204.8750 - output_1_loss: -6.9882e+03 - output_2_loss: 66193.0547\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 728.3999 - output_1_loss: -5.5067e+03 - output_2_loss: 6235.0864\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13170.9414 - output_1_loss: -5.5192e+03 - output_2_loss: 18690.1641\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 105718.5156 - output_1_loss: -5.5595e+03 - output_2_loss: 111278.0000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.6968e+03 - output_1_loss: -5.4971e+03 - output_2_loss: 800.2239\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11039.9727 - output_1_loss: -5.5134e+03 - output_2_loss: 16553.3887\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3264e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 140.1456\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4242e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 42.1963\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3105e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 156.7217\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1706e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 297.0749\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8893e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 579.2546\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14888.3633 - output_1_loss: -5.5041e+03 - output_2_loss: 20392.5020\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9168e+03 - output_1_loss: -3.4685e+03 - output_2_loss: 551.7407\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3188e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 148.0457\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 174391.7812 - output_1_loss: -7.0104e+03 - output_2_loss: 181402.1875\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 78234.1641 - output_1_loss: -6.9989e+03 - output_2_loss: 85233.1016\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2161e+03 - output_1_loss: -3.4678e+03 - output_2_loss: 251.6958\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3776.0420 - output_1_loss: -5.5115e+03 - output_2_loss: 9287.5361\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7168.7129 - output_1_loss: -5.5112e+03 - output_2_loss: 12679.9189\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 124882.4453 - output_1_loss: -5.5668e+03 - output_2_loss: 130449.2422\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.8117e+03 - output_1_loss: -5.4965e+03 - output_2_loss: 684.7556\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0405e+03 - output_1_loss: -3.4683e+03 - output_2_loss: 427.7640\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9236e+03 - output_1_loss: -3.4683e+03 - output_2_loss: 544.7510\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12411.1543 - output_1_loss: -5.5078e+03 - output_2_loss: 17918.9941\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10661.2598 - output_1_loss: -5.5149e+03 - output_2_loss: 16176.1152\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4312e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 35.1165\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9614e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 506.5060\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 100401.9922 - output_1_loss: -7.0089e+03 - output_2_loss: 107410.9062\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9736e+03 - output_1_loss: -3.4687e+03 - output_2_loss: 495.1401\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 140791.1875 - output_1_loss: -5.5709e+03 - output_2_loss: 146362.1094\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.8737e+03 - output_1_loss: -5.4968e+03 - output_2_loss: 623.0263\n",
      "Train on 11 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 - 0s - loss: -3.3422e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 124.4141\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 201370.7656 - output_1_loss: -7.0153e+03 - output_2_loss: 208386.0938\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4334e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 32.8347\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10585.0645 - output_1_loss: -5.5182e+03 - output_2_loss: 16103.2559\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0424e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 425.3544\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8453.9199 - output_1_loss: -5.5148e+03 - output_2_loss: 13968.6846\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6796e+03 - output_1_loss: -3.4695e+03 - output_2_loss: 789.9556\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9202e+03 - output_1_loss: -3.4687e+03 - output_2_loss: 548.4773\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9663e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 501.7124\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17312.8359 - output_1_loss: -5.5153e+03 - output_2_loss: 22828.1289\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8242.7656 - output_1_loss: -5.5111e+03 - output_2_loss: 13753.8662\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 155710.4688 - output_1_loss: -5.5699e+03 - output_2_loss: 161280.3438\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 124489.5312 - output_1_loss: -7.0232e+03 - output_2_loss: 131512.7500\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 18383.1074 - output_1_loss: -5.5249e+03 - output_2_loss: 23908.0293\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0373e+03 - output_1_loss: -3.4680e+03 - output_2_loss: 430.6789\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.7468e+03 - output_1_loss: -5.4979e+03 - output_2_loss: 751.1051\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1356e+03 - output_1_loss: -3.4707e+03 - output_2_loss: 1335.0604\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.5648e+03 - output_1_loss: -3.4695e+03 - output_2_loss: 904.6701\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9435e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 524.7654\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3533e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 113.2127\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 210899.4688 - output_1_loss: -7.0074e+03 - output_2_loss: 217906.8594\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 170343.5938 - output_1_loss: -5.5674e+03 - output_2_loss: 175911.0156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4239e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 42.6134\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 157628.9375 - output_1_loss: -7.0381e+03 - output_2_loss: 164666.9844\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5257.7046 - output_1_loss: -5.5075e+03 - output_2_loss: 10765.2217\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26637.5977 - output_1_loss: -5.5302e+03 - output_2_loss: 32167.8262\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 18896.8047 - output_1_loss: -5.5194e+03 - output_2_loss: 24416.2441\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9575.7891 - output_1_loss: -5.5146e+03 - output_2_loss: 15090.3945\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.5662e+03 - output_1_loss: -3.4715e+03 - output_2_loss: 1905.3274\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9741e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 494.1108\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4101e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 56.5437\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0544e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 413.5660\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3727.8560 - output_1_loss: -5.5064e+03 - output_2_loss: 9234.2930\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.3296e+03 - output_1_loss: -5.4993e+03 - output_2_loss: 1169.7339\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 36253.5898 - output_1_loss: -5.5356e+03 - output_2_loss: 41789.1602\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8181e+03 - output_1_loss: -3.4684e+03 - output_2_loss: 650.3220\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3408e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 126.1153\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 232141.3438 - output_1_loss: -7.0226e+03 - output_2_loss: 239163.9531\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 160014.5312 - output_1_loss: -5.5595e+03 - output_2_loss: 165574.0469\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 183786.5938 - output_1_loss: -7.0207e+03 - output_2_loss: 190807.2969\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3957e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 71.1189\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0787e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 389.2149\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23825.4668 - output_1_loss: -5.5223e+03 - output_2_loss: 29347.7734\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7786.8428 - output_1_loss: -5.5137e+03 - output_2_loss: 13300.5527\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 516.4307 - output_1_loss: -5.5032e+03 - output_2_loss: 6019.6201\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.8365e+03 - output_1_loss: -5.5009e+03 - output_2_loss: 1664.4032\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0146e+03 - output_1_loss: -3.4704e+03 - output_2_loss: 1455.7814\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 46840.6094 - output_1_loss: -5.5407e+03 - output_2_loss: 52381.3398\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1204e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 346.9777\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 128449.6484 - output_1_loss: -5.5536e+03 - output_2_loss: 134003.2031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3574e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 109.6909\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7203e+03 - output_1_loss: -3.4688e+03 - output_2_loss: 748.5811\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1463e+03 - output_1_loss: -3.4678e+03 - output_2_loss: 321.4413\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 33363.8242 - output_1_loss: -5.5031e+03 - output_2_loss: 38866.9102\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3709e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 95.8516\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 242249.5781 - output_1_loss: -7.0123e+03 - output_2_loss: 249261.9062\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1601e+03 - output_1_loss: -3.4698e+03 - output_2_loss: 1309.7153\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2078e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 259.1107\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 98128.0781 - output_1_loss: -5.5424e+03 - output_2_loss: 103670.5234\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 160295.5312 - output_1_loss: -7.0185e+03 - output_2_loss: 167313.9844\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11821.9805 - output_1_loss: -5.5177e+03 - output_2_loss: 17339.6328\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -7.7833e+02 - output_1_loss: -5.5010e+03 - output_2_loss: 4722.6880\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.0103e+02 - output_1_loss: -5.5083e+03 - output_2_loss: 5107.2319\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2353e+03 - output_1_loss: -3.4678e+03 - output_2_loss: 232.4903\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 34192.2656 - output_1_loss: -5.5267e+03 - output_2_loss: 39718.9336\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7079e+03 - output_1_loss: -3.4689e+03 - output_2_loss: 761.0250\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 45239.8633 - output_1_loss: -5.5130e+03 - output_2_loss: 50752.8594\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3655e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 101.3041\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 249383.6562 - output_1_loss: -7.0344e+03 - output_2_loss: 256418.1094\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3092e+03 - output_1_loss: -3.4693e+03 - output_2_loss: 1160.1167\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1354e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 332.7510\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5055.5601 - output_1_loss: -5.4965e+03 - output_2_loss: 10552.0430\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7256e+03 - output_1_loss: -3.4688e+03 - output_2_loss: 743.2185\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3012e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 165.3689\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 92054.5469 - output_1_loss: -5.5468e+03 - output_2_loss: 97601.3750\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 192407.5781 - output_1_loss: -7.0383e+03 - output_2_loss: 199445.8438\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4128e+03 - output_1_loss: -3.4688e+03 - output_2_loss: 1056.0355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8871.5215 - output_1_loss: -5.5149e+03 - output_2_loss: 14386.4287\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -8.8946e+02 - output_1_loss: -5.5026e+03 - output_2_loss: 4613.1099\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2896e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 177.7200\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 41174.6133 - output_1_loss: -5.5326e+03 - output_2_loss: 46707.2539\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 57159.5156 - output_1_loss: -5.5263e+03 - output_2_loss: 62685.8594\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2150e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 252.6819\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 204830.3438 - output_1_loss: -7.0157e+03 - output_2_loss: 211846.0938\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 239110.0625 - output_1_loss: -7.0553e+03 - output_2_loss: 246165.3438\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4072e+03 - output_1_loss: -3.4692e+03 - output_2_loss: 1061.9771\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2416e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 225.8398\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7194.4131 - output_1_loss: -5.5121e+03 - output_2_loss: 12706.5225\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6845.0596 - output_1_loss: -5.5030e+03 - output_2_loss: 12348.0908\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2314e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 236.3214\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 45329.6680 - output_1_loss: -5.5376e+03 - output_2_loss: 50867.2227\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8882e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 579.8702\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3940e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 72.1598\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 67915.8125 - output_1_loss: -5.5349e+03 - output_2_loss: 73450.7422\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 67073.5078 - output_1_loss: -5.5335e+03 - output_2_loss: 72607.0234\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8132e+03 - output_1_loss: -5.5010e+03 - output_2_loss: 2687.8342\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3665e+03 - output_1_loss: -3.4694e+03 - output_2_loss: 1102.9630\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3087e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 158.3804\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6373.0659 - output_1_loss: -5.5107e+03 - output_2_loss: 11883.7412\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0036e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 464.9655\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 165415.2500 - output_1_loss: -6.9999e+03 - output_2_loss: 172415.2031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 76643.3203 - output_1_loss: -5.5406e+03 - output_2_loss: 82183.9453\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 45205.1602 - output_1_loss: -5.5233e+03 - output_2_loss: 50728.4648\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 223572.7344 - output_1_loss: -7.0460e+03 - output_2_loss: 230618.7344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3640e+03 - output_1_loss: -3.4698e+03 - output_2_loss: 1105.8070\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4485.4004 - output_1_loss: -5.5074e+03 - output_2_loss: 9992.8408\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2724e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 194.9577\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 32337.5449 - output_1_loss: -5.5239e+03 - output_2_loss: 37861.4180\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0472e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 420.3544\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4422e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 23.6714\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 87884.5547 - output_1_loss: -5.5464e+03 - output_2_loss: 93430.9453\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8414e+03 - output_1_loss: -5.5029e+03 - output_2_loss: 2661.5173\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2757e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 191.6700\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11055.9248 - output_1_loss: -5.5181e+03 - output_2_loss: 16573.9941\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3256e+03 - output_1_loss: -3.4700e+03 - output_2_loss: 1144.3535\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1382e+03 - output_1_loss: -3.4678e+03 - output_2_loss: 329.5432\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 152548.3594 - output_1_loss: -7.0039e+03 - output_2_loss: 159552.2969\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2916e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 175.5539\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2629e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 204.0017\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4192e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 47.2503\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28483.9648 - output_1_loss: -5.5168e+03 - output_2_loss: 34000.7344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 226909.9688 - output_1_loss: -7.0496e+03 - output_2_loss: 233959.5625\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3687.3579 - output_1_loss: -5.5084e+03 - output_2_loss: 9195.7256\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 30405.6738 - output_1_loss: -5.5238e+03 - output_2_loss: 35929.5000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20278.9766 - output_1_loss: -5.5199e+03 - output_2_loss: 25798.8359\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1803e+03 - output_1_loss: -3.4703e+03 - output_2_loss: 1290.0110\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2337e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 233.8009\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3987e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 68.0707\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 70795.2891 - output_1_loss: -5.5366e+03 - output_2_loss: 76331.9297\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4091.7598 - output_1_loss: -5.5102e+03 - output_2_loss: 9601.9814\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2533e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 214.2243\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0762e+03 - output_1_loss: -3.4680e+03 - output_2_loss: 391.8085\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 134409.5781 - output_1_loss: -7.0013e+03 - output_2_loss: 141410.8594\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3770.1313 - output_1_loss: -5.5088e+03 - output_2_loss: 9278.9521\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3161e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 150.6965\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26780.8730 - output_1_loss: -5.5263e+03 - output_2_loss: 32307.1309\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12398.4922 - output_1_loss: -5.4853e+03 - output_2_loss: 17883.8301\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 194173.5625 - output_1_loss: -7.0381e+03 - output_2_loss: 201211.6875\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0524e+03 - output_1_loss: -3.4683e+03 - output_2_loss: 415.8604\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2370e+03 - output_1_loss: -3.4676e+03 - output_2_loss: 230.6427\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27187.0586 - output_1_loss: -5.5211e+03 - output_2_loss: 32708.1895\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1537e+03 - output_1_loss: -3.4718e+03 - output_2_loss: 2318.0371\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3902.9072 - output_1_loss: -5.5099e+03 - output_2_loss: 9412.7773\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3486e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 117.7235\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4290e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 37.3001\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 80365.6484 - output_1_loss: -5.5416e+03 - output_2_loss: 85907.2031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3124.3359 - output_1_loss: -5.4863e+03 - output_2_loss: 8610.5889\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1492e+02 - output_1_loss: -5.5032e+03 - output_2_loss: 5388.3130\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1389e+03 - output_1_loss: -3.4676e+03 - output_2_loss: 328.7017\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 120720.9922 - output_1_loss: -7.0028e+03 - output_2_loss: 127723.7734\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25085.3066 - output_1_loss: -5.5280e+03 - output_2_loss: 30613.2695\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2323e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 235.3318\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 204124.3594 - output_1_loss: -7.0469e+03 - output_2_loss: 211171.2969\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2469e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 220.3928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 40398.6758 - output_1_loss: -5.5225e+03 - output_2_loss: 45921.1914\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -9.8402e+02 - output_1_loss: -3.4717e+03 - output_2_loss: 2487.6599\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1849e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 282.4481\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2435.9775 - output_1_loss: -5.5063e+03 - output_2_loss: 7942.2583\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3932e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 72.9088\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26923.0098 - output_1_loss: -5.5301e+03 - output_2_loss: 32453.1191\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3917e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 75.1446\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 55897.5586 - output_1_loss: -5.5219e+03 - output_2_loss: 61419.4883\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1366.0996 - output_1_loss: -5.4916e+03 - output_2_loss: 6857.6826\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1265.0854 - output_1_loss: -5.5072e+03 - output_2_loss: 6772.2598\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 78954.3203 - output_1_loss: -6.9833e+03 - output_2_loss: 85937.5859\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1113e+03 - output_1_loss: -3.4683e+03 - output_2_loss: 356.9402\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.0037e+02 - output_1_loss: -3.4732e+03 - output_2_loss: 3372.8518\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2163e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 250.8251\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 218681.2969 - output_1_loss: -7.0530e+03 - output_2_loss: 225734.2656\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 808.1602 - output_1_loss: -5.5042e+03 - output_2_loss: 6312.3247\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4206e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 45.3920\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 31306.4238 - output_1_loss: -5.5329e+03 - output_2_loss: 36839.3281\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2919e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 175.1160\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3765e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 90.4629\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 43962.0391 - output_1_loss: -5.5153e+03 - output_2_loss: 49477.3906\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 35810.7734 - output_1_loss: -5.5193e+03 - output_2_loss: 41330.0586\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 49294.0742 - output_1_loss: -6.9669e+03 - output_2_loss: 56260.9492\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7867e+02 - output_1_loss: -3.4727e+03 - output_2_loss: 3194.0178\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2013e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 266.2255\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4691.4370 - output_1_loss: -5.5002e+03 - output_2_loss: 10191.6367\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 235340.8594 - output_1_loss: -7.0570e+03 - output_2_loss: 242397.8438\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2142.4155 - output_1_loss: -5.5081e+03 - output_2_loss: 7650.5542\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -9.9529e+02 - output_1_loss: -5.5018e+03 - output_2_loss: 4506.4707\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3473e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 119.8414\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1607e+03 - output_1_loss: -3.4680e+03 - output_2_loss: 307.2690\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4405e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 25.3690\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 41652.3828 - output_1_loss: -5.5387e+03 - output_2_loss: 47191.0352\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2954e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 171.4953\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 40190.5625 - output_1_loss: -5.5148e+03 - output_2_loss: 45705.3828\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -8.8585e+02 - output_1_loss: -3.4712e+03 - output_2_loss: 2585.3284\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0946e+03 - output_1_loss: -5.5022e+03 - output_2_loss: 3407.5510\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 41815.0859 - output_1_loss: -5.5240e+03 - output_2_loss: 47339.1094\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 38644.8398 - output_1_loss: -6.9702e+03 - output_2_loss: 45615.0273\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2677e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 199.5054\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8373.8555 - output_1_loss: -5.5061e+03 - output_2_loss: 13879.9941\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 235892.1094 - output_1_loss: -7.0566e+03 - output_2_loss: 242948.7344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1299.7451 - output_1_loss: -5.5061e+03 - output_2_loss: 6805.8403\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4536e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 12.1462\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3854e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 81.2455\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.3049e+03 - output_1_loss: -3.4703e+03 - output_2_loss: 2165.3792\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.6553e+03 - output_1_loss: -5.5010e+03 - output_2_loss: 1845.6960\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8901e+03 - output_1_loss: -3.4690e+03 - output_2_loss: 578.8692\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 49295.2969 - output_1_loss: -5.5323e+03 - output_2_loss: 54827.5586\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27589.6074 - output_1_loss: -6.9679e+03 - output_2_loss: 34557.5391\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23888.4160 - output_1_loss: -5.5216e+03 - output_2_loss: 29409.9980\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1593e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 308.6329\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24979.0000 - output_1_loss: -5.5099e+03 - output_2_loss: 30488.9375\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 772.6631 - output_1_loss: -5.5050e+03 - output_2_loss: 6277.6841\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.8661e+03 - output_1_loss: -3.4693e+03 - output_2_loss: 1603.1876\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3621e+03 - output_1_loss: -5.5024e+03 - output_2_loss: 2140.3359\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3341e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 132.5961\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12614.7168 - output_1_loss: -5.5119e+03 - output_2_loss: 18126.6016\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 186070.0469 - output_1_loss: -7.0354e+03 - output_2_loss: 193105.4531\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4608e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 4.8200\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3302e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 137.0514\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3373e+03 - output_1_loss: -3.4685e+03 - output_2_loss: 1131.1997\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0973e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 370.3867\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 38415.7891 - output_1_loss: -5.5257e+03 - output_2_loss: 43941.4648\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19009.5234 - output_1_loss: -6.9590e+03 - output_2_loss: 25968.5000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 39802.9961 - output_1_loss: -5.5360e+03 - output_2_loss: 45338.9844\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2756e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 191.4589\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25352.5156 - output_1_loss: -5.5169e+03 - output_2_loss: 30869.4492\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3896e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 76.7263\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 54.0889 - output_1_loss: -5.5034e+03 - output_2_loss: 5557.5332\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2118.6079 - output_1_loss: -5.5115e+03 - output_2_loss: 7630.1001\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6898e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 777.8401\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1499e+03 - output_1_loss: -3.4676e+03 - output_2_loss: 317.6819\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 34734.3984 - output_1_loss: -5.5241e+03 - output_2_loss: 40258.4727\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19285.6504 - output_1_loss: -5.5186e+03 - output_2_loss: 24804.2188\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 192646.2656 - output_1_loss: -7.0406e+03 - output_2_loss: 199686.8594\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4618e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 3.8204\n",
      "Train on 11 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 - 0s - loss: -3.2920e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 174.8938\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3764e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 90.2222\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28199.4609 - output_1_loss: -5.5248e+03 - output_2_loss: 33724.2930\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16778.6309 - output_1_loss: -5.5046e+03 - output_2_loss: 22283.2520\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9639.9043 - output_1_loss: -6.9469e+03 - output_2_loss: 16586.8145\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9860e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 480.8551\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 39635.4844 - output_1_loss: -5.5357e+03 - output_2_loss: 45171.2227\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28834.1914 - output_1_loss: -5.5199e+03 - output_2_loss: 34354.1016\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4182e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 48.2366\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 188759.2812 - output_1_loss: -7.0436e+03 - output_2_loss: 195802.9062\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -9.3294e+02 - output_1_loss: -5.5014e+03 - output_2_loss: 4568.5044\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9789e+03 - output_1_loss: -3.4684e+03 - output_2_loss: 489.5372\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13013.9668 - output_1_loss: -5.5126e+03 - output_2_loss: 18526.5215\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4627e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 2.8427\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2272e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 238.9906\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3027e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 164.0031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3899e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 76.5857\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15065.9268 - output_1_loss: -5.5142e+03 - output_2_loss: 20580.1191\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22116.9082 - output_1_loss: -5.5166e+03 - output_2_loss: 27633.4707\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4010e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 65.6581\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.2951e+03 - output_1_loss: -5.4990e+03 - output_2_loss: 3203.9099\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21147.9922 - output_1_loss: -5.4956e+03 - output_2_loss: 26643.6055\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10709.2988 - output_1_loss: -6.9526e+03 - output_2_loss: 17661.9238\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 45798.8438 - output_1_loss: -5.5392e+03 - output_2_loss: 51337.9961\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8995e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 569.0480\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 185055.8906 - output_1_loss: -7.0421e+03 - output_2_loss: 192097.9531\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4645e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 1.1343\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4147e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 50.9123\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3621e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 104.8778\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3707e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 96.3207\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28278.3789 - output_1_loss: -5.5225e+03 - output_2_loss: 33800.8789\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17499.5176 - output_1_loss: -5.5180e+03 - output_2_loss: 23017.4688\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3313e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 135.1614\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6455e+03 - output_1_loss: -3.4694e+03 - output_2_loss: 823.9233\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 18238.9180 - output_1_loss: -5.5197e+03 - output_2_loss: 23758.6270\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23555.3281 - output_1_loss: -5.5231e+03 - output_2_loss: 29078.4766\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 193898.7812 - output_1_loss: -7.0437e+03 - output_2_loss: 200942.5156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3334e+03 - output_1_loss: -5.4976e+03 - output_2_loss: 2164.1572\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4254e+03 - output_1_loss: -3.4654e+03 - output_2_loss: 40.0192\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7723e+03 - output_1_loss: -3.4692e+03 - output_2_loss: 696.9510\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3971.6743 - output_1_loss: -6.9431e+03 - output_2_loss: 10914.7617\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 29496.4141 - output_1_loss: -5.5218e+03 - output_2_loss: 35018.2148\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4645e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 1.1197\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4003e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 66.3315\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4375e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 27.7195\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1286e+03 - output_1_loss: -3.4705e+03 - output_2_loss: 1341.8768\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19457.0918 - output_1_loss: -5.5179e+03 - output_2_loss: 24974.9629\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14409.7949 - output_1_loss: -5.5148e+03 - output_2_loss: 19924.6113\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3394e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 127.4012\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9393e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 528.9004\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11205.4121 - output_1_loss: -5.5104e+03 - output_2_loss: 16715.8223\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24102.0020 - output_1_loss: -5.5230e+03 - output_2_loss: 29625.0312\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 137467.7656 - output_1_loss: -7.0167e+03 - output_2_loss: 144484.4844\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.5683e+03 - output_1_loss: -5.4949e+03 - output_2_loss: 926.6272\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4044e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 62.1289\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4458e+03 - output_1_loss: -3.4653e+03 - output_2_loss: 19.4747\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.3209e+03 - output_1_loss: -3.4710e+03 - output_2_loss: 2150.1680\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -8.8420e+02 - output_1_loss: -6.9396e+03 - output_2_loss: 6055.4268\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 34823.6758 - output_1_loss: -5.5283e+03 - output_2_loss: 40352.0234\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4645e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 1.1047\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3808e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 85.9206\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0358e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 431.9761\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2470e+03 - output_1_loss: -5.4933e+03 - output_2_loss: 246.2569\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3493e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 117.8721\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21106.2441 - output_1_loss: -5.5209e+03 - output_2_loss: 26627.1895\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14218.9746 - output_1_loss: -5.5150e+03 - output_2_loss: 19733.9824\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13338.3857 - output_1_loss: -5.5141e+03 - output_2_loss: 18852.4746\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 33213.1992 - output_1_loss: -5.5313e+03 - output_2_loss: 38744.4648\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 143151.0938 - output_1_loss: -7.0256e+03 - output_2_loss: 150176.7344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2961e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 171.2289\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2778e+03 - output_1_loss: -5.4947e+03 - output_2_loss: 216.8816\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.9031e+03 - output_1_loss: -3.4705e+03 - output_2_loss: 1567.4812\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1853e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 1283.3273\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.0677e+03 - output_1_loss: -6.9432e+03 - output_2_loss: 5875.5854\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 32614.9062 - output_1_loss: -5.5253e+03 - output_2_loss: 38140.1836\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1863e+03 - output_1_loss: -3.4680e+03 - output_2_loss: 281.6616\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22210.1875 - output_1_loss: -5.5273e+03 - output_2_loss: 27737.4883\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4645e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 1.0892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1305e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 336.7306\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17505.1445 - output_1_loss: -5.5166e+03 - output_2_loss: 23021.7031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1222.2288 - output_1_loss: -3.4709e+03 - output_2_loss: 4693.1016\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 31013.9609 - output_1_loss: -5.5266e+03 - output_2_loss: 36540.5391\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7370.0562 - output_1_loss: -5.5073e+03 - output_2_loss: 12877.3779\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22837.9531 - output_1_loss: -5.5235e+03 - output_2_loss: 28361.4941\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25332.3008 - output_1_loss: -5.5296e+03 - output_2_loss: 30861.9141\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 93422.3594 - output_1_loss: -7.0007e+03 - output_2_loss: 100423.0703\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3607e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 106.0519\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.3122e+03 - output_1_loss: -5.4955e+03 - output_2_loss: 183.2320\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.2569e+03 - output_1_loss: -3.4688e+03 - output_2_loss: 1211.8923\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 931.0405 - output_1_loss: -6.9440e+03 - output_2_loss: 7875.0151\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3026e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 164.5829\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14440.2871 - output_1_loss: -5.5126e+03 - output_2_loss: 19952.8516\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4406e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 25.7200\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0293e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 438.8504\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.2349e+03 - output_1_loss: -5.4959e+03 - output_2_loss: 260.9630\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -8.5454e+02 - output_1_loss: -3.4705e+03 - output_2_loss: 2615.9636\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3234e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 1145.2919\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 33912.7969 - output_1_loss: -5.5299e+03 - output_2_loss: 39442.6602\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7748.1089 - output_1_loss: -5.5094e+03 - output_2_loss: 13257.5312\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 30541.6504 - output_1_loss: -5.5301e+03 - output_2_loss: 36071.7773\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26675.8047 - output_1_loss: -5.5307e+03 - output_2_loss: 32206.4766\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 95024.5312 - output_1_loss: -7.0079e+03 - output_2_loss: 102032.4062\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3642e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 101.9224\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3723e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 94.3438\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9535e+03 - output_1_loss: -3.4685e+03 - output_2_loss: 515.0063\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4649.7549 - output_1_loss: -6.9521e+03 - output_2_loss: 11601.8662\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3330e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 133.8410\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9970.9795 - output_1_loss: -5.5072e+03 - output_2_loss: 15478.1416\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 37794.7734 - output_1_loss: -5.5321e+03 - output_2_loss: 43326.9023\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9106e+03 - output_1_loss: -3.4687e+03 - output_2_loss: 558.0749\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.0515e+03 - output_1_loss: -5.5008e+03 - output_2_loss: 1449.3636\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.1968e+02 - output_1_loss: -3.4712e+03 - output_2_loss: 2851.5156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3862e+03 - output_1_loss: -3.4683e+03 - output_2_loss: 1082.1860\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3426e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 124.0568\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4108.0083 - output_1_loss: -5.5046e+03 - output_2_loss: 9612.6182\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25065.5469 - output_1_loss: -5.5233e+03 - output_2_loss: 30588.8633\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7146.4805 - output_1_loss: -5.5072e+03 - output_2_loss: 12653.6309\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 44130.2539 - output_1_loss: -5.5409e+03 - output_2_loss: 49671.1484\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 78342.3672 - output_1_loss: -6.9943e+03 - output_2_loss: 85336.6328\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3711e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 95.4915\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 43977.0078 - output_1_loss: -5.5353e+03 - output_2_loss: 49512.2734\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3874e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 79.0322\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.5384e+03 - output_1_loss: -3.4695e+03 - output_2_loss: 931.1870\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8692.8145 - output_1_loss: -6.9592e+03 - output_2_loss: 15651.9844\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.6694e+02 - output_1_loss: -3.4717e+03 - output_2_loss: 3104.7397\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.5285e+03 - output_1_loss: -3.4678e+03 - output_2_loss: 939.3553\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 693.7866 - output_1_loss: -5.5016e+03 - output_2_loss: 6195.3877\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.2930e+03 - output_1_loss: -5.4995e+03 - output_2_loss: 1206.5299\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.9996e+03 - output_1_loss: -3.4703e+03 - output_2_loss: 1470.7037\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3245e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 142.4514\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 33029.5820 - output_1_loss: -5.5284e+03 - output_2_loss: 38557.9531\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2126.6108 - output_1_loss: -5.5016e+03 - output_2_loss: 7628.1714\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 78539.2891 - output_1_loss: -5.5552e+03 - output_2_loss: 84094.4688\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 98875.7031 - output_1_loss: -7.0053e+03 - output_2_loss: 105880.9766\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3758e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 90.9827\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26201.2402 - output_1_loss: -5.5154e+03 - output_2_loss: 31716.6230\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4080e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 58.2040\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13110.7617 - output_1_loss: -6.9648e+03 - output_2_loss: 20075.5215\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.0880e+03 - output_1_loss: -5.5003e+03 - output_2_loss: 1412.2344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -7.0889e+02 - output_1_loss: -3.4706e+03 - output_2_loss: 2761.7148\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.5220e+03 - output_1_loss: -3.4683e+03 - output_2_loss: 946.2744\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2578e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 209.7304\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 739.9854 - output_1_loss: -5.5049e+03 - output_2_loss: 6244.8701\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7017e+03 - output_1_loss: -5.4974e+03 - output_2_loss: 2795.7271\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3202e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 147.0934\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4102e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 56.0081\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.5991e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 869.1403\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 34385.5352 - output_1_loss: -5.5301e+03 - output_2_loss: 39915.6406\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 103548.2812 - output_1_loss: -5.5621e+03 - output_2_loss: 109110.4297\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 95282.9609 - output_1_loss: -6.9993e+03 - output_2_loss: 102282.2422\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27483.7129 - output_1_loss: -5.5260e+03 - output_2_loss: 33009.7227\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16096.7695 - output_1_loss: -6.9667e+03 - output_2_loss: 23063.4648\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.5677e+03 - output_1_loss: -5.5019e+03 - output_2_loss: 1934.1591\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -8.7689e+02 - output_1_loss: -3.4701e+03 - output_2_loss: 2593.2256\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8017e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 665.2045\n",
      "Train on 11 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 - 0s - loss: -3.2892e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 178.0237\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3924.9492 - output_1_loss: -5.5124e+03 - output_2_loss: 9437.3682\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.0739e+03 - output_1_loss: -5.4992e+03 - output_2_loss: 1425.3304\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 89458.5156 - output_1_loss: -7.0027e+03 - output_2_loss: 96461.1953\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3518e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 115.0622\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28860.3945 - output_1_loss: -5.5275e+03 - output_2_loss: 34387.9180\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4157e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 50.3661\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6552e+03 - output_1_loss: -3.4680e+03 - output_2_loss: 812.7574\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22219.7852 - output_1_loss: -6.9736e+03 - output_2_loss: 29193.3613\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24959.3477 - output_1_loss: -5.5173e+03 - output_2_loss: 30476.6758\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1688.4355 - output_1_loss: -5.5103e+03 - output_2_loss: 7198.7593\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 103216.0859 - output_1_loss: -5.5588e+03 - output_2_loss: 108774.8672\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2706e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 196.6885\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3600e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 106.7368\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 33526.4023 - output_1_loss: -5.5308e+03 - output_2_loss: 39057.2188\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4235e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 42.4767\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1033e+03 - output_1_loss: -3.4695e+03 - output_2_loss: 2366.1565\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0506e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 416.5147\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6880e+03 - output_1_loss: -3.4684e+03 - output_2_loss: 780.3602\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.0453e+03 - output_1_loss: -5.5029e+03 - output_2_loss: 4457.6411\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19086.9375 - output_1_loss: -5.5138e+03 - output_2_loss: 24600.7422\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9446.9863 - output_1_loss: -5.5186e+03 - output_2_loss: 14965.5938\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.6483e+03 - output_1_loss: -5.4944e+03 - output_2_loss: 846.0557\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 89705.6641 - output_1_loss: -5.5470e+03 - output_2_loss: 95252.6406\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 93929.7812 - output_1_loss: -7.0074e+03 - output_2_loss: 100937.1484\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2457e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 221.7487\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3700e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 96.6187\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4347e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 31.1001\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15345.0195 - output_1_loss: -6.9629e+03 - output_2_loss: 22307.9453\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1616e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 305.8840\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11806.9473 - output_1_loss: -5.5098e+03 - output_2_loss: 17316.7051\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 40510.5352 - output_1_loss: -5.5347e+03 - output_2_loss: 46045.2227\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2450e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 222.4544\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1723e+03 - output_1_loss: -3.4699e+03 - output_2_loss: 2297.5320\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6577e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 810.9034\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2051.2642 - output_1_loss: -5.5093e+03 - output_2_loss: 7560.5859\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17919.7070 - output_1_loss: -5.5239e+03 - output_2_loss: 23443.6426\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.6746e+03 - output_1_loss: -5.4951e+03 - output_2_loss: 820.4864\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 86317.5469 - output_1_loss: -5.5391e+03 - output_2_loss: 91856.6641\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 102767.2969 - output_1_loss: -7.0058e+03 - output_2_loss: 109773.1172\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3155.3906 - output_1_loss: -5.5051e+03 - output_2_loss: 8660.4990\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3828e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 83.6993\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4383e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 27.8565\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1244e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 343.6405\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15923.5781 - output_1_loss: -6.9641e+03 - output_2_loss: 22887.6836\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.5004e+03 - output_1_loss: -3.4693e+03 - output_2_loss: 1968.8292\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3291e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 136.6942\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27244.3457 - output_1_loss: -5.5273e+03 - output_2_loss: 32771.6523\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.7799e+03 - output_1_loss: -5.4965e+03 - output_2_loss: 716.6423\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 121831.3594 - output_1_loss: -5.5561e+03 - output_2_loss: 127387.4219\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 133087.2031 - output_1_loss: -7.0173e+03 - output_2_loss: 140104.5000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14280.1494 - output_1_loss: -5.5079e+03 - output_2_loss: 19788.0117\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0238e+02 - output_1_loss: -5.5022e+03 - output_2_loss: 5299.7856\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3998e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 66.4696\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3442e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 122.7950\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8448e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 622.8399\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0254e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 442.7180\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4667.3423 - output_1_loss: -5.5129e+03 - output_2_loss: 10180.2695\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 163977.2812 - output_1_loss: -7.0224e+03 - output_2_loss: 170999.7031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14274.9707 - output_1_loss: -6.9591e+03 - output_2_loss: 21234.0254\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1423e+03 - output_1_loss: -5.5002e+03 - output_2_loss: 3357.8943\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4021e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 64.4759\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0560e+03 - output_1_loss: -3.4687e+03 - output_2_loss: 1412.7814\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2489e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 218.6108\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3358e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 130.0202\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9471e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 520.6562\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8866e+03 - output_1_loss: -3.4689e+03 - output_2_loss: 582.3044\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7726.5659 - output_1_loss: -5.5152e+03 - output_2_loss: 13241.7910\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 36639.1758 - output_1_loss: -5.5286e+03 - output_2_loss: 42167.7891\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.9542e+03 - output_1_loss: -5.4961e+03 - output_2_loss: 541.8439\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 87872.3828 - output_1_loss: -5.5417e+03 - output_2_loss: 93414.1172\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16577.4570 - output_1_loss: -5.5166e+03 - output_2_loss: 22094.0625\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1268e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 341.2457\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1276e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 340.4587\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7702e+03 - output_1_loss: -3.4691e+03 - output_2_loss: 698.8941\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11751.0459 - output_1_loss: -5.5191e+03 - output_2_loss: 17270.1035\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 64313.0859 - output_1_loss: -5.5312e+03 - output_2_loss: 69844.2422\n",
      "Train on 11 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 - 0s - loss: 178362.3750 - output_1_loss: -7.0365e+03 - output_2_loss: 185398.9062\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13033.4629 - output_1_loss: -6.9588e+03 - output_2_loss: 19992.2480\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 189.3975 - output_1_loss: -5.5066e+03 - output_2_loss: 5696.0059\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4502e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 15.8634\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1799e+03 - output_1_loss: -3.4701e+03 - output_2_loss: 1290.1766\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1997e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 266.9795\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24661.4355 - output_1_loss: -5.5181e+03 - output_2_loss: 30179.4863\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.0299e+03 - output_1_loss: -5.4947e+03 - output_2_loss: 464.8393\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3155e+03 - output_1_loss: -3.4694e+03 - output_2_loss: 1153.8901\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8228.7051 - output_1_loss: -5.5012e+03 - output_2_loss: 13729.9502\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20880.4785 - output_1_loss: -5.5263e+03 - output_2_loss: 26406.7676\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 44041.3359 - output_1_loss: -5.5172e+03 - output_2_loss: 49558.4883\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14625.6670 - output_1_loss: -6.9638e+03 - output_2_loss: 21589.5000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7657e+03 - output_1_loss: -3.4692e+03 - output_2_loss: 703.4939\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3016e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 165.0847\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2914e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 174.8303\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21690.4180 - output_1_loss: -5.5131e+03 - output_2_loss: 27203.5234\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.0961e+03 - output_1_loss: -5.4942e+03 - output_2_loss: 398.0994\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 201996.3906 - output_1_loss: -7.0459e+03 - output_2_loss: 209042.3125\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.5868e+03 - output_1_loss: -5.5001e+03 - output_2_loss: 2913.2632\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28946.8496 - output_1_loss: -5.5113e+03 - output_2_loss: 34458.1055\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4331e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 33.3479\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.2155e+03 - output_1_loss: -3.4696e+03 - output_2_loss: 1254.1141\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9391e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 529.0442\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7579e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 710.2274\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6978.4458 - output_1_loss: -5.5068e+03 - output_2_loss: 12485.2812\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 32083.4570 - output_1_loss: -5.5336e+03 - output_2_loss: 37617.0156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.1649e+03 - output_1_loss: -5.4936e+03 - output_2_loss: 328.7041\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 211090.0781 - output_1_loss: -7.0486e+03 - output_2_loss: 218138.6406\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5249.8447 - output_1_loss: -6.9494e+03 - output_2_loss: 12199.2402\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3124e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 154.1926\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3377e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 128.8117\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20317.0762 - output_1_loss: -5.5155e+03 - output_2_loss: 25832.5684\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7069e+03 - output_1_loss: -3.4688e+03 - output_2_loss: 761.8670\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1780.9805 - output_1_loss: -5.5089e+03 - output_2_loss: 7289.9214\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17661.5469 - output_1_loss: -5.5078e+03 - output_2_loss: 23169.3145\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4461e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 20.0159\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.2264e+03 - output_1_loss: -3.4699e+03 - output_2_loss: 1243.4988\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3262e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 140.2458\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3878e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 78.6613\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9423e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 525.1787\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3963.1060 - output_1_loss: -5.4969e+03 - output_2_loss: 9460.0244\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 18256.8203 - output_1_loss: -5.5191e+03 - output_2_loss: 23775.9570\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1227.8784 - output_1_loss: -5.5091e+03 - output_2_loss: 6737.0283\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 219994.2188 - output_1_loss: -7.0502e+03 - output_2_loss: 227044.4531\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1330.8667 - output_1_loss: -6.9448e+03 - output_2_loss: 8275.6494\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12329.6172 - output_1_loss: -5.5097e+03 - output_2_loss: 17839.2910\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4491e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 16.9943\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1240e+03 - output_1_loss: -3.4721e+03 - output_2_loss: 2348.0967\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16262.4902 - output_1_loss: -5.5123e+03 - output_2_loss: 21774.8203\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9466e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 521.2778\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3148e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 152.0302\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3282.6895 - output_1_loss: -5.4954e+03 - output_2_loss: 8778.0908\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1607.8379 - output_1_loss: -5.5085e+03 - output_2_loss: 7116.2998\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17461.9336 - output_1_loss: -5.5171e+03 - output_2_loss: 22979.0586\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4528e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 13.1219\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8776e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 591.0576\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9994e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 467.9889\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24066.4102 - output_1_loss: -5.5263e+03 - output_2_loss: 29592.6875\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3047e+02 - output_1_loss: -5.5007e+03 - output_2_loss: 5270.2773\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4556e+03 - output_1_loss: -3.4699e+03 - output_2_loss: 1014.2401\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 164603.2656 - output_1_loss: -7.0242e+03 - output_2_loss: 171627.4375\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3213.5059 - output_1_loss: -5.4950e+03 - output_2_loss: 8708.4902\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.8375e+03 - output_1_loss: -6.9404e+03 - output_2_loss: 5102.9097\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.7460e+03 - output_1_loss: -3.4706e+03 - output_2_loss: 1724.6000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11836.7129 - output_1_loss: -5.5084e+03 - output_2_loss: 17345.1152\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3612e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 105.0412\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1707e+03 - output_1_loss: -3.4701e+03 - output_2_loss: 1299.4117\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0843e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 382.6995\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1361.9688 - output_1_loss: -5.5033e+03 - output_2_loss: 6865.2261\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20311.0352 - output_1_loss: -5.5161e+03 - output_2_loss: 25827.1484\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4309e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 35.5544\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 32702.2930 - output_1_loss: -5.5316e+03 - output_2_loss: 38233.9023\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6547.4722 - output_1_loss: -5.5048e+03 - output_2_loss: 12052.2852\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3206.2886 - output_1_loss: -5.5108e+03 - output_2_loss: 8717.0977\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8852e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 582.6217\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3668e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 99.2489\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1249e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 342.4652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 197742.2031 - output_1_loss: -7.0406e+03 - output_2_loss: 204782.7656\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1446e+03 - output_1_loss: -5.4921e+03 - output_2_loss: 3347.4573\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.7749e+03 - output_1_loss: -6.9381e+03 - output_2_loss: 3163.2034\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.2152e+03 - output_1_loss: -3.4717e+03 - output_2_loss: 2256.5212\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28206.6562 - output_1_loss: -5.5238e+03 - output_2_loss: 33730.4062\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4087e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 58.0146\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8734e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 593.6072\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6046.1191 - output_1_loss: -5.5127e+03 - output_2_loss: 11558.8662\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 130.5112 - output_1_loss: -5.4996e+03 - output_2_loss: 5630.1099\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9984e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 468.9416\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2045e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 262.5990\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.0783e+03 - output_1_loss: -3.4716e+03 - output_2_loss: 2393.3342\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 35880.5469 - output_1_loss: -5.5243e+03 - output_2_loss: 41404.8164\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 37570.0078 - output_1_loss: -5.5342e+03 - output_2_loss: 43104.2344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7360.3750 - output_1_loss: -5.5147e+03 - output_2_loss: 12875.1191\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3828e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 84.1239\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3998e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 66.0041\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9076e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 559.8669\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 243931.8281 - output_1_loss: -7.0533e+03 - output_2_loss: 250985.1094\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7592e+03 - output_1_loss: -5.4924e+03 - output_2_loss: 2733.2666\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1417e+03 - output_1_loss: -6.9476e+03 - output_2_loss: 5805.8984\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14320.2988 - output_1_loss: -5.5201e+03 - output_2_loss: 19840.4434\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1532e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 313.5916\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 45831.9844 - output_1_loss: -5.5383e+03 - output_2_loss: 51370.2461\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0326e+03 - output_1_loss: -5.5009e+03 - output_2_loss: 3468.3274\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3562e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 110.9028\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3695e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 97.0050\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3349e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 131.2643\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.3423e+03 - output_1_loss: -5.4990e+03 - output_2_loss: 4156.7231\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.0967e+03 - output_1_loss: -3.4715e+03 - output_2_loss: 2374.8301\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 40819.2227 - output_1_loss: -5.5270e+03 - output_2_loss: 46346.1914\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4856.8589 - output_1_loss: -5.4980e+03 - output_2_loss: 10354.8916\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9351e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 532.2305\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 228598.0469 - output_1_loss: -7.0422e+03 - output_2_loss: 235640.2031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1278.0645 - output_1_loss: -6.9498e+03 - output_2_loss: 8227.8711\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17067.4023 - output_1_loss: -5.5229e+03 - output_2_loss: 22590.2715\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -9.0950e+02 - output_1_loss: -5.5048e+03 - output_2_loss: 4595.2466\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1549e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 312.6181\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2961e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 171.1959\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3741e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 91.6572\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24149.4883 - output_1_loss: -5.5168e+03 - output_2_loss: 29666.3047\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3945e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 72.1236\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9572e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 509.9759\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.5930e+03 - output_1_loss: -5.4926e+03 - output_2_loss: 2899.6506\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1373e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 330.3732\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2262e+03 - output_1_loss: -3.4676e+03 - output_2_loss: 241.4270\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3966e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 68.9227\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.4570e+03 - output_1_loss: -3.4709e+03 - output_2_loss: 2013.9222\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 45181.8516 - output_1_loss: -5.5291e+03 - output_2_loss: 50710.9336\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4968.7271 - output_1_loss: -5.5057e+03 - output_2_loss: 10474.4463\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 224489.4531 - output_1_loss: -7.0465e+03 - output_2_loss: 231535.9531\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5379.3452 - output_1_loss: -6.9560e+03 - output_2_loss: 12335.3525\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10551.2646 - output_1_loss: -5.5121e+03 - output_2_loss: 16063.3721\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7347.7549 - output_1_loss: -5.5137e+03 - output_2_loss: 12861.4463\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4101e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 56.3694\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0155e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 451.3214\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0945e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 373.3459\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1562e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 311.6355\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0261e+03 - output_1_loss: -3.4696e+03 - output_2_loss: 1443.5077\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 36860.1250 - output_1_loss: -5.5278e+03 - output_2_loss: 42387.9023\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 221899.9688 - output_1_loss: -7.0496e+03 - output_2_loss: 228949.5156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2578e+03 - output_1_loss: -5.4983e+03 - output_2_loss: 2240.4343\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7829.7651 - output_1_loss: -5.5088e+03 - output_2_loss: 13338.5537\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9163e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 551.5801\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3995e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 66.6059\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 36567.5781 - output_1_loss: -5.5222e+03 - output_2_loss: 42089.7656\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4869.8550 - output_1_loss: -5.5077e+03 - output_2_loss: 10377.5605\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9908.2148 - output_1_loss: -6.9614e+03 - output_2_loss: 16869.6582\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1037.8481 - output_1_loss: -5.5020e+03 - output_2_loss: 6539.8267\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3872e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 79.3587\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0651e+03 - output_1_loss: -5.5025e+03 - output_2_loss: 3437.3281\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1472e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 320.2820\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1606e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 307.3440\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3129e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 154.0151\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.2147e+03 - output_1_loss: -3.4695e+03 - output_2_loss: 1254.8375\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20558.6289 - output_1_loss: -5.5124e+03 - output_2_loss: 26071.0000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4510.0410 - output_1_loss: -5.5075e+03 - output_2_loss: 10017.5068\n",
      "Train on 11 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 - 0s - loss: 156910.3906 - output_1_loss: -7.0210e+03 - output_2_loss: 163931.3594\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5895.2666 - output_1_loss: -5.5071e+03 - output_2_loss: 11402.3496\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2998e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 167.2954\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0082e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 459.2969\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1173e+03 - output_1_loss: -3.4676e+03 - output_2_loss: 350.2975\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 39462.6953 - output_1_loss: -5.5269e+03 - output_2_loss: 44989.5508\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7689e+03 - output_1_loss: -3.4680e+03 - output_2_loss: 699.0632\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23479.1914 - output_1_loss: -6.9748e+03 - output_2_loss: 30453.9453\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3658.4497 - output_1_loss: -5.5096e+03 - output_2_loss: 9168.0908\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4446.4746 - output_1_loss: -5.5082e+03 - output_2_loss: 9954.6338\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1275.8076 - output_1_loss: -5.5096e+03 - output_2_loss: 6785.4438\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1913e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 276.1456\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2485e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 218.6367\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4114e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 54.7603\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24157.1621 - output_1_loss: -5.5202e+03 - output_2_loss: 29677.4082\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8448e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 623.1357\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 178313.8281 - output_1_loss: -7.0339e+03 - output_2_loss: 185347.7344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 35651.9531 - output_1_loss: -6.9835e+03 - output_2_loss: 42635.5000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14779.5781 - output_1_loss: -5.5207e+03 - output_2_loss: 20300.2852\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1847e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 281.7239\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3716.8291 - output_1_loss: -5.5123e+03 - output_2_loss: 9229.1416\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0796e+03 - output_1_loss: -3.4683e+03 - output_2_loss: 388.6721\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2216e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 245.1319\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2695e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 197.3815\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 34609.0352 - output_1_loss: -5.5233e+03 - output_2_loss: 40132.2891\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6280.9727 - output_1_loss: -5.5133e+03 - output_2_loss: 11794.2822\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6557.1348 - output_1_loss: -5.5123e+03 - output_2_loss: 12069.4336\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9765e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 491.1729\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23580.2461 - output_1_loss: -5.5247e+03 - output_2_loss: 29104.9824\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2703e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 195.6827\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3684e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 98.4293\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.5587e+03 - output_1_loss: -3.4698e+03 - output_2_loss: 911.0821\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1568e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 310.6494\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2523e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 214.9909\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12859.9004 - output_1_loss: -5.5081e+03 - output_2_loss: 18368.0215\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 147620.8750 - output_1_loss: -7.0199e+03 - output_2_loss: 154640.7656\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 47571.2773 - output_1_loss: -6.9905e+03 - output_2_loss: 54561.7852\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1979e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 269.3795\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.2886e+02 - output_1_loss: -5.5026e+03 - output_2_loss: 5073.7290\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3649e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 100.6545\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 32977.1680 - output_1_loss: -5.5261e+03 - output_2_loss: 38503.2500\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1048e+03 - output_1_loss: -3.4706e+03 - output_2_loss: 1365.8285\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0466e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 421.5171\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2399e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 227.3903\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2422.5449 - output_1_loss: -5.5050e+03 - output_2_loss: 7927.5112\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7628.8262 - output_1_loss: -5.5115e+03 - output_2_loss: 13140.3340\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11074.6934 - output_1_loss: -5.5137e+03 - output_2_loss: 16588.3613\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1924e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 274.9510\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4032e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 63.0095\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3675e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 97.9524\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11574.7852 - output_1_loss: -5.5099e+03 - output_2_loss: 17084.6641\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 189727.5625 - output_1_loss: -7.0335e+03 - output_2_loss: 196761.0938\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.5745e+03 - output_1_loss: -3.4715e+03 - output_2_loss: 1896.9965\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 68033.3828 - output_1_loss: -7.0034e+03 - output_2_loss: 75036.7500\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 57.7988 - output_1_loss: -5.5050e+03 - output_2_loss: 5562.7617\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11821.4434 - output_1_loss: -5.5153e+03 - output_2_loss: 17336.7168\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3375e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 129.6331\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 29377.4668 - output_1_loss: -5.5257e+03 - output_2_loss: 34903.1641\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2105e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 256.4930\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3021e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 164.6447\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4058e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 60.6667\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10845.2773 - output_1_loss: -5.5091e+03 - output_2_loss: 16354.4033\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2430.4648 - output_1_loss: -5.5054e+03 - output_2_loss: 7935.8560\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5713.2378 - output_1_loss: -5.5117e+03 - output_2_loss: 11224.9346\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3392e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 127.8454\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 368.6025 - output_1_loss: -5.5061e+03 - output_2_loss: 5874.7305\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3366e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 130.4280\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 148829.1875 - output_1_loss: -7.0096e+03 - output_2_loss: 155838.8438\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1982e+03 - output_1_loss: -3.4695e+03 - output_2_loss: 1271.2812\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3508e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 115.5332\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 65381.4688 - output_1_loss: -6.9992e+03 - output_2_loss: 72380.6484\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14057.6904 - output_1_loss: -5.5163e+03 - output_2_loss: 19574.0391\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1934e+03 - output_1_loss: -3.4704e+03 - output_2_loss: 1277.0366\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22700.3477 - output_1_loss: -5.5215e+03 - output_2_loss: 28221.8867\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2957.6206 - output_1_loss: -5.5112e+03 - output_2_loss: 8468.8320\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1930e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 274.4849\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3858e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 80.8492\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1004e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 367.7844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2176.0435 - output_1_loss: -5.4999e+03 - output_2_loss: 7675.8940\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5278.6226 - output_1_loss: -5.5121e+03 - output_2_loss: 10790.7119\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9789.6406 - output_1_loss: -5.5158e+03 - output_2_loss: 15305.4805\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4187e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 47.2399\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.3461e+03 - output_1_loss: -3.4717e+03 - output_2_loss: 2125.5264\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7409.6411 - output_1_loss: -5.5165e+03 - output_2_loss: 12926.1689\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 170618.6250 - output_1_loss: -7.0259e+03 - output_2_loss: 177644.5156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1351e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 1331.9708\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0094e+03 - output_1_loss: -3.4685e+03 - output_2_loss: 459.0982\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 80419.9375 - output_1_loss: -7.0050e+03 - output_2_loss: 87424.9688\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7026.4028 - output_1_loss: -5.5141e+03 - output_2_loss: 12540.5000\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4417e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 23.9183\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10500.7666 - output_1_loss: -5.5141e+03 - output_2_loss: 16014.8467\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21768.6660 - output_1_loss: -5.5216e+03 - output_2_loss: 27290.2734\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.5243e+02 - output_1_loss: -3.4729e+03 - output_2_loss: 3120.5195\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3005e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 165.8972\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4153e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 50.6897\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.7947e+02 - output_1_loss: -5.5014e+03 - output_2_loss: 4821.8867\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1828.7007 - output_1_loss: -5.4999e+03 - output_2_loss: 7328.6177\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.9498e+03 - output_1_loss: -3.4698e+03 - output_2_loss: 1519.9872\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9488e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 519.8010\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11164.9541 - output_1_loss: -5.5187e+03 - output_2_loss: 16683.6250\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4469e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 18.5513\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9926.8438 - output_1_loss: -5.5131e+03 - output_2_loss: 15439.9570\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26546.8047 - output_1_loss: -5.5269e+03 - output_2_loss: 32073.6992\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9306.8926 - output_1_loss: -5.5177e+03 - output_2_loss: 14824.5742\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 145488.6250 - output_1_loss: -7.0260e+03 - output_2_loss: 152514.6094\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 91758.1562 - output_1_loss: -7.0109e+03 - output_2_loss: 98769.0703\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9171e+03 - output_1_loss: -3.4684e+03 - output_2_loss: 551.3289\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11183.4062 - output_1_loss: -5.5136e+03 - output_2_loss: 16697.0371\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.7845e+03 - output_1_loss: -3.4694e+03 - output_2_loss: 1684.9269\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9709e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 497.6948\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4006e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 65.9094\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5605.5063 - output_1_loss: -5.5089e+03 - output_2_loss: 11114.3701\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2009.0562 - output_1_loss: -5.5019e+03 - output_2_loss: 7510.9268\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3467e+03 - output_1_loss: -3.4688e+03 - output_2_loss: 1122.1163\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19766.2754 - output_1_loss: -5.5244e+03 - output_2_loss: 25290.6895\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4458e+03 - output_1_loss: -3.4654e+03 - output_2_loss: 19.5543\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27903.2930 - output_1_loss: -5.5223e+03 - output_2_loss: 33425.6289\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2639.9585 - output_1_loss: -5.5017e+03 - output_2_loss: 8141.6987\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 129445.8359 - output_1_loss: -7.0233e+03 - output_2_loss: 136469.1250\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.5535e+03 - output_1_loss: -3.4694e+03 - output_2_loss: 915.9596\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1666e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 301.3155\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 95033.7812 - output_1_loss: -7.0114e+03 - output_2_loss: 102045.2031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1420e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 323.5871\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4485e+03 - output_1_loss: -3.4654e+03 - output_2_loss: 16.9100\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27067.7812 - output_1_loss: -5.5270e+03 - output_2_loss: 32594.8262\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 619.3167 - output_1_loss: -3.4739e+03 - output_2_loss: 4093.2485\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 913.0068 - output_1_loss: -5.5003e+03 - output_2_loss: 6413.3418\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1900.4766 - output_1_loss: -5.5008e+03 - output_2_loss: 7401.3262\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 129620.6953 - output_1_loss: -7.0237e+03 - output_2_loss: 136644.3906\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6215e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 846.6082\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9829e+03 - output_1_loss: -3.4684e+03 - output_2_loss: 485.5144\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28420.2148 - output_1_loss: -5.5278e+03 - output_2_loss: 33947.9727\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1658e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 299.7052\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21035.7051 - output_1_loss: -5.5186e+03 - output_2_loss: 26554.3359\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1829.3022 - output_1_loss: -5.5049e+03 - output_2_loss: 7334.2271\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0794e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 386.8055\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 95309.0547 - output_1_loss: -7.0030e+03 - output_2_loss: 102312.0547\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 37731.9102 - output_1_loss: -5.5310e+03 - output_2_loss: 43262.8906\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4496e+03 - output_1_loss: -3.4654e+03 - output_2_loss: 15.8548\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23811.1602 - output_1_loss: -5.5221e+03 - output_2_loss: 29333.2422\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -9.6187e+02 - output_1_loss: -3.4703e+03 - output_2_loss: 2508.4346\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8534.2559 - output_1_loss: -5.5137e+03 - output_2_loss: 14047.9473\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1017.0103 - output_1_loss: -5.5007e+03 - output_2_loss: 6517.6646\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 124210.9297 - output_1_loss: -7.0208e+03 - output_2_loss: 131231.7500\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8814e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 585.9069\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9808e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 487.3682\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1627e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 304.0851\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27657.7051 - output_1_loss: -5.5254e+03 - output_2_loss: 33183.1094\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7405.6216 - output_1_loss: -5.5149e+03 - output_2_loss: 12920.4805\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.4227e+02 - output_1_loss: -3.4709e+03 - output_2_loss: 2828.6748\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0936e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 371.8213\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 127521.6641 - output_1_loss: -7.0098e+03 - output_2_loss: 134531.4375\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17672.8047 - output_1_loss: -5.5214e+03 - output_2_loss: 23194.2324\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 121673.2422 - output_1_loss: -7.0190e+03 - output_2_loss: 128692.2188\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0647e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 402.0173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21647.6699 - output_1_loss: -5.5113e+03 - output_2_loss: 27158.9883\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4246e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 41.6621\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 38278.8438 - output_1_loss: -5.5345e+03 - output_2_loss: 43813.3242\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 36171.6367 - output_1_loss: -5.5318e+03 - output_2_loss: 41703.4414\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 971.3364 - output_1_loss: -5.5032e+03 - output_2_loss: 6474.5786\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.3544e+03 - output_1_loss: -3.4684e+03 - output_2_loss: 2113.9402\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1135e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 352.6763\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0058e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 462.0865\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28687.8887 - output_1_loss: -5.5296e+03 - output_2_loss: 34217.4414\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 122727.0469 - output_1_loss: -7.0171e+03 - output_2_loss: 129744.1172\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2091e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 256.5009\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16352.9941 - output_1_loss: -5.5227e+03 - output_2_loss: 21875.6855\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4146e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 51.8222\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 71800.2812 - output_1_loss: -6.9773e+03 - output_2_loss: 78777.5391\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1906e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 276.3750\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.6451e+03 - output_1_loss: -3.4678e+03 - output_2_loss: 1822.7173\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1292e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 337.0947\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9940e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 474.0572\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 39358.2109 - output_1_loss: -5.5349e+03 - output_2_loss: 44893.1523\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 29195.9062 - output_1_loss: -5.5232e+03 - output_2_loss: 34719.1406\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2276e+03 - output_1_loss: -3.4654e+03 - output_2_loss: 237.8250\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25121.4688 - output_1_loss: -5.5234e+03 - output_2_loss: 30644.9043\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28399.1328 - output_1_loss: -5.5262e+03 - output_2_loss: 33925.3164\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -8.4200e+02 - output_1_loss: -5.4994e+03 - output_2_loss: 4657.3965\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0552e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 413.0083\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 45359.9141 - output_1_loss: -6.9501e+03 - output_2_loss: 52310.0156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 68198.6719 - output_1_loss: -6.9823e+03 - output_2_loss: 75180.9297\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1889e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 277.0254\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25877.0156 - output_1_loss: -5.5288e+03 - output_2_loss: 31405.8379\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3785e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 87.1613\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.9994e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 1467.6641\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9805e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 487.6534\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 44586.4648 - output_1_loss: -5.5373e+03 - output_2_loss: 50123.8086\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 39828.8008 - output_1_loss: -5.5319e+03 - output_2_loss: 45360.7109\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3061e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 160.0892\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 36844.7539 - output_1_loss: -5.5261e+03 - output_2_loss: 42370.8164\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22495.3984 - output_1_loss: -5.5214e+03 - output_2_loss: 28016.7832\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6366e+03 - output_1_loss: -5.4932e+03 - output_2_loss: 2856.6658\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3129e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 153.6561\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 42846.7461 - output_1_loss: -6.9648e+03 - output_2_loss: 49811.5508\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 79006.8438 - output_1_loss: -6.9986e+03 - output_2_loss: 86005.4062\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4445e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 1021.5818\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2156e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 250.0949\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0146e+03 - output_1_loss: -3.4680e+03 - output_2_loss: 453.4760\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 33326.6797 - output_1_loss: -5.5326e+03 - output_2_loss: 38859.3086\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 18844.2402 - output_1_loss: -5.5121e+03 - output_2_loss: 24356.3008\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3216e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 144.8631\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3797e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 86.6046\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25747.4883 - output_1_loss: -5.5164e+03 - output_2_loss: 31263.9082\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 45250.8984 - output_1_loss: -5.5345e+03 - output_2_loss: 50785.4023\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9977e+03 - output_1_loss: -3.4651e+03 - output_2_loss: 467.3880\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2194e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 246.2830\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3402e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 125.6286\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26584.6348 - output_1_loss: -5.5202e+03 - output_2_loss: 32104.8105\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.6148e+03 - output_1_loss: -5.4967e+03 - output_2_loss: 1881.9777\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 36587.6641 - output_1_loss: -5.5324e+03 - output_2_loss: 42120.0430\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 32209.3672 - output_1_loss: -6.9626e+03 - output_2_loss: 39172.0117\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 40644.5469 - output_1_loss: -6.9683e+03 - output_2_loss: 47612.8320\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3662e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 100.2197\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1641e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 302.8722\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2659e+03 - output_1_loss: -3.4654e+03 - output_2_loss: 199.4447\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3692e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 96.6209\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16281.3613 - output_1_loss: -5.5100e+03 - output_2_loss: 21791.3555\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0626e+03 - output_1_loss: -5.5015e+03 - output_2_loss: 3438.9475\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3293e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 137.1030\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23970.4023 - output_1_loss: -6.9588e+03 - output_2_loss: 30929.1992\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 34254.5469 - output_1_loss: -5.5282e+03 - output_2_loss: 39782.7773\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 46809.2656 - output_1_loss: -5.5336e+03 - output_2_loss: 52342.8164\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0560e+03 - output_1_loss: -3.4647e+03 - output_2_loss: 408.6846\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2052e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 261.3447\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23156.3223 - output_1_loss: -5.5201e+03 - output_2_loss: 28676.3828\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 38923.4531 - output_1_loss: -5.5310e+03 - output_2_loss: 44454.4531\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 98012.5391 - output_1_loss: -7.0123e+03 - output_2_loss: 105024.8203\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4104e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 55.5555\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 43654.3203 - output_1_loss: -5.5293e+03 - output_2_loss: 49183.6641\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1889e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 277.9821\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3692e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 96.0255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23072.5703 - output_1_loss: -5.5221e+03 - output_2_loss: 28594.6270\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19583.4980 - output_1_loss: -5.5124e+03 - output_2_loss: 25095.8945\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.5408e+03 - output_1_loss: -5.4962e+03 - output_2_loss: 1955.3827\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3366e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 129.6785\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20107.0312 - output_1_loss: -6.9581e+03 - output_2_loss: 27065.0957\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23542.2207 - output_1_loss: -5.5130e+03 - output_2_loss: 29055.1875\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4242e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 41.4671\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9111e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 555.9776\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2101e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 256.9419\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20499.0977 - output_1_loss: -5.4967e+03 - output_2_loss: 25995.7734\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15153.2402 - output_1_loss: -5.5131e+03 - output_2_loss: 20666.3379\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4718e+03 - output_1_loss: -5.4971e+03 - output_2_loss: 2025.3632\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3445e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 121.6775\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 42237.4922 - output_1_loss: -6.9742e+03 - output_2_loss: 49211.7266\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14249.3125 - output_1_loss: -6.9581e+03 - output_2_loss: 21207.4062\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16424.2344 - output_1_loss: -5.5073e+03 - output_2_loss: 21931.4922\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20600.9199 - output_1_loss: -5.5058e+03 - output_2_loss: 26106.7363\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4255e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 40.1278\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7703e+03 - output_1_loss: -3.4678e+03 - output_2_loss: 697.5087\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3379e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 127.3756\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3706e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 95.3956\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25738.5215 - output_1_loss: -5.5251e+03 - output_2_loss: 31263.5918\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.6890e+03 - output_1_loss: -5.4990e+03 - output_2_loss: 1809.9249\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 37319.8281 - output_1_loss: -6.9719e+03 - output_2_loss: 44291.7539\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3534e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 112.3754\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13729.0166 - output_1_loss: -5.5014e+03 - output_2_loss: 19230.4121\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4346e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 31.0913\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 29612.0742 - output_1_loss: -5.5225e+03 - output_2_loss: 35134.6133\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3472e+03 - output_1_loss: -3.4653e+03 - output_2_loss: 118.0966\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10317.0195 - output_1_loss: -5.5118e+03 - output_2_loss: 15828.8604\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6553e+03 - output_1_loss: -3.4693e+03 - output_2_loss: 814.0457\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9410.8965 - output_1_loss: -6.9545e+03 - output_2_loss: 16365.3525\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16100.0312 - output_1_loss: -5.5146e+03 - output_2_loss: 21614.6387\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 37609.4609 - output_1_loss: -6.9726e+03 - output_2_loss: 44582.0156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1344e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 331.1570\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3913e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 74.2588\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23801.2520 - output_1_loss: -5.5246e+03 - output_2_loss: 29325.8945\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 35501.2734 - output_1_loss: -5.5286e+03 - output_2_loss: 41029.8789\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4079e+03 - output_1_loss: -5.5010e+03 - output_2_loss: 2093.1299\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4356e+03 - output_1_loss: -3.4653e+03 - output_2_loss: 29.7446\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9497.9023 - output_1_loss: -6.9559e+03 - output_2_loss: 16453.8418\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19014.1738 - output_1_loss: -5.5184e+03 - output_2_loss: 24532.5566\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4125e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 53.9808\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4254e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 40.1486\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4269e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 38.8630\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22154.1953 - output_1_loss: -5.5240e+03 - output_2_loss: 27678.2383\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 39153.6055 - output_1_loss: -5.5303e+03 - output_2_loss: 44683.8867\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0939e+03 - output_1_loss: -3.4674e+03 - output_2_loss: 373.4188\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10841.6689 - output_1_loss: -5.5006e+03 - output_2_loss: 16342.2598\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 63713.0938 - output_1_loss: -6.9944e+03 - output_2_loss: 70707.4844\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.3586e+03 - output_1_loss: -3.4713e+03 - output_2_loss: 2112.7393\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4351e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 30.1272\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 29007.4805 - output_1_loss: -5.5305e+03 - output_2_loss: 34538.0117\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24431.1348 - output_1_loss: -5.5239e+03 - output_2_loss: 29954.9883\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7891.4541 - output_1_loss: -5.5172e+03 - output_2_loss: 13408.6494\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1161e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 351.1761\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3369.3335 - output_1_loss: -6.9343e+03 - output_2_loss: 10303.6064\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4303e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 35.3897\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4251e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 40.4846\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4128e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 53.6159\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 32480.7344 - output_1_loss: -5.5284e+03 - output_2_loss: 38009.0859\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 18212.7891 - output_1_loss: -5.5001e+03 - output_2_loss: 23712.8887\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6719.5747 - output_1_loss: -5.5054e+03 - output_2_loss: 12224.9688\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1596e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 307.5918\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 88989.0000 - output_1_loss: -7.0071e+03 - output_2_loss: 95996.0547\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6888e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 778.2141\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4308e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 35.2856\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7605.9312 - output_1_loss: -5.5075e+03 - output_2_loss: 13113.4316\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12665.7637 - output_1_loss: -5.5046e+03 - output_2_loss: 18170.4121\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4242e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 41.2953\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5715.4028 - output_1_loss: -5.5125e+03 - output_2_loss: 11227.8652\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3602.6094 - output_1_loss: -6.9415e+03 - output_2_loss: 10544.0918\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1620e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 305.1803\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4322e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 33.3413\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2742e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 193.0640\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4314e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 34.3088\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8397.2773 - output_1_loss: -5.5111e+03 - output_2_loss: 13908.3789\n",
      "Train on 11 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 - 0s - loss: 30795.0859 - output_1_loss: -5.5258e+03 - output_2_loss: 36320.8945\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9039.5723 - output_1_loss: -5.4976e+03 - output_2_loss: 14537.2178\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3075.4624 - output_1_loss: -5.4982e+03 - output_2_loss: 8573.6924\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 123600.7344 - output_1_loss: -7.0204e+03 - output_2_loss: 130621.1172\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4417e+03 - output_1_loss: -3.4690e+03 - output_2_loss: 1027.2437\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4026.7534 - output_1_loss: -6.9522e+03 - output_2_loss: 10978.9883\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16766.5566 - output_1_loss: -5.5147e+03 - output_2_loss: 22281.2617\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1675e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 299.5209\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4362e+03 - output_1_loss: -3.4654e+03 - output_2_loss: 29.2156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8321e+03 - output_1_loss: -3.4688e+03 - output_2_loss: 636.6849\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2017e+03 - output_1_loss: -3.4676e+03 - output_2_loss: 265.9252\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6477.8120 - output_1_loss: -5.5131e+03 - output_2_loss: 11990.8750\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5252.3535 - output_1_loss: -5.4937e+03 - output_2_loss: 10746.0439\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3002.3730 - output_1_loss: -5.5015e+03 - output_2_loss: 8503.8281\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5656.1436 - output_1_loss: -3.4783e+03 - output_2_loss: 9134.4248\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4326e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 33.0943\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13934.4121 - output_1_loss: -5.5190e+03 - output_2_loss: 19453.4551\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20469.4551 - output_1_loss: -5.5081e+03 - output_2_loss: 25977.5059\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9688e+03 - output_1_loss: -3.4685e+03 - output_2_loss: 499.6701\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4992.7974 - output_1_loss: -5.4987e+03 - output_2_loss: 10491.4658\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 150437.4531 - output_1_loss: -7.0269e+03 - output_2_loss: 157464.3125\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8862.4482 - output_1_loss: -6.9605e+03 - output_2_loss: 15822.9385\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5091.0562 - output_1_loss: -5.4985e+03 - output_2_loss: 10589.5312\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2160e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 250.6002\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4255e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 40.6932\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1953e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 271.7359\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28992.9102 - output_1_loss: -5.5216e+03 - output_2_loss: 34514.4844\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11263.3672 - output_1_loss: -5.5153e+03 - output_2_loss: 16778.6934\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 209.2539 - output_1_loss: -5.4925e+03 - output_2_loss: 5701.7988\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2467.1350 - output_1_loss: -3.4746e+03 - output_2_loss: 5941.7739\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4356e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 29.9049\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2179e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 248.9673\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26944.7812 - output_1_loss: -5.5153e+03 - output_2_loss: 32460.0801\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4181e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 48.1904\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2444e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 221.9999\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1828e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 284.4078\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4404.1304 - output_1_loss: -5.4960e+03 - output_2_loss: 9900.1201\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 142375.4688 - output_1_loss: -7.0203e+03 - output_2_loss: 149395.8125\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10399.2383 - output_1_loss: -6.9615e+03 - output_2_loss: 17360.6992\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 130.8242 - output_1_loss: -5.4948e+03 - output_2_loss: 5625.6479\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2758.5312 - output_1_loss: -5.5019e+03 - output_2_loss: 8260.3906\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 31603.1562 - output_1_loss: -5.5266e+03 - output_2_loss: 37129.7734\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8932e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 575.4412\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6053.9478 - output_1_loss: -5.4903e+03 - output_2_loss: 11544.2344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4014e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 65.0217\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1695e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 297.6947\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 129500.0156 - output_1_loss: -7.0165e+03 - output_2_loss: 136516.5469\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8129.8877 - output_1_loss: -3.4788e+03 - output_2_loss: 11608.6748\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4422e+03 - output_1_loss: -3.4653e+03 - output_2_loss: 23.1044\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2095e+01 - output_1_loss: -5.4976e+03 - output_2_loss: 5465.4844\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3067.6230 - output_1_loss: -5.5037e+03 - output_2_loss: 8571.3281\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20332.7969 - output_1_loss: -5.5107e+03 - output_2_loss: 25843.4668\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2122e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 254.7350\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 35061.8828 - output_1_loss: -5.5302e+03 - output_2_loss: 40592.1016\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3199e+03 - output_1_loss: -3.4700e+03 - output_2_loss: 1150.1135\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6255.3179 - output_1_loss: -5.5033e+03 - output_2_loss: 11758.6562\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4680.9727 - output_1_loss: -5.4858e+03 - output_2_loss: 10166.7783\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6586e+03 - output_1_loss: -3.4690e+03 - output_2_loss: 810.3340\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 837.5723 - output_1_loss: -6.9363e+03 - output_2_loss: 7773.8833\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10969.8379 - output_1_loss: -3.4801e+03 - output_2_loss: 14449.9629\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.8738e+03 - output_1_loss: -5.4958e+03 - output_2_loss: 3621.9929\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4309e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 35.0178\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 38451.7656 - output_1_loss: -5.5332e+03 - output_2_loss: 43984.9336\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.9917e+03 - output_1_loss: -3.4705e+03 - output_2_loss: 1478.8467\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -9.0472e+02 - output_1_loss: -3.4721e+03 - output_2_loss: 2567.3936\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 136503.4688 - output_1_loss: -7.0206e+03 - output_2_loss: 143524.1094\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4405e+03 - output_1_loss: -3.4653e+03 - output_2_loss: 24.7943\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1565.8379 - output_1_loss: -5.4981e+03 - output_2_loss: 7063.9673\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20949.6641 - output_1_loss: -5.5150e+03 - output_2_loss: 26464.6895\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2581e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 208.0989\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.8456e+03 - output_1_loss: -5.4972e+03 - output_2_loss: 3651.6677\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4149.8066 - output_1_loss: -5.5002e+03 - output_2_loss: 9649.9727\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4881.6748 - output_1_loss: -5.4992e+03 - output_2_loss: 10380.8623\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 38993.0000 - output_1_loss: -5.5310e+03 - output_2_loss: 44524.0469\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0235e+02 - output_1_loss: -6.9380e+03 - output_2_loss: 6635.6763\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 417.5334 - output_1_loss: -3.4736e+03 - output_2_loss: 3891.1399\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8848.5732 - output_1_loss: -3.4785e+03 - output_2_loss: 12327.0820\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4381e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 27.0665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4318e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 34.0075\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9574e+03 - output_1_loss: -5.4959e+03 - output_2_loss: 2538.4680\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8320e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 635.1604\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 40078.5820 - output_1_loss: -5.5308e+03 - output_2_loss: 45609.3516\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 112477.9922 - output_1_loss: -7.0106e+03 - output_2_loss: 119488.5547\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1845e+03 - output_1_loss: -6.9387e+03 - output_2_loss: 5754.2578\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1675.5537 - output_1_loss: -5.4992e+03 - output_2_loss: 7174.7627\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13354.8223 - output_1_loss: -5.5083e+03 - output_2_loss: 18863.0781\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2448e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 221.8829\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3762.0405 - output_1_loss: -5.5000e+03 - output_2_loss: 9262.0059\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6309.6426 - output_1_loss: -5.5046e+03 - output_2_loss: 11814.2070\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -8.0987e+02 - output_1_loss: -3.4703e+03 - output_2_loss: 2660.4312\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 117057.8672 - output_1_loss: -7.0124e+03 - output_2_loss: 124070.2500\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9922.8125 - output_1_loss: -3.4786e+03 - output_2_loss: 13401.3877\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4386e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 26.6330\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 368.6724 - output_1_loss: -6.9455e+03 - output_2_loss: 7314.1973\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1542.3652 - output_1_loss: -5.5008e+03 - output_2_loss: 7043.1826\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3989e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 67.6695\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0949e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 372.9554\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.8345e+03 - output_1_loss: -5.4981e+03 - output_2_loss: 1663.5649\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7333e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 734.8328\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5721.4346 - output_1_loss: -5.5069e+03 - output_2_loss: 11228.2920\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 362.7041 - output_1_loss: -5.5012e+03 - output_2_loss: 5863.8911\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 42870.1094 - output_1_loss: -5.5321e+03 - output_2_loss: 48402.2266\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11009.2520 - output_1_loss: -5.5101e+03 - output_2_loss: 16519.3379\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2497e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 217.6040\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5941.6602 - output_1_loss: -5.5116e+03 - output_2_loss: 11453.2656\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3865e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 80.2690\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.8811e+03 - output_1_loss: -5.5030e+03 - output_2_loss: 3621.9041\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5550.3623 - output_1_loss: -5.5080e+03 - output_2_loss: 11058.3760\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.4388e+03 - output_1_loss: -3.4689e+03 - output_2_loss: 2030.0977\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 112878.2578 - output_1_loss: -7.0140e+03 - output_2_loss: 119892.2734\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5819.0137 - output_1_loss: -3.4691e+03 - output_2_loss: 9288.1621\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.7266e+03 - output_1_loss: -6.9341e+03 - output_2_loss: 5207.5737\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2301e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 236.5362\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0518e+03 - output_1_loss: -3.4704e+03 - output_2_loss: 1418.6056\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6609.1299 - output_1_loss: -5.5124e+03 - output_2_loss: 12121.5566\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8666e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 600.3607\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3777e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 89.0724\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.0991e+03 - output_1_loss: -5.4944e+03 - output_2_loss: 4395.3237\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26294.1016 - output_1_loss: -5.5130e+03 - output_2_loss: 31807.1328\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.7458e+03 - output_1_loss: -3.4688e+03 - output_2_loss: 1722.9231\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7581.2637 - output_1_loss: -5.5070e+03 - output_2_loss: 13088.2754\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 99633.3359 - output_1_loss: -7.0096e+03 - output_2_loss: 106642.9766\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5012.9902 - output_1_loss: -3.4699e+03 - output_2_loss: 8482.8633\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2526e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 213.8373\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1439e+03 - output_1_loss: -5.5044e+03 - output_2_loss: 4360.4692\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9098e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 556.9252\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1278e+02 - output_1_loss: -5.4986e+03 - output_2_loss: 5285.8403\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.2561e+03 - output_1_loss: -5.4921e+03 - output_2_loss: 4236.0205\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.8398e+03 - output_1_loss: -6.9336e+03 - output_2_loss: 5093.8018\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3073.0874 - output_1_loss: -3.4687e+03 - output_2_loss: 6541.7505\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9416e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 525.5270\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8655.8389 - output_1_loss: -5.5140e+03 - output_2_loss: 14169.8096\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4098e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 56.3645\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2837e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 182.4114\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 52323.5664 - output_1_loss: -5.5380e+03 - output_2_loss: 57861.5352\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2368e+03 - output_1_loss: -5.4959e+03 - output_2_loss: 2259.0786\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.0567e+03 - output_1_loss: -3.4712e+03 - output_2_loss: 2414.5134\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4180.3037 - output_1_loss: -5.5064e+03 - output_2_loss: 9686.7178\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 37707.3086 - output_1_loss: -6.9491e+03 - output_2_loss: 44656.4102\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2672.0769 - output_1_loss: -3.4680e+03 - output_2_loss: 6140.0308\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9719e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 494.5956\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6813.0366 - output_1_loss: -5.5148e+03 - output_2_loss: 12327.8213\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9511e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 515.4209\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.7546e+02 - output_1_loss: -5.5041e+03 - output_2_loss: 4828.6362\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.3741e+03 - output_1_loss: -6.9394e+03 - output_2_loss: 5565.3027\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11263.3945 - output_1_loss: -5.5185e+03 - output_2_loss: 16781.9355\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3769e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 89.9279\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3118e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 154.6741\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8657e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 602.1826\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 32716.4062 - output_1_loss: -5.5273e+03 - output_2_loss: 38243.6836\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4176e+03 - output_1_loss: -3.4694e+03 - output_2_loss: 1051.7843\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0537e+03 - output_1_loss: -5.4888e+03 - output_2_loss: 2435.1064\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.8826e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 1584.5024\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2888.8062 - output_1_loss: -5.5098e+03 - output_2_loss: 8398.6045\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 39898.8750 - output_1_loss: -6.9754e+03 - output_2_loss: 46874.3164\n",
      "Train on 11 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 - 0s - loss: 1531.2378 - output_1_loss: -3.4713e+03 - output_2_loss: 5002.5239\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3601.3081 - output_1_loss: -5.5097e+03 - output_2_loss: 9110.9883\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3747e+03 - output_1_loss: -3.4668e+03 - output_2_loss: 92.1087\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7615e+03 - output_1_loss: -5.4966e+03 - output_2_loss: 2735.0969\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2824e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 184.6835\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1640e+03 - output_1_loss: -3.4702e+03 - output_2_loss: 1306.1908\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1514e+03 - output_1_loss: -6.9381e+03 - output_2_loss: 3786.7048\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1622e+03 - output_1_loss: -5.4888e+03 - output_2_loss: 2326.5959\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.9963e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 1470.2336\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5460.2153 - output_1_loss: -5.5130e+03 - output_2_loss: 10973.1758\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11922.9336 - output_1_loss: -5.5188e+03 - output_2_loss: 17441.7422\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -8.5992e+01 - output_1_loss: -3.4710e+03 - output_2_loss: 3384.9844\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14896.5117 - output_1_loss: -5.5229e+03 - output_2_loss: 20419.4434\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 35801.3164 - output_1_loss: -5.5333e+03 - output_2_loss: 41334.6602\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8528e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 614.2698\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2357e+03 - output_1_loss: -3.4676e+03 - output_2_loss: 231.9507\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2092e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 258.5176\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15087.2910 - output_1_loss: -6.9328e+03 - output_2_loss: 22020.0801\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2325e+03 - output_1_loss: -5.4894e+03 - output_2_loss: 2256.9294\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 649.7180 - output_1_loss: -3.4720e+03 - output_2_loss: 4121.7017\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1013e+03 - output_1_loss: -5.4968e+03 - output_2_loss: 2395.4990\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6900e+03 - output_1_loss: -3.4680e+03 - output_2_loss: 777.9205\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22317.5645 - output_1_loss: -5.5274e+03 - output_2_loss: 27844.9980\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.2632e+03 - output_1_loss: -6.9354e+03 - output_2_loss: 2672.2361\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9765e+03 - output_1_loss: -3.4686e+03 - output_2_loss: 492.0970\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0615e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 1404.5604\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8225.9277 - output_1_loss: -5.5157e+03 - output_2_loss: 13741.5820\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5932.7280 - output_1_loss: -5.5018e+03 - output_2_loss: 11434.5605\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.5778e+03 - output_1_loss: -5.4927e+03 - output_2_loss: 1914.8755\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26548.0898 - output_1_loss: -5.5261e+03 - output_2_loss: 32074.1934\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0384e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 428.0693\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3726e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 93.4813\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4753.9492 - output_1_loss: -3.4762e+03 - output_2_loss: 8230.1123\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.6853e+03 - output_1_loss: -3.4710e+03 - output_2_loss: 1785.7531\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28468.0352 - output_1_loss: -5.5312e+03 - output_2_loss: 33999.2812\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28885.1367 - output_1_loss: -6.9704e+03 - output_2_loss: 35855.5156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1220e+03 - output_1_loss: -3.4684e+03 - output_2_loss: 1346.4790\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21255.5000 - output_1_loss: -5.5255e+03 - output_2_loss: 26780.9551\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.8959e+03 - output_1_loss: -5.4918e+03 - output_2_loss: 1595.9624\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.5624e+03 - output_1_loss: -5.4941e+03 - output_2_loss: 1931.6854\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27018.6426 - output_1_loss: -5.5251e+03 - output_2_loss: 32543.7207\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2230e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 242.8170\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.8152e+03 - output_1_loss: -6.9291e+03 - output_2_loss: 2113.9287\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3741e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 91.7723\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11907.1748 - output_1_loss: -3.4807e+03 - output_2_loss: 15387.9053\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2245e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 242.5403\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.3825e+02 - output_1_loss: -3.4727e+03 - output_2_loss: 2934.4351\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9482.7168 - output_1_loss: -5.5152e+03 - output_2_loss: 14997.9434\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12973.1035 - output_1_loss: -5.5071e+03 - output_2_loss: 18480.1543\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 66710.4688 - output_1_loss: -6.9946e+03 - output_2_loss: 73705.0938\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3744e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 91.5227\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 21344.6289 - output_1_loss: -3.4852e+03 - output_2_loss: 24829.8457\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2401e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 226.7663\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4907e+03 - output_1_loss: -3.4681e+03 - output_2_loss: 977.4266\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 313.2300 - output_1_loss: -3.4731e+03 - output_2_loss: 3786.3018\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24241.1875 - output_1_loss: -5.5245e+03 - output_2_loss: 29765.6855\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15676.1484 - output_1_loss: -5.5212e+03 - output_2_loss: 21197.3906\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.9785e+03 - output_1_loss: -5.4929e+03 - output_2_loss: 1514.4086\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -6.1705e+02 - output_1_loss: -5.5047e+03 - output_2_loss: 4887.6919\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26824.3359 - output_1_loss: -5.5248e+03 - output_2_loss: 32349.1582\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2674e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 198.1201\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.3692e+03 - output_1_loss: -6.9371e+03 - output_2_loss: 2567.8953\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 30957.8242 - output_1_loss: -3.4897e+03 - output_2_loss: 34447.5234\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1032.2368 - output_1_loss: -3.4728e+03 - output_2_loss: 4505.0010\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19539.3379 - output_1_loss: -5.5238e+03 - output_2_loss: 25063.1055\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3277.0435 - output_1_loss: -5.5099e+03 - output_2_loss: 8786.9365\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20585.2734 - output_1_loss: -5.5210e+03 - output_2_loss: 26106.2441\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23387.6406 - output_1_loss: -6.9401e+03 - output_2_loss: 30327.7383\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2645e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 201.7402\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3866e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 79.1377\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2229e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 244.3292\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9476e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 518.0439\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17647.1387 - output_1_loss: -5.5138e+03 - output_2_loss: 23160.9648\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.0246e+03 - output_1_loss: -5.4894e+03 - output_2_loss: 1464.7906\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 40235.3203 - output_1_loss: -3.4931e+03 - output_2_loss: 43728.4219\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25431.2168 - output_1_loss: -5.5246e+03 - output_2_loss: 30955.8242\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.5368e+03 - output_1_loss: -6.9422e+03 - output_2_loss: 2405.4204\n",
      "Train on 11 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 - 0s - loss: 21244.9375 - output_1_loss: -6.9256e+03 - output_2_loss: 28170.5391\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2929e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 173.7850\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0568e+03 - output_1_loss: -3.4682e+03 - output_2_loss: 411.3524\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1307e+03 - output_1_loss: -3.4678e+03 - output_2_loss: 337.0772\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9713e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 494.6091\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1546e+03 - output_1_loss: -3.4678e+03 - output_2_loss: 2313.2185\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 8212.8320 - output_1_loss: -5.5000e+03 - output_2_loss: 13712.7930\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.7942e+03 - output_1_loss: -5.4962e+03 - output_2_loss: 1701.9781\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3495e+03 - output_1_loss: -5.4923e+03 - output_2_loss: 3142.7305\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11271.1641 - output_1_loss: -5.5049e+03 - output_2_loss: 16776.0801\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24773.7891 - output_1_loss: -5.5249e+03 - output_2_loss: 30298.7266\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 37512.3750 - output_1_loss: -5.5269e+03 - output_2_loss: 43039.2891\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8542e+03 - output_1_loss: -3.4687e+03 - output_2_loss: 614.4839\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22490.5801 - output_1_loss: -3.4757e+03 - output_2_loss: 25966.2754\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8099e+03 - output_1_loss: -3.4690e+03 - output_2_loss: 659.0971\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.3515e+03 - output_1_loss: -6.9389e+03 - output_2_loss: 1587.4618\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 68768.9062 - output_1_loss: -6.9875e+03 - output_2_loss: 75756.4219\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3304e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 135.6409\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9509e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 516.5064\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.3649e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 2103.0237\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5050.5703 - output_1_loss: -5.4995e+03 - output_2_loss: 10550.0732\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.3356e+03 - output_1_loss: -5.4928e+03 - output_2_loss: 1157.2128\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.3575e+03 - output_1_loss: -3.4701e+03 - output_2_loss: 1112.5250\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.4543e+03 - output_1_loss: -5.4917e+03 - output_2_loss: 3037.3845\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 11783.9648 - output_1_loss: -5.5122e+03 - output_2_loss: 17296.1758\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 24268.5684 - output_1_loss: -5.5244e+03 - output_2_loss: 29793.0059\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.6008e+03 - output_1_loss: -6.9425e+03 - output_2_loss: 2341.6924\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20890.4316 - output_1_loss: -5.5109e+03 - output_2_loss: 26401.2988\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3336e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 132.3534\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1746e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 291.6332\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19773.6211 - output_1_loss: -3.4735e+03 - output_2_loss: 23247.1113\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2779.6592 - output_1_loss: -5.5032e+03 - output_2_loss: 8282.8506\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.2811e+03 - output_1_loss: -5.5045e+03 - output_2_loss: 4223.3862\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0094e+03 - output_1_loss: -3.4705e+03 - output_2_loss: 1461.0852\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6697e+03 - output_1_loss: -5.4944e+03 - output_2_loss: 2824.6975\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23909.4805 - output_1_loss: -5.5243e+03 - output_2_loss: 29433.7578\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 79473.3516 - output_1_loss: -6.9925e+03 - output_2_loss: 86465.8281\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0334e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 433.3156\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0773e+02 - output_1_loss: -3.4721e+03 - output_2_loss: 3264.3916\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1894e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 276.3533\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 25695.6172 - output_1_loss: -3.4844e+03 - output_2_loss: 29180.0371\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4941.1606 - output_1_loss: -5.4987e+03 - output_2_loss: 10439.8213\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6482.2710 - output_1_loss: -5.5147e+03 - output_2_loss: 11996.9229\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.8181e+03 - output_1_loss: -6.9375e+03 - output_2_loss: 2119.4287\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20711.7051 - output_1_loss: -5.5180e+03 - output_2_loss: 26229.7266\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3490e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 116.7516\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0565e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 410.0297\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.1912e+02 - output_1_loss: -3.4718e+03 - output_2_loss: 3052.6343\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 542.5483 - output_1_loss: -5.4935e+03 - output_2_loss: 6036.0405\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2302e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 235.4670\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23674.0098 - output_1_loss: -3.4849e+03 - output_2_loss: 27158.9551\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7529e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 714.5781\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3522.6504 - output_1_loss: -5.5100e+03 - output_2_loss: 9032.6816\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2508.0063 - output_1_loss: -5.4927e+03 - output_2_loss: 8000.6904\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14857.5273 - output_1_loss: -5.5015e+03 - output_2_loss: 20358.9922\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 95244.9922 - output_1_loss: -6.9996e+03 - output_2_loss: 102244.6250\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 29085.7188 - output_1_loss: -5.5261e+03 - output_2_loss: 34611.7812\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3531e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 113.3344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.0004e+02 - output_1_loss: -5.5001e+03 - output_2_loss: 5000.0269\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26582.2617 - output_1_loss: -3.4866e+03 - output_2_loss: 30068.8633\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8732e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 593.7410\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.0047e+03 - output_1_loss: -6.9422e+03 - output_2_loss: 2937.5139\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 112.5879 - output_1_loss: -5.4943e+03 - output_2_loss: 5606.9282\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3844.7837 - output_1_loss: -5.4907e+03 - output_2_loss: 9335.4785\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0531e+03 - output_1_loss: -3.4653e+03 - output_2_loss: 412.2707\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.5777e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 1889.3402\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 511.5591 - output_1_loss: -5.4972e+03 - output_2_loss: 6008.7500\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2195e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 247.0602\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5261.2422 - output_1_loss: -5.5115e+03 - output_2_loss: 10772.7373\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 94130.6172 - output_1_loss: -6.9971e+03 - output_2_loss: 101127.7031\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 200.3086 - output_1_loss: -5.5039e+03 - output_2_loss: 5704.2534\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27176.0293 - output_1_loss: -5.5257e+03 - output_2_loss: 32701.7559\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2288.6440 - output_1_loss: -5.4924e+03 - output_2_loss: 7781.0625\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4087e+03 - output_1_loss: -3.4653e+03 - output_2_loss: 56.5910\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.9784e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 1487.5868\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1186.2217 - output_1_loss: -5.5006e+03 - output_2_loss: 6686.7861\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2196e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 247.1799\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12079.5625 - output_1_loss: -3.4708e+03 - output_2_loss: 15550.3555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9218e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 545.1068\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.7703e+03 - output_1_loss: -6.9434e+03 - output_2_loss: 3173.1677\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4105.0635 - output_1_loss: -5.5067e+03 - output_2_loss: 9611.7939\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1290e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 336.6344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4278e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 38.5353\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1254e+03 - output_1_loss: -3.4662e+03 - output_2_loss: 1340.7809\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 395.7124 - output_1_loss: -5.4924e+03 - output_2_loss: 5888.1455\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 80690.6641 - output_1_loss: -6.9270e+03 - output_2_loss: 87617.6406\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3416.3418 - output_1_loss: -5.5105e+03 - output_2_loss: 8926.8750\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8400e+03 - output_1_loss: -3.4687e+03 - output_2_loss: 628.7681\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 7445.6943 - output_1_loss: -3.4678e+03 - output_2_loss: 10913.4561\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9527e+03 - output_1_loss: -3.4667e+03 - output_2_loss: 514.0069\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.1831e+03 - output_1_loss: -6.9480e+03 - output_2_loss: 4764.9819\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 30397.4883 - output_1_loss: -5.5288e+03 - output_2_loss: 35926.2695\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -7.4508e+02 - output_1_loss: -5.4941e+03 - output_2_loss: 4748.9849\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6585e+03 - output_1_loss: -3.4689e+03 - output_2_loss: 810.4109\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1596e+03 - output_1_loss: -3.4679e+03 - output_2_loss: 308.3274\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 316.3423 - output_1_loss: -5.4937e+03 - output_2_loss: 5810.0278\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7952e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 670.3038\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 274.6094 - output_1_loss: -5.4925e+03 - output_2_loss: 5767.1406\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5989.9473 - output_1_loss: -5.5097e+03 - output_2_loss: 11499.6211\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 76260.2578 - output_1_loss: -6.9121e+03 - output_2_loss: 83172.3828\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3259.2197 - output_1_loss: -6.9568e+03 - output_2_loss: 10216.0576\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 40276.3828 - output_1_loss: -5.5352e+03 - output_2_loss: 45811.6094\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1490.1514 - output_1_loss: -5.5025e+03 - output_2_loss: 6992.6479\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1428.7388 - output_1_loss: -5.5052e+03 - output_2_loss: 6933.9502\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1473e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 319.3578\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4952.4321 - output_1_loss: -3.4670e+03 - output_2_loss: 8419.4414\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8853e+03 - output_1_loss: -3.4677e+03 - output_2_loss: 582.3242\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9472e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 517.9919\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.4727e+02 - output_1_loss: -5.4914e+03 - output_2_loss: 5344.1260\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16028.9590 - output_1_loss: -5.5180e+03 - output_2_loss: 21546.9395\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0679e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 397.3020\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2230.6582 - output_1_loss: -5.5028e+03 - output_2_loss: 7733.5073\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3489e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 117.6461\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 223.9541 - output_1_loss: -5.4942e+03 - output_2_loss: 5718.1104\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 84188.5391 - output_1_loss: -6.9788e+03 - output_2_loss: 91167.3750\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1907e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 275.6846\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4591.5259 - output_1_loss: -3.4701e+03 - output_2_loss: 8061.6401\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9801e+03 - output_1_loss: -3.4673e+03 - output_2_loss: 487.2158\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9806e+03 - output_1_loss: -3.4654e+03 - output_2_loss: 484.7388\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9085.8662 - output_1_loss: -6.9636e+03 - output_2_loss: 16049.4316\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 86552.3125 - output_1_loss: -5.5557e+03 - output_2_loss: 92108.0234\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23585.3887 - output_1_loss: -5.5222e+03 - output_2_loss: 29107.5762\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3419.0435 - output_1_loss: -5.5045e+03 - output_2_loss: 8923.5029\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.8966e+03 - output_1_loss: -5.4910e+03 - output_2_loss: 2594.3921\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2383.4297 - output_1_loss: -5.5077e+03 - output_2_loss: 7891.1245\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 77.4873 - output_1_loss: -5.4996e+03 - output_2_loss: 5577.1089\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 89595.5547 - output_1_loss: -6.9863e+03 - output_2_loss: 96581.8984\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1321e+03 - output_1_loss: -3.4669e+03 - output_2_loss: 334.8402\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19884.8867 - output_1_loss: -6.9746e+03 - output_2_loss: 26859.5293\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2433e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 222.4016\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2775e+03 - output_1_loss: -3.4672e+03 - output_2_loss: 189.7401\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2499e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 216.0766\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3705.6897 - output_1_loss: -3.4656e+03 - output_2_loss: 7171.2739\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.9915e+03 - output_1_loss: -3.4650e+03 - output_2_loss: 473.4732\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 103.5239 - output_1_loss: -5.5006e+03 - output_2_loss: 5604.1533\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 41531.4219 - output_1_loss: -5.5277e+03 - output_2_loss: 47059.1680\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10611.9785 - output_1_loss: -5.5039e+03 - output_2_loss: 16115.8467\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 26203.6465 - output_1_loss: -6.9787e+03 - output_2_loss: 33182.3594\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3375.6001 - output_1_loss: -5.5046e+03 - output_2_loss: 8880.1689\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7066e+03 - output_1_loss: -5.4960e+03 - output_2_loss: 2789.3638\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 969.0322 - output_1_loss: -5.5034e+03 - output_2_loss: 6472.4292\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 61817.3984 - output_1_loss: -6.9309e+03 - output_2_loss: 68748.2969\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4159.3428 - output_1_loss: -3.4633e+03 - output_2_loss: 7622.6895\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2618e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 204.5352\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1027e+03 - output_1_loss: -3.4665e+03 - output_2_loss: 363.7409\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1875.0488 - output_1_loss: -5.5047e+03 - output_2_loss: 7379.7017\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1027e+03 - output_1_loss: -3.4671e+03 - output_2_loss: 364.4272\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3020e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 164.9974\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 35920.1797 - output_1_loss: -6.9851e+03 - output_2_loss: 42905.2383\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.4774e+02 - output_1_loss: -5.5031e+03 - output_2_loss: 5055.4058\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3096.6646 - output_1_loss: -5.5090e+03 - output_2_loss: 8605.7109\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3338e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 131.8130\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 45599.8008 - output_1_loss: -5.5220e+03 - output_2_loss: 51121.7500\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 15334.5488 - output_1_loss: -5.5156e+03 - output_2_loss: 20850.1016\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1468e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 320.6439\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1547e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 311.5442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 12943.0391 - output_1_loss: -5.5157e+03 - output_2_loss: 18458.7812\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 75870.5391 - output_1_loss: -6.9737e+03 - output_2_loss: 82844.2109\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3978.7710 - output_1_loss: -3.4704e+03 - output_2_loss: 7449.1641\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6281.0308 - output_1_loss: -5.5126e+03 - output_2_loss: 11793.6084\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2130e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 252.2473\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 44471.1484 - output_1_loss: -5.5191e+03 - output_2_loss: 49990.2891\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7087e+03 - output_1_loss: -3.4689e+03 - output_2_loss: 760.2060\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3353e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 131.1073\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2394e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 226.5566\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 57167.2734 - output_1_loss: -6.9960e+03 - output_2_loss: 64163.2891\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2528e+03 - output_1_loss: -5.4972e+03 - output_2_loss: 2244.3984\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.5029e+02 - output_1_loss: -5.5028e+03 - output_2_loss: 5152.5454\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3389e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 126.6111\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28940.3457 - output_1_loss: -5.5285e+03 - output_2_loss: 34468.8633\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16270.4512 - output_1_loss: -5.5217e+03 - output_2_loss: 21792.1641\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 18385.0508 - output_1_loss: -5.5202e+03 - output_2_loss: 23905.2168\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.0028e+03 - output_1_loss: -3.4704e+03 - output_2_loss: 1467.6719\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2806e+03 - output_1_loss: -3.4664e+03 - output_2_loss: 185.7271\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 53415.5078 - output_1_loss: -6.9403e+03 - output_2_loss: 60355.8164\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2127.7817 - output_1_loss: -3.4634e+03 - output_2_loss: 5591.2139\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2155e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 249.7158\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 33695.2734 - output_1_loss: -5.5171e+03 - output_2_loss: 39212.3633\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3273e+03 - output_1_loss: -3.4666e+03 - output_2_loss: 139.3907\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 79635.3125 - output_1_loss: -7.0002e+03 - output_2_loss: 86635.5625\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 976.2480 - output_1_loss: -3.4746e+03 - output_2_loss: 4450.8979\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.2977e+03 - output_1_loss: -5.5025e+03 - output_2_loss: 3204.8127\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -9.8959e+02 - output_1_loss: -5.5007e+03 - output_2_loss: 4511.0957\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3638e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 101.8783\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13330.5430 - output_1_loss: -5.5021e+03 - output_2_loss: 18832.5957\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.6676e+02 - output_1_loss: -3.4629e+03 - output_2_loss: 3096.1292\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4518.2554 - output_1_loss: -5.4944e+03 - output_2_loss: 10012.6240\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2181e+03 - output_1_loss: -3.4651e+03 - output_2_loss: 247.0447\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27425.2305 - output_1_loss: -5.5061e+03 - output_2_loss: 32931.3398\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13365.2012 - output_1_loss: -5.4854e+03 - output_2_loss: 18850.5605\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3427e+03 - output_1_loss: -3.4663e+03 - output_2_loss: 123.6069\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2903e+03 - output_1_loss: -3.4675e+03 - output_2_loss: 177.2046\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 2779.6555 - output_1_loss: -3.4761e+03 - output_2_loss: 6255.7642\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 76608.9297 - output_1_loss: -6.9824e+03 - output_2_loss: 83591.3047\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6015e+03 - output_1_loss: -5.4998e+03 - output_2_loss: 2898.3289\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 13319.8008 - output_1_loss: -5.4974e+03 - output_2_loss: 18817.2207\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 56487.6758 - output_1_loss: -6.9695e+03 - output_2_loss: 63457.1914\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27415.9609 - output_1_loss: -5.4793e+03 - output_2_loss: 32895.2305\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3595e+03 - output_1_loss: -3.4661e+03 - output_2_loss: 106.5754\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.7931e+03 - output_1_loss: -5.4956e+03 - output_2_loss: 1702.5653\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3676e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 97.9331\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5778.6465 - output_1_loss: -3.4778e+03 - output_2_loss: 9256.4541\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 100945.6016 - output_1_loss: -6.9956e+03 - output_2_loss: 107941.1953\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7412e+03 - output_1_loss: -5.4998e+03 - output_2_loss: 2758.5750\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -5.9387e+02 - output_1_loss: -3.4645e+03 - output_2_loss: 2870.6775\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 5575.4131 - output_1_loss: -5.5035e+03 - output_2_loss: 11078.8682\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1588e+03 - output_1_loss: -3.4670e+03 - output_2_loss: 308.1761\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17433.6289 - output_1_loss: -5.5177e+03 - output_2_loss: 22951.3516\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3807e+03 - output_1_loss: -3.4660e+03 - output_2_loss: 85.2984\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 37733.6016 - output_1_loss: -6.9461e+03 - output_2_loss: 44679.7344\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 14534.2402 - output_1_loss: -5.5112e+03 - output_2_loss: 20045.4238\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.2860e+03 - output_1_loss: -5.5028e+03 - output_2_loss: 3216.7546\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 39241.6602 - output_1_loss: -5.4593e+03 - output_2_loss: 44700.9883\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22711.7695 - output_1_loss: -5.5221e+03 - output_2_loss: 28233.8750\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3839e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 81.8383\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -1.1481e+03 - output_1_loss: -5.5054e+03 - output_2_loss: 4357.2988\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3750e+03 - output_1_loss: -3.4653e+03 - output_2_loss: 90.3744\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 1790.2239 - output_1_loss: -3.4700e+03 - output_2_loss: 5260.2354\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 62070.6953 - output_1_loss: -6.9466e+03 - output_2_loss: 69017.3203\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -8.3781e+02 - output_1_loss: -3.4637e+03 - output_2_loss: 2625.8901\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3653.6284 - output_1_loss: -5.4963e+03 - output_2_loss: 9149.9219\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0462e+03 - output_1_loss: -3.4676e+03 - output_2_loss: 421.4565\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 10953.3184 - output_1_loss: -5.5093e+03 - output_2_loss: 16462.6660\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2353e+03 - output_1_loss: -5.5007e+03 - output_2_loss: 2265.3257\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3965e+03 - output_1_loss: -3.4657e+03 - output_2_loss: 69.1827\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 34168.7734 - output_1_loss: -6.9598e+03 - output_2_loss: 41128.5586\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3884e+03 - output_1_loss: -3.4651e+03 - output_2_loss: 76.6757\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 17043.1621 - output_1_loss: -5.4794e+03 - output_2_loss: 22522.5234\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 9822.0977 - output_1_loss: -5.5115e+03 - output_2_loss: 15333.5654\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 18607.0664 - output_1_loss: -5.4811e+03 - output_2_loss: 24088.1816\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.8177e+03 - output_1_loss: -5.4990e+03 - output_2_loss: 1681.3160\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3832e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 82.3554\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 27915.2344 - output_1_loss: -6.9544e+03 - output_2_loss: 34869.6719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1475e+03 - output_1_loss: -5.4986e+03 - output_2_loss: 2351.0320\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 3548.5908 - output_1_loss: -3.4744e+03 - output_2_loss: 7022.9629\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 87304.1094 - output_1_loss: -6.9877e+03 - output_2_loss: 94291.8438\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6308e+03 - output_1_loss: -3.4656e+03 - output_2_loss: 834.7415\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4143.6470 - output_1_loss: -5.5050e+03 - output_2_loss: 9648.6230\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1682e+03 - output_1_loss: -3.4658e+03 - output_2_loss: 297.5978\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3945e+03 - output_1_loss: -3.4655e+03 - output_2_loss: 71.0019\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20986.6211 - output_1_loss: -5.4741e+03 - output_2_loss: 26460.7168\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4153e+03 - output_1_loss: -3.4652e+03 - output_2_loss: 49.8929\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3890e+03 - output_1_loss: -3.4650e+03 - output_2_loss: 75.9342\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 4700.6919 - output_1_loss: -3.4766e+03 - output_2_loss: 8177.2427\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 164423.0781 - output_1_loss: -7.0303e+03 - output_2_loss: 171453.3906\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.6892e+03 - output_1_loss: -3.4653e+03 - output_2_loss: 776.0209\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6980.3433 - output_1_loss: -5.5118e+03 - output_2_loss: 12492.1338\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 32979.3320 - output_1_loss: -5.5103e+03 - output_2_loss: 38489.6055\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 19451.5508 - output_1_loss: -5.5200e+03 - output_2_loss: 24971.5176\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.1732e+03 - output_1_loss: -5.4899e+03 - output_2_loss: 1316.7209\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 22276.6973 - output_1_loss: -6.9460e+03 - output_2_loss: 29222.7246\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.0162e+03 - output_1_loss: -5.4994e+03 - output_2_loss: 2483.2827\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4209e+03 - output_1_loss: -3.4650e+03 - output_2_loss: 44.1320\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6231.4980 - output_1_loss: -3.4780e+03 - output_2_loss: 9709.4961\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.1938e+03 - output_1_loss: -3.4654e+03 - output_2_loss: 271.6483\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3898e+03 - output_1_loss: -3.4653e+03 - output_2_loss: 75.5188\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 6852.6099 - output_1_loss: -5.5118e+03 - output_2_loss: 12364.4541\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 28135.0273 - output_1_loss: -5.5049e+03 - output_2_loss: 33639.9531\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 20963.7734 - output_1_loss: -5.5104e+03 - output_2_loss: 26474.1543\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.3883e+03 - output_1_loss: -3.4649e+03 - output_2_loss: 76.5770\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 107229.3203 - output_1_loss: -6.9983e+03 - output_2_loss: 114227.6484\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -2.7685e+03 - output_1_loss: -3.4649e+03 - output_2_loss: 696.4391\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 23619.4336 - output_1_loss: -5.5210e+03 - output_2_loss: 29140.4297\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -4.4721e+03 - output_1_loss: -5.4945e+03 - output_2_loss: 1022.3660\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.4048e+03 - output_1_loss: -3.4659e+03 - output_2_loss: 61.0270\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: 16190.7324 - output_1_loss: -6.9483e+03 - output_2_loss: 23139.0293\n",
      "Train on 11 samples\n",
      "11/11 - 0s - loss: -3.2800e+03 - output_1_loss: -5.4967e+03 - output_2_loss: 2216.6848\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-1\n",
      "Dumping agent-1 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-2\n",
      "Dumping agent-2 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-3\n",
      "Dumping agent-3 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-4\n",
      "Dumping agent-4 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-5\n",
      "Dumping agent-5 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-6\n",
      "Dumping agent-6 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-7\n",
      "Dumping agent-7 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-8\n",
      "Dumping agent-8 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-9\n",
      "Dumping agent-9 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-10\n",
      "Dumping agent-10 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-11\n",
      "Dumping agent-11 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-12\n",
      "Dumping agent-12 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saving architecture, weights, optimizer state for agent-13\n",
      "Dumping agent-13 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n"
     ]
    }
   ],
   "source": [
    "Balance_MultiAc_Agents.train(1000)\n",
    "\n",
    "Balance_MultiAc_Agents.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_28  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_29  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  960       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  630       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_30  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,307\n",
      "Trainable params: 12,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_31  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_32  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,416\n",
      "Trainable params: 11,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model2_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  256       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  168       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_33  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,055\n",
      "Trainable params: 11,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_34  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,373\n",
      "Trainable params: 11,373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  512       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  336       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_35  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,522\n",
      "Trainable params: 11,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  256       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  168       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_36  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,055\n",
      "Trainable params: 11,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_37  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_38  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model2_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_39  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  512       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  336       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_40  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,565\n",
      "Trainable params: 11,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_41  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,416\n",
      "Trainable params: 11,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_28  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_29  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  960       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  630       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_30  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,307\n",
      "Trainable params: 12,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  576       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  378       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_31  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,628\n",
      "Trainable params: 11,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_32  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,416\n",
      "Trainable params: 11,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  256       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  168       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_33  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,055\n",
      "Trainable params: 11,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_34  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,373\n",
      "Trainable params: 11,373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  512       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  336       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_35  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,522\n",
      "Trainable params: 11,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  256       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  168       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_36  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,055\n",
      "Trainable params: 11,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_37  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_38  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  320       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  210       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_39  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,161\n",
      "Trainable params: 11,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  512       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  336       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_40  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,565\n",
      "Trainable params: 11,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_41  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,416\n",
      "Trainable params: 11,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "Balance_MultiAc_Agents = MasterAC_Agent(model_name, vissim_working_directory, sim_length, Balance_dictionary, n_step_size, gamma, alpha, entropy, value, \\\n",
    "                timesteps_per_second = timesteps_per_second, verbose = True, horizon = 100, \\\n",
    "                n_sample = 10)\n",
    "\n",
    "Balance_MultiAc_Agents.load(best = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Agents = []\n",
    "for idx, info in Balance_dictionary['junctions'].items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(info['state_size'], len(acts), idx, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patate\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12.0, -72.0, -64.0, -6.0, -71.0, -6.0, -6.0, -63.0, -19.0, -72.0] \n",
      " [-56.0, -72.0, -90.0, -15.0, -69.0, -25.0, -49.0, -78.0, -24.0, -72.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.33, 0.35, 0.32], [0.33, 0.34, 0.33], [0.33, 0.33, 0.34], [0.34, 0.34, 0.33], [0.32, 0.35, 0.32], [0.34, 0.34, 0.33], [0.34, 0.34, 0.33], [0.33, 0.34, 0.33], [0.34, 0.34, 0.33], [0.33, 0.34, 0.33]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.34 0.34 0.33]\n",
      "Agent 1 : Predicted Values and True Return : \n",
      " [-32.0, -71.0, -44.0, -28.0, -54.0, -60.0, -5.0, -75.0, -93.0, -5.0] \n",
      " [-55.0, -76.0, -118.0, -105.0, -85.0, -82.0, -64.0, -52.0, -87.0, -37.0]\n",
      "Agent 1 : Proba distribution on those states : \n",
      " [[0.33, 0.34, 0.33], [0.32, 0.34, 0.34], [0.3, 0.36, 0.34], [0.33, 0.33, 0.33], [0.32, 0.35, 0.33], [0.33, 0.33, 0.34], [0.33, 0.34, 0.34], [0.32, 0.35, 0.33], [0.33, 0.34, 0.33], [0.33, 0.34, 0.34]]\n",
      "Agent 1 : Proba distribution on the 0 state : \n",
      " [0.33 0.34 0.34]\n",
      "Agent 2 : Predicted Values and True Return : \n",
      " [-7.0, -108.0, -72.0, -127.0, -14.0, -146.0, -38.0, -193.0, -161.0, -161.0] \n",
      " [-31.0, -178.0, -160.0, -175.0, -42.0, -171.0, -58.0, -159.0, -153.0, -153.0]\n",
      "Agent 2 : Proba distribution on those states : \n",
      " [[0.25, 0.26, 0.24, 0.25], [0.24, 0.23, 0.26, 0.26], [0.26, 0.24, 0.25, 0.25], [0.24, 0.24, 0.22, 0.31], [0.24, 0.26, 0.25, 0.25], [0.23, 0.23, 0.26, 0.28], [0.24, 0.25, 0.24, 0.27], [0.26, 0.22, 0.27, 0.25], [0.26, 0.24, 0.22, 0.28], [0.26, 0.24, 0.22, 0.28]]\n",
      "Agent 2 : Proba distribution on the 0 state : \n",
      " [0.25 0.26 0.24 0.25]\n",
      "Agent 3 : Predicted Values and True Return : \n",
      " [-7.0, -11.0, -7.0, -13.0, -16.0, -4.0, -12.0, -13.0, -7.0, -16.0] \n",
      " [-24.0, -28.0, -20.0, -42.0, -38.0, -33.0, -29.0, -19.0, -38.0, -26.0]\n",
      "Agent 3 : Proba distribution on those states : \n",
      " [[0.33, 0.33, 0.33], [0.34, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.34, 0.33], [0.34, 0.33, 0.34], [0.33, 0.33, 0.33], [0.33, 0.34, 0.33], [0.35, 0.34, 0.31], [0.33, 0.33, 0.33], [0.31, 0.34, 0.35]]\n",
      "Agent 3 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Agent 4 : Predicted Values and True Return : \n",
      " [-7.0, -25.0, -36.0, -13.0, -25.0, -45.0, -30.0, -20.0, -19.0, -7.0] \n",
      " [-30.0, -16.0, -64.0, -20.0, -15.0, -34.0, -18.0, -19.0, -5.0, -8.0]\n",
      "Agent 4 : Proba distribution on those states : \n",
      " [[0.33, 0.33, 0.33], [0.34, 0.34, 0.32], [0.34, 0.34, 0.31], [0.33, 0.34, 0.33], [0.34, 0.33, 0.33], [0.35, 0.33, 0.31], [0.34, 0.35, 0.31], [0.33, 0.34, 0.33], [0.35, 0.34, 0.31], [0.33, 0.33, 0.33]]\n",
      "Agent 4 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Agent 5 : Predicted Values and True Return : \n",
      " [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -3.0, -1.0] \n",
      " [-5.0, -3.0, -0.0, -1.0, -2.0, -1.0, -1.0, -2.0, -3.0, -2.0]\n",
      "Agent 5 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 5 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 6 : Predicted Values and True Return : \n",
      " [-36.0, -3.0, -6.0, -6.0, -19.0, -3.0, -7.0, -13.0, -3.0, -3.0] \n",
      " [-21.0, -8.0, -5.0, -11.0, -12.0, -7.0, -18.0, -13.0, -13.0, -10.0]\n",
      "Agent 6 : Proba distribution on those states : \n",
      " [[0.49, 0.51], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.48, 0.52], [0.5, 0.5], [0.5, 0.5], [0.49, 0.51], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 6 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 7 : Predicted Values and True Return : \n",
      " [-43.0, -8.0, -4.0, -7.0, -12.0, -7.0, -32.0, -43.0, -7.0, -24.0] \n",
      " [-86.0, -66.0, -30.0, -19.0, -59.0, -21.0, -27.0, -65.0, -34.0, -43.0]\n",
      "Agent 7 : Proba distribution on those states : \n",
      " [[0.3, 0.34, 0.36], [0.34, 0.34, 0.33], [0.33, 0.34, 0.34], [0.33, 0.33, 0.33], [0.34, 0.34, 0.32], [0.33, 0.33, 0.34], [0.32, 0.33, 0.35], [0.3, 0.34, 0.36], [0.33, 0.33, 0.34], [0.33, 0.34, 0.34]]\n",
      "Agent 7 : Proba distribution on the 0 state : \n",
      " [0.33 0.34 0.34]\n",
      "Agent 8 : Predicted Values and True Return : \n",
      " [-6.0, -3.0, -3.0, -4.0, -7.0, -3.0, -4.0, -4.0, -3.0, -3.0] \n",
      " [-4.0, -4.0, -3.0, -6.0, -13.0, -6.0, -10.0, -3.0, -11.0, -2.0]\n",
      "Agent 8 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 8 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 9 : Predicted Values and True Return : \n",
      " [-4.0, -3.0, -3.0, -3.0, -4.0, -6.0, -7.0, -3.0, -3.0, -7.0] \n",
      " [-3.0, -5.0, -7.0, -5.0, -11.0, -7.0, -2.0, -3.0, -4.0, -8.0]\n",
      "Agent 9 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 9 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 10 : Predicted Values and True Return : \n",
      " [-6.0, -3.0, -3.0, -9.0, -5.0, -18.0, -6.0, -6.0, -3.0, -6.0] \n",
      " [-5.0, -2.0, -3.0, -4.0, -4.0, -10.0, -5.0, -8.0, -4.0, -3.0]\n",
      "Agent 10 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.51, 0.49], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 10 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 11 : Predicted Values and True Return : \n",
      " [-2.0, -2.0, -5.0, -4.0, -3.0, -7.0, -5.0, -2.0, -3.0, -4.0] \n",
      " [-2.0, -5.0, -4.0, -7.0, -11.0, -11.0, -6.0, -3.0, -3.0, -5.0]\n",
      "Agent 11 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 11 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 12 : Predicted Values and True Return : \n",
      " [-15.0, -19.0, -24.0, -15.0, -15.0, -22.0, -71.0, -12.0, -25.0, -59.0] \n",
      " [-16.0, -50.0, -49.0, -16.0, -17.0, -31.0, -61.0, -26.0, -56.0, -39.0]\n",
      "Agent 12 : Proba distribution on those states : \n",
      " [[0.25, 0.25, 0.25, 0.25], [0.24, 0.25, 0.25, 0.25], [0.25, 0.25, 0.26, 0.24], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.27, 0.24, 0.24, 0.25], [0.25, 0.25, 0.24, 0.26], [0.25, 0.25, 0.25, 0.25], [0.24, 0.26, 0.25, 0.24], [0.25, 0.26, 0.25, 0.24]]\n",
      "Agent 12 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Agent 13 : Predicted Values and True Return : \n",
      " [-70.0, -121.0, -102.0, -52.0, -113.0, -90.0, -113.0, -4.0, -137.0, -113.0] \n",
      " [-105.0, -123.0, -99.0, -111.0, -104.0, -99.0, -135.0, -12.0, -126.0, -104.0]\n",
      "Agent 13 : Proba distribution on those states : \n",
      " [[0.33, 0.33, 0.34], [0.32, 0.32, 0.36], [0.33, 0.32, 0.35], [0.33, 0.34, 0.33], [0.35, 0.34, 0.31], [0.33, 0.32, 0.35], [0.34, 0.32, 0.34], [0.33, 0.33, 0.33], [0.33, 0.31, 0.35], [0.35, 0.34, 0.31]]\n",
      "Agent 13 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Model: \"model2_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  256       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  168       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution_8 ( multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,055\n",
      "Trainable params: 11,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 8 : Entropy reduced to 500.0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model2_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  512       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  336       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution_12  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,565\n",
      "Trainable params: 11,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 12 : Entropy reduced to 500.0 \n",
      "Model: \"model2_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  448       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  294       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  129       \n",
      "_________________________________________________________________\n",
      "probability_distribution_13  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,416\n",
      "Trainable params: 11,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 13 : Entropy reduced to 500.0 \n",
      "patate\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-56.0, -56.0, -67.0, -22.0, -70.0, -56.0, -22.0, -57.0, -30.0, -34.0] \n",
      " [-93.0, -81.0, -30.0, -5.0, -78.0, -81.0, -61.0, -63.0, -19.0, -21.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.33, 0.35, 0.32], [0.33, 0.33, 0.33], [0.34, 0.34, 0.32], [0.33, 0.33, 0.33], [0.33, 0.33, 0.34], [0.34, 0.34, 0.32], [0.33, 0.33, 0.33], [0.33, 0.35, 0.32], [0.33, 0.34, 0.33], [0.33, 0.33, 0.34]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Agent 1 : Predicted Values and True Return : \n",
      " [-89.0, -134.0, -17.0, -151.0, -32.0, -25.0, -27.0, -79.0, -55.0, -17.0] \n",
      " [-52.0, -95.0, -7.0, -68.0, -100.0, -71.0, -87.0, -71.0, -99.0, -9.0]\n",
      "Agent 1 : Proba distribution on those states : \n",
      " [[0.33, 0.33, 0.34], [0.34, 0.33, 0.33], [0.33, 0.33, 0.33], [0.34, 0.33, 0.34], [0.33, 0.34, 0.33], [0.33, 0.33, 0.34], [0.33, 0.33, 0.33], [0.33, 0.34, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33]]\n",
      "Agent 1 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Agent 2 : Predicted Values and True Return : \n",
      " [-151.0, -189.0, -186.0, -115.0, -189.0, -159.0, -179.0, -104.0, -262.0, -61.0] \n",
      " [-155.0, -149.0, -106.0, -91.0, -104.0, -97.0, -120.0, -96.0, -160.0, -175.0]\n",
      "Agent 2 : Proba distribution on those states : \n",
      " [[0.26, 0.25, 0.25, 0.25], [0.25, 0.26, 0.25, 0.24], [0.25, 0.25, 0.26, 0.24], [0.25, 0.25, 0.25, 0.25], [0.24, 0.25, 0.26, 0.25], [0.26, 0.25, 0.25, 0.24], [0.25, 0.26, 0.25, 0.24], [0.25, 0.24, 0.26, 0.25], [0.24, 0.27, 0.26, 0.23], [0.23, 0.26, 0.25, 0.26]]\n",
      "Agent 2 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Agent 3 : Predicted Values and True Return : \n",
      " [-23.0, -22.0, -29.0, -22.0, -22.0, -22.0, -23.0, -49.0, -29.0, -33.0] \n",
      " [-24.0, -32.0, -14.0, -38.0, -38.0, -18.0, -32.0, -11.0, -28.0, -13.0]\n",
      "Agent 3 : Proba distribution on those states : \n",
      " [[0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.34, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33]]\n",
      "Agent 3 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Agent 4 : Predicted Values and True Return : \n",
      " [-100.0, -12.0, -62.0, -18.0, -43.0, -42.0, -18.0, -22.0, -100.0, -71.0] \n",
      " [-11.0, -31.0, -13.0, -19.0, -36.0, -16.0, -19.0, -30.0, -11.0, -34.0]\n",
      "Agent 4 : Proba distribution on those states : \n",
      " [[0.34, 0.35, 0.31], [0.33, 0.33, 0.33], [0.34, 0.34, 0.32], [0.33, 0.34, 0.33], [0.33, 0.34, 0.33], [0.34, 0.34, 0.33], [0.33, 0.34, 0.33], [0.34, 0.34, 0.33], [0.34, 0.35, 0.31], [0.34, 0.35, 0.32]]\n",
      "Agent 4 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Agent 5 : Predicted Values and True Return : \n",
      " [-6.0, -9.0, -17.0, -6.0, -6.0, -6.0, -6.0, -7.0, -7.0, -6.0] \n",
      " [-1.0, -2.0, -2.0, -3.0, -2.0, -3.0, -3.0, -2.0, -2.0, -1.0]\n",
      "Agent 5 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.51, 0.49], [0.51, 0.49], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 5 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 6 : Predicted Values and True Return : \n",
      " [-16.0, -19.0, -14.0, -24.0, -14.0, -20.0, -40.0, -26.0, -14.0, -14.0] \n",
      " [-18.0, -33.0, -14.0, -9.0, -9.0, -18.0, -26.0, -36.0, -5.0, -6.0]\n",
      "Agent 6 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.49, 0.51], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 6 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 7 : Predicted Values and True Return : \n",
      " [-55.0, -27.0, -21.0, -16.0, -20.0, -16.0, -27.0, -16.0, -16.0, -49.0] \n",
      " [-37.0, -78.0, -33.0, -27.0, -65.0, -42.0, -20.0, -18.0, -50.0, -21.0]\n",
      "Agent 7 : Proba distribution on those states : \n",
      " [[0.34, 0.34, 0.32], [0.33, 0.34, 0.33], [0.33, 0.34, 0.33], [0.33, 0.33, 0.33], [0.33, 0.34, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.33, 0.33, 0.33], [0.34, 0.33, 0.33]]\n",
      "Agent 7 : Proba distribution on the 0 state : \n",
      " [0.33 0.33 0.33]\n",
      "Agent 8 : Predicted Values and True Return : \n",
      " [-4.0, -4.0, -4.0, -4.0, -5.0, -4.0, -4.0, -4.0, -5.0, -7.0] \n",
      " [-9.0, -7.0, -6.0, -3.0, -3.0, -8.0, -3.0, -3.0, -9.0, -4.0]\n",
      "Agent 8 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.51, 0.49]]\n",
      "Agent 8 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 9 : Predicted Values and True Return : \n",
      " [-7.0, -4.0, -5.0, -4.0, -4.0, -4.0, -4.0, -4.0, -4.0, -4.0] \n",
      " [-8.0, -11.0, -6.0, -11.0, -3.0, -7.0, -5.0, -4.0, -4.0, -5.0]\n",
      "Agent 9 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 9 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 10 : Predicted Values and True Return : \n",
      " [-7.0, -7.0, -7.0, -8.0, -6.0, -9.0, -10.0, -7.0, -8.0, -7.0] \n",
      " [-8.0, -4.0, -1.0, -4.0, -8.0, -6.0, -2.0, -5.0, -10.0, -7.0]\n",
      "Agent 10 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.51, 0.49], [0.5, 0.5], [0.5, 0.5], [0.51, 0.49], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 10 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 11 : Predicted Values and True Return : \n",
      " [-5.0, -5.0, -5.0, -5.0, -5.0, -5.0, -5.0, -5.0, -5.0, -5.0] \n",
      " [-6.0, -6.0, -10.0, -8.0, -5.0, -10.0, -6.0, -3.0, -2.0, -3.0]\n",
      "Agent 11 : Proba distribution on those states : \n",
      " [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
      "Agent 11 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 12 : Predicted Values and True Return : \n",
      " [-47.0, -25.0, -61.0, -16.0, -37.0, -16.0, -54.0, -49.0, -31.0, -16.0] \n",
      " [-44.0, -19.0, -18.0, -8.0, -39.0, -7.0, -16.0, -19.0, -12.0, -6.0]\n",
      "Agent 12 : Proba distribution on those states : \n",
      " [[0.26, 0.24, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.26, 0.25, 0.24, 0.25], [0.26, 0.25, 0.24, 0.25], [0.26, 0.25, 0.25, 0.25], [0.26, 0.25, 0.24, 0.25], [0.25, 0.24, 0.26, 0.24], [0.26, 0.25, 0.24, 0.25], [0.24, 0.26, 0.25, 0.24], [0.26, 0.25, 0.24, 0.25]]\n",
      "Agent 12 : Proba distribution on the 0 state : \n",
      " [0.26 0.25 0.24 0.25]\n",
      "Agent 13 : Predicted Values and True Return : \n",
      " [-13.0, -86.0, -43.0, -83.0, -26.0, -105.0, -63.0, -13.0, -98.0, -31.0] \n",
      " [-9.0, -105.0, -83.0, -103.0, -66.0, -98.0, -99.0, -54.0, -103.0, -49.0]\n",
      "Agent 13 : Proba distribution on those states : \n",
      " [[0.33, 0.34, 0.33], [0.35, 0.34, 0.31], [0.35, 0.33, 0.33], [0.35, 0.33, 0.33], [0.34, 0.33, 0.33], [0.35, 0.32, 0.34], [0.35, 0.31, 0.34], [0.33, 0.34, 0.33], [0.35, 0.32, 0.33], [0.34, 0.33, 0.33]]\n",
      "Agent 13 : Proba distribution on the 0 state : \n",
      " [0.33 0.34 0.33]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ef3781782ae8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_step_size\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                 \u001b[0mAgents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Actor_Critic_Class.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m                 \u001b[0macts_and_advs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madvs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m                 \u001b[1;31m# performs a full training step on the collected batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m                 \u001b[1;31m# note: no need to mess around with gradients, Keras API handles it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m                 \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0macts_and_advs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3510\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3512\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m--> 572\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args)\u001b[0m\n\u001b[0;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[0;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[1;32m--> 445\u001b[1;33m             ctx=ctx)\n\u001b[0m\u001b[0;32m    446\u001b[0m       \u001b[1;31m# Replace empty list with None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = int(Agents[idx].choose_action(s))\n",
    "\n",
    "for i in range(30000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "            if (i+1)%reduce_entropy_every == 0:\n",
    "                if Agents[idx].params['entropy'] >= entropy_threshold :\n",
    "                    Agents[idx].reduce_entropy()\n",
    "                    print (\"Agent {} : Entropy reduced to {} \" .format(idx, Agents[idx].params['entropy']))\n",
    "        \n",
    "    # For the saving , monitoring of the agent \n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        \n",
    "        \n",
    "        # Only for AC\n",
    "        for idx, agent in enumerate(Agents):\n",
    "            predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "            print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "            print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "            print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "            agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance RL DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name  = 'Balance'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 1800\n",
    "agent_type = \"DuelingDDQN\"\n",
    "\n",
    "# all controller actions\n",
    "Balance_dictionary = balance_dictionary(agent_type)\n",
    "\n",
    "## DQN Hyperaramenters\n",
    "episodes = 400\n",
    "copy_weights_frequency = 10\n",
    "\n",
    "PER_activated = True\n",
    "memory_size = 1000\n",
    "batch_size = 128\n",
    "\n",
    "gamma = 0.95\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfEAAAEyCAYAAADqTulnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3dd7hU5bn+8e/Nho2FpoIVFGuUGBtbEhMLUUHEAh4bKmqM0WOsxyTmqImxxPw05lhjNGrsJhor4hFbrEcTCygiYiOIAUVFRbBFis/vj3dtGTa7DLBnr5k99+e61jWzyqx51l7oM++73qKIwMzMzCpPh7wDMDMzs6XjJG5mZlahnMTNzMwqlJO4mZlZhXISNzMzq1BO4mZmZhXKSdysTEi6TtLZbfh990k6tK2+rzmSHpP0o1Y61xmSbmrtY83KkZO42RKSNFXSF5I+LVguzTuu5jSWrCJi14i4Pq+YzGzZdcw7ALMKtUdE/C3vIAAkdYyI+XnHYWZtzyVxs1Yk6XJJtxes/1bSw0oGSpou6VRJH2Ql+oOaOdcRkiZL+kjSaElrFuwLScdIegN4I9t2saRpkuZIGidpu2z7EOBUYP+s1uDFbPvXVdiSOkj6paS3JL0v6QZJ3bN9fbPvO1TSv7LYf9FM3EMlTZL0iaS3Jf2sYN8wSeOzGP+ZxVZvHUlPZZ97UFLPgs99R9LfJX0s6UVJAwv2rSvp8exzDwGFnxsoaXqD+KZK2rmJ2Jv8HrNy5CRu1rp+Cmwm6QdZEj0cODQWjm+8OinJrAUcClwp6RsNTyJpR+AcYD9gDeAt4JYGhw0Hvg30y9afA7YAVgb+AtwmabmIuB/4f8BfI6JLRGzeSNw/yJbvA+sBXYCGjwi2Bb4B7AT8StImTfwNrgb+MyK6ApsCj2TXNAC4ATgJ6AFsD0wt+NyBwGHAqkAt8LPsc2sB9wJnZ9f2M+AOSb2yz/0FGEf6u/6a9HddYkV8j1nZcRI3WzqjstJa/XIEQER8DowELgBuAo6LiOkNPntaRHwZEY+TksZ+jZz/IOCaiHg+Ir4ETgG2kdS34JhzIuKjiPgi++6bIuLDiJgfEecDnUlJtxgHARdExJSI+DT7vhGSCh+5nRkRX0TEi8CLQGM/BgDmAf0kdYuIWRHxfLb98OyaHoqIryLi7Yh4teBz10bE69n13Er6QQLp7zkmIsZkn3sIGAsMlbQ2sDUL/6ZPAPcUec0NNfk9S3k+s5JzEjdbOsMjokfBclX9joh4FpgCiJSMCs2KiM8K1t8C1mRxa2b76s/5KfAhqQRfb1rhByT9VNIrkmZL+hjoTkHVcgsW+b7sfUdgtYJt7xa8/5xUWm/M3qTE91ZWzb1Ntr0P8M9mYmjq/OsA+xb+aCLVCqyRxd3Y33RpNPc9ZmXJSdyslUk6hlQKfgf4eYPdK0lasWB97ey4ht4hJZX6c64IrAK8XXBMFOzfDvhvUql+pYjoAcwm/ZBY5NgmLPJ9WVzzgfda+NxiIuK5iBhGqhYfxcIfMtOA9Zf0fNnnbmzwo2nFiDgXmEHjf9N6nwEr1K9IqgGaqh5v7nvMypKTuFkrkrQR6ZnqSOBg4OeStmhw2JmSarPEuztwWyOn+gtwmKQtJHUmPdN+JiKmNvHVXUlJdybQUdKvgG4F+98D+kpq6r/5m4ETs0ZiXVj4DH2JWr1n13WQpO4RMQ+YAyzIdl+dXdNOWUO6tSRtXMRpbwL2kLSLpBpJy2UN1npHxFukKu/6v+m2wB4Fn30dWE7SbpI6Ab8k/cBaou9Zkr+BWVtyEjdbOvdo0X7id2XPj28CfhsRL0bEG6RW4TdmiRhSlfEsUsn3z8BRDZ4LAxARDwOnAXeQSpvrAyOaiecB4D5S0noL+DeLVrfX/1D4UNLzLO4a4EbgCeDN7PPHtfRHaMLBwFRJc4CjSD9o6h8zHAZcSKoleJxFS/+NiohpwDDS33Im6bpOYuH/vw4kNfD7CDid1Hiu/rOzgaOBP5FqMT4DGrZRKPZ7zMqOFjaaNbNSyror3RQRLtmZWavwL0wzM7MK5SRuZmZWoVydbmZmVqFcEjczM6tQTuJmZmYVquJmMevZs2f07ds37zDMzMzaxLhx4z6IiEYHKaq4JN63b1/Gjh2bdxhmZmZtQlKTQwm7Ot3MzKxCOYmbmZlVKCdxMzOzCuUkbmZmVqGcxM3MzCpUyZK4pGskvS9pYhP7JekSSZMlTZC0ValiMTMza49KWRK/DhjSzP5dgQ2z5Ujg8hLGYmZm1u6ULIlHxBOk+X2bMgy4IZKngR6S1ihVPGZmZu1Nns/E1wKmFaxPz7a1mUmT4Kyz4Kuv2vJbzczMWkeeSVyNbGt0SjVJR0oaK2nszJkzWy2A55+H00+H8eNb7ZRmZmZtJs8kPh3oU7DeG3insQMj4sqIqIuIul69Gh0+dqnsvHN6ffDBVjulmZlZm8kziY8GDslaqX8HmB0RM9oygNVXh803dxI3M7PKVLIJUCTdDAwEekqaDpwOdAKIiD8CY4ChwGTgc+CwUsXSnMGD4aKL4LPPYMUV84jAzMxs6ZQsiUfEAS3sD+CYUn1/sQYPht/9Dh5/HIYOzTsaMzOz4lX9iG3bbgvLLecqdTMzqzxVn8SXWw522MFJ3MzMKk/VJ3GAQYPglVdg2rSWjzUzMysXTuKk5+IADz2UbxxmZmZLwkkc2HTT1N3MVepmZlZJnMQBKZXG//Y3D8FqZmaVw0k8M3gwfPghvPBC3pGYmZkVx0k84yFYzcys0jiJZ1ZbDbbYwknczMwqh5N4gcGD4amn4NNP847EzMysZU7iBQYPhnnz4JFH8o7EzMysZU7iBbbbDrp2hXvvzTsSMzOzljmJF6itTaO3jRkDEXlHY2Zm1jwn8QZ22w2mT4cJE/KOxMzMrHlO4g3UT0fqKnUzMyt3TuINrL469O/vJG5mZuXPSbwRu+0GTz+dRnAzMzMrV07ijRg6NI2hfv/9eUdiZmbWNCfxRmy9NfTq5Sp1MzMrb07ijejQAXbdNZXE58/POxozM7PGOYk3YbfdYNYseOaZvCMxMzNrnJN4EwYPhpoaV6mbmVn5chJvQo8esO22TuJmZla+nMSbsdtuaeS2adPyjsTMzGxxTuLN2GOP9HrPPfnGYWZm1hgn8WZsvDF84xtw1115R2JmZrY4J/EWDB8Ojz2WWqqbmZmVEyfxFuy1V+orPmZM3pGYmZktykm8BVtvDWus4Sp1MzMrP07iLejQAYYNS6O3ffFF3tGYmZkt5CRehL32gs8+g4cfzjsSMzOzhZzEizBwIHTv7ip1MzMrL07iRaitTQO/jB4NCxbkHY2ZmVniJF6k4cPhgw/gqafyjsTMzCxxEi/SkCHQuTOMGpV3JGZmZomTeJG6doWdd05JPCLvaMzMzEqcxCUNkfSapMmSTm5k/9qSHpX0gqQJkoaWMp5lNXw4vPlmmhTFzMwsbyVL4pJqgD8AuwL9gAMk9Wtw2C+BWyNiS2AEcFmp4mkNe+6Z+o3ffnvekZiZmRWRxCX9h6Q3JM2WNEfSJ5LmFHHuAcDkiJgSEXOBW4BhDY4JoFv2vjvwzpIE39ZWXRW+/3249VZXqZuZWf6KKYmfB+wZEd0joltEdI2Ibi1+CtYCCmfinp5tK3QGMFLSdGAMcFwR583VfvvB66+7St3MzPJXTBJ/LyJeWYpzq5FtDcuvBwDXRURvYChwo6TFYpJ0pKSxksbOnDlzKUJpPXvtBTU1qTRuZmaWp2KS+FhJf5V0QFa1/h+S/qOIz00H+hSs92bx6vLDgVsBIuIfwHJAz4YniogrI6IuIup69epVxFeXTq9esOOOrlI3M7P8FZPEuwGfA4OBPbJl9yI+9xywoaR1JdWSGq6NbnDMv4CdACRtQkri+Ra1i7DffjB5Mowfn3ckZmZWzTq2dEBEHLY0J46I+ZKOBR4AaoBrIuJlSWcBYyNiNPBT4CpJJ5Kq2n8QUf7l2732gqOOSqXxLbfMOxozM6tWailnSuoN/B74HinRPgmcEBHTSx/e4urq6mLs2LF5fPUihgyBN95IJXI19vTfzMysFUgaFxF1je0rpjr9WlI1+Jqk1uX3ZNuq2n77wZQp8PzzeUdiZmbVqpgk3isiro2I+dlyHZBv67IyMHw4dOzoVupmZpafYpL4B5JGSqrJlpHAh6UOrNytvDIMGuRW6mZmlp9ikvgPgf2Ad4EZwD7Ztqq3334wdSo891zekZiZWTUqpnX6v4A92yCWijNsGNTWws03w4ABeUdjZmbVpskkLunnEXGepN+z+EhrRMTxJY2sAqy0Euy2W0riv/tdekZuZmbWVpqrTq8fanUsMK6RxYCDD4b33oOHH847EjMzqzZNlh0j4p7s7ecRcVvhPkn7ljSqCjJ0KPToATfeCLvsknc0ZmZWTYpp2HZKkduqUufOqYHbXXfBp5/mHY2ZmVWTJpO4pF2z5+FrSbqkYLkOmN9mEVaAkSPh889h1Ki8IzEzs2rSXEn8HdLz8H+z6LPw0YArjgt873vQty/cdFPekZiZWTVp7pn4i8CLkv4SEfPaMKaK06EDHHQQnHMOzJgBa6yRd0RmZlYNinkm3lfS7ZImSZpSv5Q8sgpz0EHw1Vdwyy15R2JmZtWi2AlQLic9B/8+cANwYymDqkSbbAL9+7tK3czM2k4xSXz5iHiYNG3pWxFxBrBjacOqTAcfnGY1mzQp70jMzKwaFJPE/y2pA/CGpGMl7QWsWuK4KtKIEVBTAzfckHckZmZWDYpJ4v8FrAAcD/QHRgKHljKoSrXaamnwl+uvh/nuhGdmZiXWbBKXVAPsFxGfRsT0iDgsIvaOiKfbKL6Kc/jh8O67MGZM3pGYmVl712wSj4gFQH9JaqN4Kt7QoalEfvXVeUdiZmbtXTHzbr0A3C3pNuCz+o0RcWfJoqpgnTrBoYfC+eenEvnqq+cdkZmZtVfFPBNfGfiQ1CJ9j2zZvZRBVbrDDoMFC9zAzczMSksRi00VXtbq6upi7NixeYfRom23hZkz4dVXwQ8jzMxsaUkaFxF1je1rsSQuqbekuyS9L+k9SXdI6t36YbYvhx8Or78OTz2VdyRmZtZeFTti22hgTWAt4J5smzVj332hSxc3cDMzs9IpJon3iohrI2J+tlwH9CpxXBWvS5c0+Mutt8KcOXlHY2Zm7VExSfwDSSMl1WTLSFJDN2vB4Yenecb/+te8IzEzs/aomCT+Q2A/4F1gBrBPts1a8O1vw6abwuWXQ4W1HzQzswrQYhKPiH9FxJ4R0SsiVo2I4RHxVlsEV+kkOPpoeOEFePbZvKMxM7P2psnBXiT9Hmiy/BgRx5ckonZm5Ej4+c/hsstSydzMzKy1NDdiW/l3xq4AXbvCIYekVurnnw89e+YdkZmZtRdNJvGIuL5wXVK3tDk+KXlU7cyPf5xK4tdeCyedlHc0ZmbWXhQz2EudpJeACcBESS9K6l/60NqPTTeF7bdPDdy++irvaMzMrL0opnX6NcDREdE3ItYBjsGDvSyxo4+GN9+EBx7IOxIzM2sviknin0TE/9WvRMSTgKvUl9Bee6UpSi+7LO9IzMysvSgmiT8r6QpJAyXtIOky4DFJW0naqtQBthe1tXDEEXDvvTB1at7RmJlZe1BMEt8C2Ag4HTgD2AT4LnA+8D/NfVDSEEmvSZos6eQmjtlP0iRJL0v6yxJFX2GOPDL1Hf/jH/OOxMzM2oOSTUUqqQZ4HRgETAeeAw6IiEkFx2wI3ArsGBGzJK0aEe83d95KmYq0KfvsA488AtOmwYor5h2NmZmVu2WdivRGSd0L1teR9HAR3zsAmBwRUyJiLnALMKzBMUcAf4iIWQAtJfD24MQTYdYsuP76lo81MzNrTjHV6U8Cz0gaKukI4CHgoiI+txYwrWB9erat0EbARpKekvS0pCHFBF3JvvtdGDAALrrI3c3MzGzZNDdiGwARcYWkl4FHgQ+ALSPi3SLOrcZO18j3bwgMBHoD/ydp04j4eJETSUcCRwKsvfbaRXx1+ZJSafyAA1Ijtz32yDsiMzOrVMVUpx9M6it+CHAdMEbS5kWcezrQp2C9N/BOI8fcHRHzIuJN4DVSUl9ERFwZEXURUderV+VPZb733tCnD1x4Yd6RmJlZJSumOn1vYNuIuDkiTgGOAop5ovscsKGkdSXVAiOA0Q2OGQV8H0BST1L1+pRig69UnTrBccfBo4/C+PF5R2NmZpWqmKlIhxc2OIuIZ0mN1lr63HzgWOAB4BXg1oh4WdJZkvbMDnsA+FDSJFJ1/UkR8eFSXEfFOeKI1DrdpXEzM1taLXYxk7QRcDmwWkRsKmkzYM+IOLstAmyo0ruYFTr++NRn/K23YI018o7GzMzK0TJ1MQOuAk4B5gFExARS1bgtoxNOgPnz4dJL847EzMwqUTFJfIWsCr3Q/FIEU23WXz+NqX7ZZTBnTt7RmJlZpSkmiX8gaX2y7mGS9gFmlDSqKnLKKfDxxx6K1czMllwxSfwY4ApgY0lvA/9FaqFuraCuDgYNggsugC++yDsaMzOrJMW0Tp8SETsDvYCNI2LbiHir9KFVj1NPhffeg+uuyzsSMzOrJMWUxAGIiM8iwvOIl8AOO8A228B558G8eXlHY2ZmlaLoJG6lI6XS+NSpcMsteUdjZmaVwkm8TOy2G3zrW3DuuZ4YxczMitPiBCjZvOC7AX0Lj4+IC0oXVvWRUkv1Aw+E0aNh+PC8IzIzs3JXTEn8HuAHwCpA14LFWtm++6a+42efDS0MpGdmZtZySRzoHRGblTwSo2NH+OUv4bDDUml82LC8IzIzs3JWTEn8PkmDSx6JATByJGy4IZx+up+Nm5lZ84pJ4k8Dd0n6QtIcSZ9I8iChJdKxY0rgL74Id96ZdzRmZlbOikni5wPbkMZQ7xYRXSOiW4njqmojRsAmm6RkvmBB3tGYmVm5KiaJvwFMjJbmLLVWU1MDZ5wBkybBbbflHY2ZmZWrYuYTvw5YD7gP+LJ+e15dzNrTfOLN+eor2HzzNILbxImpmt3MzKrPss4n/ibwMFCLu5i1mQ4d4Mwz4bXX4Oab847GzMzKUYsl8a8PlLoCERGfljak5lVLSRxSX/H+/WH2bHjlFaitzTsiMzNra8tUEpe0qaQXgInAy5LGSfpmawdpi5PgnHNgyhTPN25mZosrpjr9SuAnEbFORKwD/BS4qrRhWb3Bg2HnneGss1KJ3MzMrF4xSXzFiHi0fiUiHgNWLFlEtggpTVH64Yfw29/mHY2ZmZWTYpL4FEmnSeqbLb8kNXazNrLllnDQQXDhhTB9et7RmJlZuSgmif8Q6AXcCdyVvT+slEHZ4s4+O3U7O/30vCMxM7Ny0WISj4hZEXF8RGwVEVtGxAkRMastgrOF+vaF446D666Dl17KOxozMysHTXYxk3QP0GT/s4jYs1RBNaeaupg19NFHaarSbbaBMWPyjsbMzNrC0nYx+x/SuOlvAl+QWqRfBXxK6m5mbWzlldNUpffd5yRuZmbFDbv6RERs39K2tlLNJXGAuXNhs83S8/GJEz0AjJlZe7esw672krRewcnWJTVusxzU1sJFF8Ebb8DFF+cdjZmZ5amYJH4i8JikxyQ9BjwK/FdJo7JmDRkCe+yRBoCZMSPvaMzMLC/FtE6/H9gQOCFbvhERD5Q6MGveBRekqvWTT847EjMzy0sxJXGA/sA3gc2B/SUdUrqQrBgbbAA/+QnccAP84x95R2NmZnkoZgKUG0kt1bcFts6WRh+wW9v6xS9gzTVT//EFC/KOxszM2lrHIo6pA/pFsXOWWpvp0gX+53/gwAPhsstSMjczs+pRTHX6RGD1UgdiS2fEiDTT2S9+4XHVzcyqTTFJvCcwSdIDkkbXL6UOzIojweWXw7x5cPzxeUdjZmZtqZjq9DNKHYQtm/XWSxOjnHIK3H03DBuWd0RmZtYWWhyxbZlOLg0BLgZqgD9FxLlNHLcPcBuwdUQ0OxxbtY/Y1pR582CrreDjj2HSJOjaNe+IzMysNSzTiG2SviPpOUmfSporaYGkOUV8rgb4A7Ar0A84QFK/Ro7rChwPPNPSOa1pnTrBlVfC22/Dr36VdzRmZtYWinkmfilwAPAGsDzwo2xbSwYAkyNiSkTMBW4BGqvo/TVwHvDvoiK2Jm2zDRx1FFxyifuOm5lVg6IGe4mIyUBNRCyIiGuBgUV8bC1gWsH69Gzb1yRtCfSJiP8tLlxrybnnQu/e8IMfwBdf5B2NmZmVUjFJ/HNJtcB4SedJOhFYsYjPqZFtXz+Al9QBuBD4aYsnko6UNFbS2JkzZxbx1dWrWze4+mp4/fU0bamZmbVfxSTxg7PjjgU+A/oAexfxuenZsfV6A+8UrHcFNiVNrjIV+A4wWtJiD+8j4sqIqIuIul69PIFaS3beGX78Y7jwQnjyybyjMTOzUmm2dXrWOO36iBi5xCeWOgKvAzsBbwPPAQdGxMtNHP8Y8DO3Tm8dn36a5h2vqYHx42HFYupOzMys7Cx16/SIWECaT7x2Sb80IuaTSu8PAK8At0bEy5LOkrTnkp7PlkyXLnDttTB5Mpx6at7RmJlZKRQz2MtU4KlslLbP6jdGxAUtfTAixgBjGmxrtANURAwsIhZbAjvskEZxu+QS2H13GDQo74jMzKw1FfNM/B3gf7NjuxYsVgHOOQf69YNDDgG3CTQza19aLIlHxJltEYiVxgorwM03w4ABcNhhcM89abx1MzOrfEX1E7fKttlm8Lvfwb33wh/+kHc0ZmbWWpzEq8Sxx8LQofCzn8GECXlHY2ZmraHJJC7pt9nrvm0XjpWKlFqr9+gBBxwAn3+ed0RmZrasmiuJD5XUCTilrYKx0lp1VbjxRnjllTQYTAknsDMzszbQXBK/H/gA2EzSHEmfFL62UXzWygYNSrOc3XADXHVV3tGYmdmyaDKJR8RJEdEduDciukVE18LXNozRWtlpp8Euu8Bxx4EHvzMzq1wtNmyLiGGSVpO0e7Z48PIKV1MDN90Eq68O++wDH32Ud0RmZrY0WkziWcO2Z4F9gf2AZyXtU+rArLR69oTbb4cZM2DkSPjqq7wjMjOzJVVMF7NfAltHxKERcQgwADittGFZW9h6a7j4YrjvvvSc3MzMKksxY6d3iIj3C9Y/xP3L243//E94/nn4zW/S8KwHHph3RGZmVqxikvj9kh4Abs7W96fBpCZWuSS49FJ47TU4/HDYcMNUQjczs/JXTMO2k4ArgM2AzYErI+K/Sx2YtZ3aWrjjjtTQbdgwePvtvCMyM7NiFFMSJyLuBO4scSyWo549YfRo+O53YfhwePzxNHmKmZmVLz/btq9961vw5z/DuHFw0EGwYEHeEZmZWXOcxG0Re+6ZWqyPGgXHH++hWc3MyllR1emSaoGNstXXImJe6UKyvB13HEyblqYv7dMHTj4574jMzKwxLSZxSQOB64GpgIA+kg6NiCdKG5rl6dxzYfp0OOUUWGstOPjgvCMyM7OGiimJnw8MjojXACRtROpu1r+UgVm+OnRIU5e++y788IdpBrRddsk7KjMzK1TMM/FO9QkcICJeBzqVLiQrF507w113wTe/CXvtBU+47sXMrKwUk8THSrpa0sBsuQoYV+rArDx07w4PPgjrrAO77w7PPZd3RGZmVq+YJP5j4GXgeOAEYBJwVCmDsvKy6qrw0EOwyiowZAhMnJh3RGZmBqCosD5EdXV1MdaTYOdiyhTYbrvUf/yJJ2CjjVr+jJmZLRtJ4yKirrF9TZbEJd2avb4kaULDpVTBWvlabz3429/StKUDB8Krr+YdkZlZdWuudfoJ2evubRGIVYZNNoHHHoMdd4QddoBHHkkN38zMrO01WRKPiBnZ26Mj4q3CBTi6bcKzctSvX0rkNTWpRD7B9TJmZrkopmHboEa27dragVhl2XjjNElK586pVP7883lHZGZWfZp7Jv5jSS8B32jwPPxNwGUvY8MNUyJfccVUIn/ssbwjMjOrLs2VxP8C7AGMzl7rl/4RMbINYrMKsP768NRT0Lt36n42alTeEZmZVY/mnonPjoipEXFA9hz8CyCALpLWbrMIrez17g3/93+wxRaw995w9dV5R2RmVh1afCYuaQ9JbwBvAo+TJkK5r8RxWYVZZRV4+GEYNAh+9CM4+2xPY2pmVmrFNGw7G/gO8HpErAvsBDxV0qisIq24IoweDSNHwmmnwWGHwdy5eUdlZtZ+FZPE50XEh0AHSR0i4lFgixLHZRWqthZuuAHOOAOuvx4GD4aPPso7KjOz9qmYJP6xpC7AE8CfJV0MzC9tWFbJJDj9dLjpJvjHP2CbbeCNN/KOysys/SkmiQ8DPgdOBO4H/klqpd4iSUMkvSZpsqSTG9n/E0mTsq5rD0taZ0mCt/J20EHpOfmHH8KAATBmTN4RmZm1Ly0m8Yj4LCK+ioj5EXE98AdgSEufk1STHbsr0A84QFK/Boe9ANRFxGbA7cB5S3oBVt623TZNX9q3b5rK9Ne/TmOvm5nZsmtusJdukk6RdKmkwUqOBaYA+xVx7gHA5IiYEhFzgVtIpfqvRcSjEfF5tvo00HvpLsPK2brrpr7kBx0Ev/oV7LUXzJ6dd1RmZpWvuZL4jcA3gJeAHwEPAvsCwyJiWDOfq7cWMK1gfXq2rSmH465r7dYKK6QGb5dckqrVt94aXn4576jMzCpbc0l8vYj4QURcARwA1AG7R8T4Is+tRrY12nNY0sjs/L9rYv+RksZKGjtz5swiv97KjQTHHZdmPpszJyXyq65yf3Izs6XVXBKfV/8mIhYAb0bEJ0tw7ulAn4L13sA7DQ+StDPwC2DPiPiysRNFxJURURcRdb169VqCEKwcbbcdjB+fnpcfeSTsuy/MmpV3VGZmlae5JL65pDnZ8gmwWf17SXOKOPdzwIaS1pVUC4wgjcP+NUlbAleQEvj7S3sRVnlWXx3uvx/OOw/uvhs23xyefDLvqMzMKktzY6fXRES3bOkaER0L3ndr6cQRMR84FngAeAW4NSJelnSWpD2zw34HdAFukzRe0ugmTmftUIcOcNJJ8Pe/p0Fidtgh9S/3KG9mZsVRVNgDybq6uhg7dmzeYVgr++QTOPbY1Pht80VNs+IAAA+LSURBVM3huuvShCpmZtVO0riIqGtsXzGDvZiVXNeuaZjWUaPgvfdSo7czznCp3MysOU7iVlaGDUtdz0aMgDPPTCO9vfBC3lGZmZUnJ3ErOyuvDDfemBq81ZfKTzwxdUszM7OFnMStbO25J0yaBEccARdfDJtsArfe6n7lZmb1nMStrK20Elx+OTz9dOqWtv/+sMsu8PrreUdmZpY/J3GrCAMGwLPPwu9/D888A5tuCj/5iecqN7Pq5iRuFaOmJnVDe+01OOQQuOgi2GCD9OpW7GZWjZzEreKsvjr86U9p6Nb+/VOjt29+E+6808/Lzay6OIlbxdpsM3jwQbj3XujUCfbeO1W733efk7mZVQcncatoEgwdChMmwDXXwMyZaX3bbdNsaWZm7ZmTuLULHTvCYYelVuuXXw5vvQU77QTf/z787W8umZtZ++Qkbu1KbS0cdRRMnpwavL36KgwalKrZ77gDvvoq7wjNzFqPk7i1S8stByecAG++CVdckeYr32cf6NcvVbv/+995R2hmtuycxK1dW245OPLI1C3tlltg+eXh8MNh7bXhtNPg7bfzjtDMbOk5iVtVqKlJo709/zw89BBssw385jfQt2+abOXvf/dzczOrPE7iVlUk2HnnNLnK5Mlw/PFw//3wve9BXV2qep89O+8ozcyK4yRuVWu99eD882H69NSife7c1ChujTXg0EPh8cddOjez8uYkblWvS5eUvCdMSOOyH3IIjBoFAwfCRhvBOefAtGl5R2lmtjgncbOMlLqi/fGPMGMGXH89rLkmnHpqagi33XZw2WVpQBkzs3LgJG7WiBVWSCXyxx9Pz85//es0Y9oxx6Tq9iFDUpL/+OO8IzWzaqaosId+dXV1MXbs2LzDsCoUAS+9BDffnLqrTZ2aRoobOBCGD4c994Q+ffKO0szaG0njIqKu0X1O4mZLLiI9Px81Ki2vvZa29++fEvqwYWnOcynfOM2s8jmJm5XYq6+mbmt33w1PP52S/Nprwy67wODBaRz3lVbKO0ozq0RO4mZt6N134Z57Uv/zhx9O/c47dEiN5gYPTol9wIBUFW9m1hIncbOczJ8Pzz4LDzyQ5j5/9tk0CUuXLmmAmR12gO23h623TpO3mJk15CRuViZmzUql80cfhSeegIkT0/bllktDwW6/ferKtvXW0K1bvrGaWXlwEjcrUx98AE8+mbqyPfEEjB+fSuoSbLIJfPvbqer929+Gb33LVfBm1chJ3KxCzJ6dWr0XLh98kPYtv3xq/T5gAGyxRVo23hg6dco3ZjMrreaSuH/Xm5WR7t1T47fBg9N6ROqPXpjUL7ts4XzotbXwzW+mhL755gtfe/TI7RLMrA25JG5WYebPhzfeSFXv9csLLyw6HOwaa6Tq+H790mv9stpq7rtuVmlcnW7WzkWkrm0vvpiWV16BSZPS66efLjxupZUWJvQNNoD111+4dO+eX/xm1jRXp5u1c1IqfdeP614vAt5+OyXzwsR+zz3w/vuLnmOVVRZN6vXLOuukiWDcqM6s/Pg/S7N2TILevdMyaNCi++bMgSlT4J//TEv9+6efhr/+NbWSr9ehQ0rkffqkkegae+3Z01X1Zm3NSdysSnXrtrCVe0Pz5sFbb6Wk/q9/pWXatLSMG5fGi//yy0U/U1sLq6+eljXWaPp1tdU8sI1Za3ESN7PFdOqUnplvsEHj+yNSQ7pp0xYm+Rkz0vLuu6lU//e/Nz33+korpZJ7S8sqq6TXlVZKtQFmtigncTNbYhKsumpa+vdv+rh589Kz93ffXZjgZ8yA996DDz9MfeCnTVvYur5h6b5ehw4pkffosfjSvXvz27p3T8Pc1tSU5m9hlqeSJnFJQ4CLgRrgTxFxboP9nYEbgP7Ah8D+ETG1lDGZWdvp1AnWWistLYmAzz9Pib1+qU/09cvs2fDxx2mZMWPh+meftXz+FVZIybxw6dp18W2NbV9hhTTYTlOLawksLyVL4pJqgD8Ag4DpwHOSRkfEpILDDgdmRcQGkkYAvwX2L1VMZla+JFhxxbSss86SfXbevJTQC5N8/TJ7NnzySepqV7/Ur3/8MUyfvui2uXOXPPba2uaTfOGy3HLQuXP6TG1t4+9b2t/U+44d0w8n/6ioHqUsiQ8AJkfEFABJtwDDgMIkPgw4I3t/O3CpJEWldV43s1x16rTwOfqymjs3lezrk/onn8AXXyz98skn6ZFC4bZ589Kjgy+/TDUQrU1KCb3h0qnTkm1vaV+HDmmpqWn+tS2OkRa+Ls37Zf184fuamvT4py2UMomvBUwrWJ8OfLupYyJivqTZwCrAByWMy8ysSfWl27b6n/CCBSmZz52blsbet7S//v38+Y0v8+Yt2b5//7vlzyxYkLohtvRajXr0SDMWtoVSJvHGeow2/M1ZzDFIOhI4EmDttdde9sjMzMpETU165r7CCnlHUhoRLSf6Yn4MtHRMRFq++qp13i/L59uyC2Upk/h0oE/Bem/gnSaOmS6pI9Ad+KjhiSLiSuBKSMOuliRaMzNrdfVV+1YapWz+8BywoaR1JdUCI4DRDY4ZDRyavd8HeMTPw83MzIpTst9H2TPuY4EHSF3MromIlyWdBYyNiNHA1cCNkiaTSuAjShWPmZlZe1PSSo6IGAOMabDtVwXv/w3sW8oYzMzM2iv3JjQzM6tQTuJmZmYVyknczMysQjmJm5mZVSgncTMzswrlJG5mZlahVGljq0iaCbzVSqfrSfsZp93XUp58LeWpvVxLe7kO8LU0Z52I6NXYjopL4q1J0tiIqMs7jtbgaylPvpby1F6upb1cB/halpar083MzCqUk7iZmVmFqvYkfmXeAbQiX0t58rWUp/ZyLe3lOsDXslSq+pm4mZlZJav2kriZmVnFqtokLmmIpNckTZZ0ct7xLClJUyW9JGm8pLHZtpUlPSTpjex1pbzjbIykayS9L2liwbZGY1dySXafJkjaKr/IF9fEtZwh6e3s3oyXNLRg3ynZtbwmaZd8ol6cpD6SHpX0iqSXJZ2Qba+4+9LMtVTifVlO0rOSXsyu5cxs+7qSnsnuy18l1WbbO2frk7P9ffOMv1Az13KdpDcL7ssW2fay/TcGIKlG0guS/jdbz+eeRETVLaT5zf8JrAfUAi8C/fKOawmvYSrQs8G284CTs/cnA7/NO84mYt8e2AqY2FLswFDgPkDAd4Bn8o6/iGs5A/hZI8f2y/6tdQbWzf4N1uR9DVlsawBbZe+7Aq9n8VbcfWnmWirxvgjokr3vBDyT/b1vBUZk2/8I/Dh7fzTwx+z9COCveV9DEddyHbBPI8eX7b+xLL6fAH8B/jdbz+WeVGtJfAAwOSKmRMRc4BZgWM4xtYZhwPXZ++uB4TnG0qSIeAL4qMHmpmIfBtwQydNAD0lrtE2kLWviWpoyDLglIr6MiDeByaR/i7mLiBkR8Xz2/hPgFWAtKvC+NHMtTSnn+xIR8Wm22ilbAtgRuD3b3vC+1N+v24GdJKmNwm1WM9fSlLL9NyapN7Ab8KdsXeR0T6o1ia8FTCtYn07z/5GXowAelDRO0pHZttUiYgak/5EBq+YW3ZJrKvZKvVfHZlWA1xQ81qiIa8mq+7YklZQq+r40uBaowPuSVduOB94HHiLVFHwcEfOzQwrj/fpasv2zgVXaNuKmNbyWiKi/L7/J7suFkjpn28r5vlwE/Bz4KltfhZzuSbUm8cZ+BVVaM/3vRcRWwK7AMZK2zzugEqnEe3U5sD6wBTADOD/bXvbXIqkLcAfwXxExp7lDG9lW7tdSkfclIhZExBZAb1INwSaNHZa9VtS1SNoUOAXYGNgaWBn47+zwsrwWSbsD70fEuMLNjRzaJvekWpP4dKBPwXpv4J2cYlkqEfFO9vo+cBfpP+736qubstf384twiTUVe8Xdq4h4L/uf1VfAVSysmi3ra5HUiZT0/hwRd2abK/K+NHYtlXpf6kXEx8BjpOfDPSR1zHYVxvv1tWT7u1P84542U3AtQ7LHHxERXwLXUv735XvAnpKmkh7F7kgqmedyT6o1iT8HbJi1JqwlNTYYnXNMRZO0oqSu9e+BwcBE0jUcmh12KHB3PhEulaZiHw0ckrVU/Q4wu756t1w1eG63F+neQLqWEVlr1XWBDYFn2zq+xmTP6K4GXomICwp2Vdx9aepaKvS+9JLUI3u/PLAz6Rn/o8A+2WEN70v9/doHeCSyFlV5a+JaXi34kSjSc+TC+1J2/8Yi4pSI6B0RfUm545GIOIi87klrtpKrpIXU8vF10vOlX+QdzxLGvh6pNe2LwMv18ZOeszwMvJG9rpx3rE3EfzOpOnMe6Vfq4U3FTqqK+kN2n14C6vKOv4hruTGLdUL2H/AaBcf/IruW14Bd846/IK5tSVV8E4Dx2TK0Eu9LM9dSifdlM+CFLOaJwK+y7euRfmhMBm4DOmfbl8vWJ2f718v7Goq4lkey+zIRuImFLdjL9t9YwTUNZGHr9FzuiUdsMzMzq1DVWp1uZmZW8ZzEzczMKpSTuJmZWYVyEjczM6tQTuJmZmYVykncrB2StKBgVqjxamGmPklHSTqkFb53qqSey3oeMyuOu5iZtUOSPo2ILjl871RSf94P2vq7zaqRS+JmVSQrKf9WaV7nZyVtkG0/Q9LPsvfHS5qUTUhxS7ZtZUmjsm1PS9os276KpAezeZWvoGCcaEkjs+8YL+mKbPKLGqX5oydKeknSiTn8GczaDSdxs/Zp+QbV6fsX7JsTEQOAS0ljPjd0MrBlRGwGHJVtOxN4Idt2KnBDtv104MmI2JI0CtraAJI2AfYnTdSzBbAAOIg0+chaEbFpRHyLNFa2mS2lji0fYmYV6IsseTbm5oLXCxvZPwH4s6RRwKhs27bA3gAR8UhWAu8ObA/8R7b9XkmzsuN3AvoDz2VTJy9PmjzlHmA9Sb8H7gUeXPpLNDOXxM2qTzTxvt5upDGr+wPjspmXmptOsbFzCLg+IrbIlm9ExBkRMQvYnDSD1THAn5byGswMJ3GzarR/wes/CndI6gD0iYhHgZ8DPYAuwBOk6nAkDQQ+iDRHd+H2XYGVslM9DOwjadVs38qS1slarneIiDuA04CtSnWRZtXA1elm7dPyksYXrN8fEfXdzDpLeob0I/6ABp+rAW7KqsoFXBgRH0s6A7hW0gTgcxZOrXgmcLOk54HHgX8BRMQkSb8EHsx+GMwjlby/yM5TX4A4pfUu2az6uIuZWRVxFzCz9sXV6WZmZhXKJXEzM7MK5ZK4mZlZhXISNzMzq1BO4mZmZhXKSdzMzKxCOYmbmZlVKCdxMzOzCvX/ARs8P0O98mzyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploration Schedule (\"linear\" or \"geometric\")\n",
    "exploration_schedule = \"geometric\"\n",
    "epsilon_start = 1\n",
    "epsilon_end   = 0.001\n",
    "\n",
    "Random_Seed = 100\n",
    "\n",
    "def choose_schedule(exploration_schedule, espilon_start, epsilon_end, episodes):\n",
    "    if exploration_schedule == \"linear\":\n",
    "        epsilon_decay = 1.2*(epsilon_end - epsilon_start)/(episodes-1)\n",
    "        epsilon_sequence = [1 + epsilon_decay * entry for entry in range(episodes+1)]\n",
    "        epsilon_sequence = [0 if entry < 0 else entry for entry in epsilon_sequence]\n",
    "    elif exploration_schedule == \"geometric\":\n",
    "        epsilon_decay = np.power(epsilon_end/epsilon_start, 1./(episodes-1)) # Geometric decay\n",
    "        epsilon_sequence = [epsilon_start * epsilon_decay ** entry for entry in range(episodes+1)]\n",
    "    elif exploration_schedule == \"entropy\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"ERROR: Unrecognized choice of exploration schedule.\")\n",
    "        \n",
    "    # Plotting exploration schedule\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x_series = np.array(range(1,episodes+1))\n",
    "    y_series = epsilon_sequence[0:episodes]\n",
    "    plt.plot(x_series, y_series, '-b')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Ratio of random exploration')\n",
    "    plt.title('Exploration schedule')\n",
    "    plt.show()\n",
    "    return(epsilon_sequence)\n",
    "\n",
    "epsilon_sequence = choose_schedule(exploration_schedule, epsilon_start, epsilon_end, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 48)           432         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 48)           2352        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 48)           2352        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 48)           2352        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            49          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            147         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 3)            0           dense_5[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,684\n",
      "Trainable params: 7,684\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 0\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 48)           432         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 48)           2352        dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 48)           2352        dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 48)           2352        dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 1)            49          dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 3)            147         dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 3)            0           dense_17[0][0]                   \n",
      "                                                                 dense_15[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,684\n",
      "Trainable params: 7,684\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 1\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 14)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 48)           720         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 48)           2352        dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 48)           2352        dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 48)           2352        dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 1)            49          dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 4)            196         dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 4)            0           dense_29[0][0]                   \n",
      "                                                                 dense_27[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 8,021\n",
      "Trainable params: 8,021\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 2\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 48)           432         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 48)           2352        dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 48)           2352        dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 48)           2352        dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 1)            49          dense_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 3)            147         dense_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 3)            0           dense_41[0][0]                   \n",
      "                                                                 dense_39[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,684\n",
      "Trainable params: 7,684\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 48)           336         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_49 (Dense)                (None, 48)           2352        dense_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_52 (Dense)                (None, 48)           2352        dense_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_50 (Dense)                (None, 48)           2352        dense_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_53 (Dense)                (None, 1)            49          dense_52[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_51 (Dense)                (None, 3)            147         dense_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 3)            0           dense_53[0][0]                   \n",
      "                                                                 dense_51[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,588\n",
      "Trainable params: 7,588\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 4\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_60 (Dense)                (None, 48)           192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_61 (Dense)                (None, 48)           2352        dense_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_64 (Dense)                (None, 48)           2352        dense_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_62 (Dense)                (None, 48)           2352        dense_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_65 (Dense)                (None, 1)            49          dense_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_63 (Dense)                (None, 2)            98          dense_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 2)            0           dense_65[0][0]                   \n",
      "                                                                 dense_63[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,395\n",
      "Trainable params: 7,395\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 5\n",
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_72 (Dense)                (None, 48)           336         input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_73 (Dense)                (None, 48)           2352        dense_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_76 (Dense)                (None, 48)           2352        dense_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_74 (Dense)                (None, 48)           2352        dense_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_77 (Dense)                (None, 1)            49          dense_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_75 (Dense)                (None, 2)            98          dense_74[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 2)            0           dense_77[0][0]                   \n",
      "                                                                 dense_75[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,539\n",
      "Trainable params: 7,539\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 6\n",
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_84 (Dense)                (None, 48)           384         input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_85 (Dense)                (None, 48)           2352        dense_84[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_88 (Dense)                (None, 48)           2352        dense_85[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_86 (Dense)                (None, 48)           2352        dense_85[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_89 (Dense)                (None, 1)            49          dense_88[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_87 (Dense)                (None, 3)            147         dense_86[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 3)            0           dense_89[0][0]                   \n",
      "                                                                 dense_87[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,636\n",
      "Trainable params: 7,636\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_96 (Dense)                (None, 48)           192         input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_97 (Dense)                (None, 48)           2352        dense_96[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_100 (Dense)               (None, 48)           2352        dense_97[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_98 (Dense)                (None, 48)           2352        dense_97[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_101 (Dense)               (None, 1)            49          dense_100[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_99 (Dense)                (None, 2)            98          dense_98[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 2)            0           dense_101[0][0]                  \n",
      "                                                                 dense_99[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,395\n",
      "Trainable params: 7,395\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 8\n",
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_108 (Dense)               (None, 48)           240         input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_109 (Dense)               (None, 48)           2352        dense_108[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_112 (Dense)               (None, 48)           2352        dense_109[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_110 (Dense)               (None, 48)           2352        dense_109[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_113 (Dense)               (None, 1)            49          dense_112[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_111 (Dense)               (None, 2)            98          dense_110[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 2)            0           dense_113[0][0]                  \n",
      "                                                                 dense_111[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,443\n",
      "Trainable params: 7,443\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 9\n",
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_120 (Dense)               (None, 48)           240         input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_121 (Dense)               (None, 48)           2352        dense_120[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_124 (Dense)               (None, 48)           2352        dense_121[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_122 (Dense)               (None, 48)           2352        dense_121[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_125 (Dense)               (None, 1)            49          dense_124[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_123 (Dense)               (None, 2)            98          dense_122[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 2)            0           dense_125[0][0]                  \n",
      "                                                                 dense_123[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,443\n",
      "Trainable params: 7,443\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 10\n",
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_23 (InputLayer)           [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_132 (Dense)               (None, 48)           240         input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_133 (Dense)               (None, 48)           2352        dense_132[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_136 (Dense)               (None, 48)           2352        dense_133[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_134 (Dense)               (None, 48)           2352        dense_133[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_137 (Dense)               (None, 1)            49          dense_136[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_135 (Dense)               (None, 2)            98          dense_134[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 2)            0           dense_137[0][0]                  \n",
      "                                                                 dense_135[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,443\n",
      "Trainable params: 7,443\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_24\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_144 (Dense)               (None, 48)           384         input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_145 (Dense)               (None, 48)           2352        dense_144[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_148 (Dense)               (None, 48)           2352        dense_145[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_146 (Dense)               (None, 48)           2352        dense_145[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_149 (Dense)               (None, 1)            49          dense_148[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_147 (Dense)               (None, 4)            196         dense_146[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 4)            0           dense_149[0][0]                  \n",
      "                                                                 dense_147[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,685\n",
      "Trainable params: 7,685\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 12\n",
      "Model: \"model_26\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_27 (InputLayer)           [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_156 (Dense)               (None, 48)           336         input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_157 (Dense)               (None, 48)           2352        dense_156[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_160 (Dense)               (None, 48)           2352        dense_157[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_158 (Dense)               (None, 48)           2352        dense_157[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_161 (Dense)               (None, 1)            49          dense_160[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_159 (Dense)               (None, 3)            147         dense_158[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "policy (Lambda)                 (None, 3)            0           dense_161[0][0]                  \n",
      "                                                                 dense_159[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,588\n",
      "Trainable params: 7,588\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Deploying instance of Dueling Double Deep Q Learning Agent(s) at intersection 13\n"
     ]
    }
   ],
   "source": [
    "Balance_MultiDQN_Agents = MasterDQN_Agent(model_name, vissim_working_directory, sim_length, Balance_dictionary,\\\n",
    "                gamma, alpha, agent_type, memory_size, PER_activated, batch_size, copy_weights_frequency, epsilon_sequence,\\\n",
    "                Random_Seed = Random_Seed, timesteps_per_second = 1, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Balance.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 3600 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links and Lanes.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 1.11 seconds.\n",
      "\n",
      "After 0 actions taken by the Agents,  Agent 0 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 1 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 2 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 3 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 4 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 5 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 6 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 7 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 8 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 9 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 10 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 11 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 12 memory is 0.0 percent full\n",
      "After 0 actions taken by the Agents,  Agent 13 memory is 0.0 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 0 memory is 7.2 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 1 memory is 6.9 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 2 memory is 6.8 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 3 memory is 6.8 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 4 memory is 6.8 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 5 memory is 7.2 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 6 memory is 7.5 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 7 memory is 7.0 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 8 memory is 7.4 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 9 memory is 7.4 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 10 memory is 7.5 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 11 memory is 7.5 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 12 memory is 6.8 percent full\n",
      "After 1000 actions taken by the Agents,  Agent 13 memory is 7.2 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 0 memory is 14.1 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 1 memory is 13.9 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 2 memory is 13.6 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 3 memory is 13.9 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 4 memory is 13.8 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 5 memory is 14.5 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 6 memory is 15.1 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 7 memory is 14.0 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 8 memory is 14.8 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 9 memory is 14.7 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 10 memory is 14.8 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 11 memory is 15.1 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 12 memory is 13.6 percent full\n",
      "After 2000 actions taken by the Agents,  Agent 13 memory is 14.1 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 0 memory is 21.3 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 1 memory is 20.8 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 2 memory is 20.4 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 3 memory is 20.9 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 4 memory is 20.8 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 5 memory is 21.6 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 6 memory is 22.5 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 7 memory is 20.9 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 8 memory is 22.1 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 9 memory is 22.3 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 10 memory is 22.2 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 11 memory is 22.5 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 12 memory is 20.5 percent full\n",
      "After 3000 actions taken by the Agents,  Agent 13 memory is 21.2 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 0 memory is 28.3 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 1 memory is 27.8 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 2 memory is 27.0 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 3 memory is 27.8 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 4 memory is 27.8 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 5 memory is 28.8 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 6 memory is 30.2 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 7 memory is 28.0 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 8 memory is 29.6 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 9 memory is 29.6 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 10 memory is 29.8 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 11 memory is 29.9 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 12 memory is 27.2 percent full\n",
      "After 4000 actions taken by the Agents,  Agent 13 memory is 28.2 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 0 memory is 35.2 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 1 memory is 34.8 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 2 memory is 33.9 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 3 memory is 34.8 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 4 memory is 34.9 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 5 memory is 36.2 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 6 memory is 37.9 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 7 memory is 34.7 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 8 memory is 37.1 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 9 memory is 36.9 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 10 memory is 37.0 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 11 memory is 37.3 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 12 memory is 34.1 percent full\n",
      "After 5000 actions taken by the Agents,  Agent 13 memory is 35.2 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 0 memory is 42.2 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 1 memory is 41.8 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 2 memory is 40.7 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 3 memory is 41.9 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 4 memory is 41.8 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 5 memory is 43.7 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 6 memory is 45.5 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 7 memory is 41.3 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 8 memory is 44.5 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 9 memory is 44.3 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 10 memory is 44.2 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 11 memory is 44.8 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 12 memory is 41.0 percent full\n",
      "After 6000 actions taken by the Agents,  Agent 13 memory is 42.3 percent full\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 8000 actions taken by the Agents,  Agent 0 memory is 56.2 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 1 memory is 55.8 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 2 memory is 54.4 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 3 memory is 55.9 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 4 memory is 55.4 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 5 memory is 58.6 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 6 memory is 60.7 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 7 memory is 55.3 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 8 memory is 59.1 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 9 memory is 59.3 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 10 memory is 59.1 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 11 memory is 59.6 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 12 memory is 54.6 percent full\n",
      "After 8000 actions taken by the Agents,  Agent 13 memory is 56.0 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 0 memory is 63.2 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 1 memory is 62.7 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 2 memory is 61.3 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 3 memory is 63.0 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 4 memory is 62.6 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 5 memory is 65.9 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 6 memory is 68.1 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 7 memory is 61.9 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 8 memory is 66.6 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 9 memory is 66.7 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 10 memory is 66.6 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 11 memory is 67.1 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 12 memory is 61.3 percent full\n",
      "After 9000 actions taken by the Agents,  Agent 13 memory is 63.0 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 0 memory is 70.1 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 1 memory is 69.7 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 2 memory is 67.9 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 3 memory is 69.9 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 4 memory is 69.7 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 5 memory is 73.4 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 6 memory is 75.6 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 7 memory is 68.9 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 8 memory is 74.0 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 9 memory is 74.1 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 10 memory is 74.1 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 11 memory is 74.6 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 12 memory is 68.1 percent full\n",
      "After 10000 actions taken by the Agents,  Agent 13 memory is 69.9 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 0 memory is 76.9 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 1 memory is 76.7 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 2 memory is 74.8 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 3 memory is 76.7 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 4 memory is 76.6 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 5 memory is 80.7 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 6 memory is 83.2 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 7 memory is 75.9 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 8 memory is 81.8 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 9 memory is 81.4 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 10 memory is 81.4 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 11 memory is 82.0 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 12 memory is 75.1 percent full\n",
      "After 11000 actions taken by the Agents,  Agent 13 memory is 76.8 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 0 memory is 84.0 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 1 memory is 83.5 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 2 memory is 81.6 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 3 memory is 83.6 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 4 memory is 83.6 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 5 memory is 88.1 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 6 memory is 90.7 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 7 memory is 82.8 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 8 memory is 89.2 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 9 memory is 89.1 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 10 memory is 88.8 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 11 memory is 89.1 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 12 memory is 81.9 percent full\n",
      "After 12000 actions taken by the Agents,  Agent 13 memory is 84.0 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 0 memory is 97.9 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 1 memory is 97.7 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 2 memory is 95.3 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 3 memory is 97.7 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 4 memory is 97.8 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 5 memory is 102.8 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 6 memory is 105.4 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 7 memory is 96.9 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 8 memory is 103.9 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 9 memory is 103.5 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 10 memory is 103.4 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 11 memory is 104.2 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 12 memory is 95.5 percent full\n",
      "After 14000 actions taken by the Agents,  Agent 13 memory is 98.0 percent full\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent0_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent1_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent2_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent3_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent4_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent5_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent6_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent7_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent8_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent9_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent10_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent11_PERPre_1000.p\n",
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent12_PERPre_1000.p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory filled. Saving as:C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\Agent13_PERPre_1000.p\n"
     ]
    }
   ],
   "source": [
    "Balance_MultiDQN_Agents.prepopulate_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Balance.inpx ...\n",
      "Model File load process successful.\n",
      "Simulation length set to 1800 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                COM SETUP COMPLETE                   *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 100\n",
      "Deploying Network Parser...\n",
      "Successful Network Crawl: Identified SignalControllers, Links and Lanes.\n",
      "\n",
      "Setting Simulation mode to: training\n",
      "Starting Deployments of Signal Control Units...\n",
      "SCUs successfully deployed. Elapsed time 1.05 seconds.\n",
      "\n",
      "Episode 1 is finished\n",
      "Average Reward for Agent 0 this episode : -9.14\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.3804\n",
      "Average Reward for Agent 1 this episode : -8.09\n",
      "Saving architecture, weights, optimizer state for best agent-1\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent1_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.2765\n",
      "Average Reward for Agent 2 this episode : -22.76\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent2_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 283.3306\n",
      "Average Reward for Agent 3 this episode : -4.65\n",
      "Saving architecture, weights, optimizer state for best agent-3\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent3_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.3318\n",
      "Average Reward for Agent 4 this episode : -4.03\n",
      "Saving architecture, weights, optimizer state for best agent-4\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent4_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.7298\n",
      "Average Reward for Agent 5 this episode : -0.61\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3412\n",
      "Average Reward for Agent 6 this episode : -2.96\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.6568\n",
      "Average Reward for Agent 7 this episode : -4.0\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.4407\n",
      "Average Reward for Agent 8 this episode : -1.0\n",
      "Saving architecture, weights, optimizer state for best agent-8\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.8942\n",
      "Average Reward for Agent 9 this episode : -0.91\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4833\n",
      "Average Reward for Agent 10 this episode : -0.83\n",
      "Saving architecture, weights, optimizer state for best agent-10\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent10_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3785\n",
      "Average Reward for Agent 11 this episode : -0.64\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3695\n",
      "Average Reward for Agent 12 this episode : -8.11\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.6909\n",
      "Average Reward for Agent 13 this episode : -9.47\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent13_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 539.1490\n",
      "Reducing exploration for all agents to 0.9828\n",
      "Episode 2 is finished\n",
      "Average Reward for Agent 0 this episode : -7.94\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.7246\n",
      "Average Reward for Agent 1 this episode : -9.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.9159\n",
      "Average Reward for Agent 2 this episode : -21.65\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent2_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 178.8548\n",
      "Average Reward for Agent 3 this episode : -4.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.7310\n",
      "Average Reward for Agent 4 this episode : -4.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.0268\n",
      "Average Reward for Agent 5 this episode : -0.34\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9154\n",
      "Average Reward for Agent 6 this episode : -4.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.0959\n",
      "Average Reward for Agent 7 this episode : -4.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.7624\n",
      "Average Reward for Agent 8 this episode : -0.5\n",
      "Saving architecture, weights, optimizer state for best agent-8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3741\n",
      "Average Reward for Agent 9 this episode : -0.51\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3777\n",
      "Average Reward for Agent 10 this episode : -0.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4060\n",
      "Average Reward for Agent 11 this episode : -0.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8977\n",
      "Average Reward for Agent 12 this episode : -10.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.8198\n",
      "Average Reward for Agent 13 this episode : -15.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 266.3109\n",
      "Reducing exploration for all agents to 0.966\n",
      "Episode 3 is finished\n",
      "Average Reward for Agent 0 this episode : -7.87\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.3141\n",
      "Average Reward for Agent 1 this episode : -9.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.7916\n",
      "Average Reward for Agent 2 this episode : -23.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 143.2689\n",
      "Average Reward for Agent 3 this episode : -3.05\n",
      "Saving architecture, weights, optimizer state for best agent-3\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent3_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.2439\n",
      "Average Reward for Agent 4 this episode : -4.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2109\n",
      "Average Reward for Agent 5 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5564\n",
      "Average Reward for Agent 6 this episode : -1.6\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.9247\n",
      "Average Reward for Agent 7 this episode : -3.49\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.3447\n",
      "Average Reward for Agent 8 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6219\n",
      "Average Reward for Agent 9 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7330\n",
      "Average Reward for Agent 10 this episode : -1.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1871\n",
      "Average Reward for Agent 11 this episode : -0.58\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6408\n",
      "Average Reward for Agent 12 this episode : -5.2\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7444\n",
      "Average Reward for Agent 13 this episode : -10.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 168.6539\n",
      "Reducing exploration for all agents to 0.9494\n",
      "Episode 4 is finished\n",
      "Average Reward for Agent 0 this episode : -6.55\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.8731\n",
      "Average Reward for Agent 1 this episode : -10.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.9808\n",
      "Average Reward for Agent 2 this episode : -23.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.8398\n",
      "Average Reward for Agent 3 this episode : -3.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8426\n",
      "Average Reward for Agent 4 this episode : -3.21\n",
      "Saving architecture, weights, optimizer state for best agent-4\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent4_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2364\n",
      "Average Reward for Agent 5 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5233\n",
      "Average Reward for Agent 6 this episode : -3.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9227\n",
      "Average Reward for Agent 7 this episode : -3.25\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0759\n",
      "Average Reward for Agent 8 this episode : -0.46\n",
      "Saving architecture, weights, optimizer state for best agent-8\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6392\n",
      "Average Reward for Agent 9 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6643\n",
      "Average Reward for Agent 10 this episode : -0.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8338\n",
      "Average Reward for Agent 11 this episode : -1.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7722\n",
      "Average Reward for Agent 12 this episode : -4.08\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6141\n",
      "Average Reward for Agent 13 this episode : -14.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.0274\n",
      "Reducing exploration for all agents to 0.9331\n",
      "Episode 5 is finished\n",
      "Average Reward for Agent 0 this episode : -7.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.4908\n",
      "Average Reward for Agent 1 this episode : -8.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.1416\n",
      "Average Reward for Agent 2 this episode : -21.6\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent2_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.7274\n",
      "Average Reward for Agent 3 this episode : -4.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4158\n",
      "Average Reward for Agent 4 this episode : -2.82\n",
      "Saving architecture, weights, optimizer state for best agent-4\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent4_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 5 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4203\n",
      "Average Reward for Agent 6 this episode : -2.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8617\n",
      "Average Reward for Agent 7 this episode : -5.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7148\n",
      "Average Reward for Agent 8 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8013\n",
      "Average Reward for Agent 9 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6257\n",
      "Average Reward for Agent 10 this episode : -0.53\n",
      "Saving architecture, weights, optimizer state for best agent-10\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent10_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4954\n",
      "Average Reward for Agent 11 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8114\n",
      "Average Reward for Agent 12 this episode : -4.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5877\n",
      "Average Reward for Agent 13 this episode : -5.28\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent13_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.5047\n",
      "Reducing exploration for all agents to 0.9171\n",
      "Episode 6 is finished\n",
      "Average Reward for Agent 0 this episode : -6.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3908\n",
      "Average Reward for Agent 1 this episode : -6.83\n",
      "Saving architecture, weights, optimizer state for best agent-1\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent1_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.3953\n",
      "Average Reward for Agent 2 this episode : -19.68\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent2_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.2103\n",
      "Average Reward for Agent 3 this episode : -3.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9408\n",
      "Average Reward for Agent 4 this episode : -3.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7336\n",
      "Average Reward for Agent 5 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5326\n",
      "Average Reward for Agent 6 this episode : -2.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3889\n",
      "Average Reward for Agent 7 this episode : -3.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.6101\n",
      "Average Reward for Agent 8 this episode : -0.35\n",
      "Saving architecture, weights, optimizer state for best agent-8\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0716\n",
      "Average Reward for Agent 9 this episode : -0.46\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3643\n",
      "Average Reward for Agent 10 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9440\n",
      "Average Reward for Agent 11 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0864\n",
      "Average Reward for Agent 12 this episode : -3.6\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5372\n",
      "Average Reward for Agent 13 this episode : -5.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.9860\n",
      "Reducing exploration for all agents to 0.9013\n",
      "Episode 7 is finished\n",
      "Average Reward for Agent 0 this episode : -8.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3728\n",
      "Average Reward for Agent 1 this episode : -8.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.7771\n",
      "Average Reward for Agent 2 this episode : -21.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.6391\n",
      "Average Reward for Agent 3 this episode : -3.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2971\n",
      "Average Reward for Agent 4 this episode : -3.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0203\n",
      "Average Reward for Agent 5 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6555\n",
      "Average Reward for Agent 6 this episode : -3.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3738\n",
      "Average Reward for Agent 7 this episode : -3.1\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.7322\n",
      "Average Reward for Agent 8 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6496\n",
      "Average Reward for Agent 9 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6364\n",
      "Average Reward for Agent 10 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5316\n",
      "Average Reward for Agent 11 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9854\n",
      "Average Reward for Agent 12 this episode : -4.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9965\n",
      "Average Reward for Agent 13 this episode : -7.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0670\n",
      "Reducing exploration for all agents to 0.8859\n",
      "Episode 8 is finished\n",
      "Average Reward for Agent 0 this episode : -5.7\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.7536\n",
      "Average Reward for Agent 1 this episode : -8.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.0148\n",
      "Average Reward for Agent 2 this episode : -15.57\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent2_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.1962\n",
      "Average Reward for Agent 3 this episode : -4.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.2435\n",
      "Average Reward for Agent 4 this episode : -2.78\n",
      "Saving architecture, weights, optimizer state for best agent-4\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent4_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3268\n",
      "Average Reward for Agent 5 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5013\n",
      "Average Reward for Agent 6 this episode : -2.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9313\n",
      "Average Reward for Agent 7 this episode : -3.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2173\n",
      "Average Reward for Agent 8 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8844\n",
      "Average Reward for Agent 9 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5761\n",
      "Average Reward for Agent 10 this episode : -0.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0010\n",
      "Average Reward for Agent 11 this episode : -0.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7597\n",
      "Average Reward for Agent 12 this episode : -3.44\n",
      "Saving architecture, weights, optimizer state for best agent-12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.9708\n",
      "Average Reward for Agent 13 this episode : -14.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7990\n",
      "Reducing exploration for all agents to 0.8707\n",
      "Episode 9 is finished\n",
      "Average Reward for Agent 0 this episode : -4.79\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6407\n",
      "Average Reward for Agent 1 this episode : -7.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.0892\n",
      "Average Reward for Agent 2 this episode : -17.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.8566\n",
      "Average Reward for Agent 3 this episode : -3.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.4184\n",
      "Average Reward for Agent 4 this episode : -4.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.9068\n",
      "Average Reward for Agent 5 this episode : -0.34\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4772\n",
      "Average Reward for Agent 6 this episode : -2.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9600\n",
      "Average Reward for Agent 7 this episode : -3.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9764\n",
      "Average Reward for Agent 8 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6040\n",
      "Average Reward for Agent 9 this episode : -0.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4217\n",
      "Average Reward for Agent 10 this episode : -0.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8022\n",
      "Average Reward for Agent 11 this episode : -0.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6485\n",
      "Average Reward for Agent 12 this episode : -5.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6304\n",
      "Average Reward for Agent 13 this episode : -5.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7976\n",
      "Reducing exploration for all agents to 0.8557\n",
      "Episode 10 is finished\n",
      "Average Reward for Agent 0 this episode : -6.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.3396\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -6.75\n",
      "Saving architecture, weights, optimizer state for best agent-1\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent1_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.0779\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -21.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.8456\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -2.43\n",
      "Saving architecture, weights, optimizer state for best agent-3\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent3_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6868\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -2.69\n",
      "Saving architecture, weights, optimizer state for best agent-4\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent4_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7365\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3016\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.2974\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -2.86\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7857\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7469\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.38\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5698\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -1.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1898\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.55\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5066\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -4.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.1315\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -7.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7770\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.841\n",
      "Episode 11 is finished\n",
      "Average Reward for Agent 0 this episode : -7.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.6552\n",
      "Average Reward for Agent 1 this episode : -7.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8250\n",
      "Average Reward for Agent 2 this episode : -19.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.8954\n",
      "Average Reward for Agent 3 this episode : -2.3\n",
      "Saving architecture, weights, optimizer state for best agent-3\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent3_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.2309\n",
      "Average Reward for Agent 4 this episode : -2.31\n",
      "Saving architecture, weights, optimizer state for best agent-4\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent4_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0035\n",
      "Average Reward for Agent 5 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7205\n",
      "Average Reward for Agent 6 this episode : -2.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.4604\n",
      "Average Reward for Agent 7 this episode : -2.65\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6959\n",
      "Average Reward for Agent 8 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7498\n",
      "Average Reward for Agent 9 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7644\n",
      "Average Reward for Agent 10 this episode : -0.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.2162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 11 this episode : -0.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7866\n",
      "Average Reward for Agent 12 this episode : -3.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0895\n",
      "Average Reward for Agent 13 this episode : -11.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.0518\n",
      "Reducing exploration for all agents to 0.8266\n",
      "Episode 12 is finished\n",
      "Average Reward for Agent 0 this episode : -5.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.2512\n",
      "Average Reward for Agent 1 this episode : -8.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.3611\n",
      "Average Reward for Agent 2 this episode : -18.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.4201\n",
      "Average Reward for Agent 3 this episode : -3.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3003\n",
      "Average Reward for Agent 4 this episode : -3.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7058\n",
      "Average Reward for Agent 5 this episode : -0.32\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5809\n",
      "Average Reward for Agent 6 this episode : -3.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.3478\n",
      "Average Reward for Agent 7 this episode : -3.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.2255\n",
      "Average Reward for Agent 8 this episode : -0.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9016\n",
      "Average Reward for Agent 9 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7350\n",
      "Average Reward for Agent 10 this episode : -0.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6570\n",
      "Average Reward for Agent 11 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0553\n",
      "Average Reward for Agent 12 this episode : -3.11\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6261\n",
      "Average Reward for Agent 13 this episode : -4.52\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent13_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.7353\n",
      "Reducing exploration for all agents to 0.8124\n",
      "Episode 13 is finished\n",
      "Average Reward for Agent 0 this episode : -5.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6281\n",
      "Average Reward for Agent 1 this episode : -8.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.4011\n",
      "Average Reward for Agent 2 this episode : -23.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.1234\n",
      "Average Reward for Agent 3 this episode : -3.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.9800\n",
      "Average Reward for Agent 4 this episode : -3.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6972\n",
      "Average Reward for Agent 5 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5910\n",
      "Average Reward for Agent 6 this episode : -3.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.8368\n",
      "Average Reward for Agent 7 this episode : -3.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.7051\n",
      "Average Reward for Agent 8 this episode : -0.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9657\n",
      "Average Reward for Agent 9 this episode : -0.36\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6837\n",
      "Average Reward for Agent 10 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0510\n",
      "Average Reward for Agent 11 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1597\n",
      "Average Reward for Agent 12 this episode : -4.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0018\n",
      "Average Reward for Agent 13 this episode : -6.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.0034\n",
      "Reducing exploration for all agents to 0.7985\n",
      "Episode 14 is finished\n",
      "Average Reward for Agent 0 this episode : -7.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.2812\n",
      "Average Reward for Agent 1 this episode : -8.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.5375\n",
      "Average Reward for Agent 2 this episode : -15.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.0130\n",
      "Average Reward for Agent 3 this episode : -2.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0031\n",
      "Average Reward for Agent 4 this episode : -4.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8746\n",
      "Average Reward for Agent 5 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7721\n",
      "Average Reward for Agent 6 this episode : -2.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.3094\n",
      "Average Reward for Agent 7 this episode : -2.44\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.2483\n",
      "Average Reward for Agent 8 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8653\n",
      "Average Reward for Agent 9 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6918\n",
      "Average Reward for Agent 10 this episode : -0.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1700\n",
      "Average Reward for Agent 11 this episode : -0.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2560\n",
      "Average Reward for Agent 12 this episode : -3.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3593\n",
      "Average Reward for Agent 13 this episode : -4.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.3877\n",
      "Reducing exploration for all agents to 0.7848\n",
      "Episode 15 is finished\n",
      "Average Reward for Agent 0 this episode : -7.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.0974\n",
      "Average Reward for Agent 1 this episode : -7.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.8728\n",
      "Average Reward for Agent 2 this episode : -18.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.2482\n",
      "Average Reward for Agent 3 this episode : -3.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.9252\n",
      "Average Reward for Agent 4 this episode : -4.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0581\n",
      "Average Reward for Agent 5 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6308\n",
      "Average Reward for Agent 6 this episode : -2.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.2170\n",
      "Average Reward for Agent 7 this episode : -2.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.0746\n",
      "Average Reward for Agent 8 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0691\n",
      "Average Reward for Agent 9 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2154\n",
      "Average Reward for Agent 10 this episode : -1.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8831\n",
      "Average Reward for Agent 11 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4695\n",
      "Average Reward for Agent 12 this episode : -4.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0623\n",
      "Average Reward for Agent 13 this episode : -10.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.8584\n",
      "Reducing exploration for all agents to 0.7713\n",
      "Episode 16 is finished\n",
      "Average Reward for Agent 0 this episode : -5.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.7233\n",
      "Average Reward for Agent 1 this episode : -6.72\n",
      "Saving architecture, weights, optimizer state for best agent-1\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent1_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.3880\n",
      "Average Reward for Agent 2 this episode : -20.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.3533\n",
      "Average Reward for Agent 3 this episode : -3.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.2908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 4 this episode : -4.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4681\n",
      "Average Reward for Agent 5 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8428\n",
      "Average Reward for Agent 6 this episode : -3.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.9218\n",
      "Average Reward for Agent 7 this episode : -3.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.1881\n",
      "Average Reward for Agent 8 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3325\n",
      "Average Reward for Agent 9 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7765\n",
      "Average Reward for Agent 10 this episode : -0.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5976\n",
      "Average Reward for Agent 11 this episode : -0.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4145\n",
      "Average Reward for Agent 12 this episode : -3.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.1477\n",
      "Average Reward for Agent 13 this episode : -5.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.8134\n",
      "Reducing exploration for all agents to 0.7581\n",
      "Episode 17 is finished\n",
      "Average Reward for Agent 0 this episode : -7.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.6016\n",
      "Average Reward for Agent 1 this episode : -7.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.6510\n",
      "Average Reward for Agent 2 this episode : -17.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.6198\n",
      "Average Reward for Agent 3 this episode : -3.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.0590\n",
      "Average Reward for Agent 4 this episode : -3.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.8954\n",
      "Average Reward for Agent 5 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6985\n",
      "Average Reward for Agent 6 this episode : -4.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.3096\n",
      "Average Reward for Agent 7 this episode : -2.16\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5888\n",
      "Average Reward for Agent 8 this episode : -0.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6930\n",
      "Average Reward for Agent 9 this episode : -0.34\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8289\n",
      "Average Reward for Agent 10 this episode : -0.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1076\n",
      "Average Reward for Agent 11 this episode : -0.54\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1634\n",
      "Average Reward for Agent 12 this episode : -4.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9608\n",
      "Average Reward for Agent 13 this episode : -5.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.3906\n",
      "Reducing exploration for all agents to 0.745\n",
      "Episode 18 is finished\n",
      "Average Reward for Agent 0 this episode : -7.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.0170\n",
      "Average Reward for Agent 1 this episode : -8.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.3071\n",
      "Average Reward for Agent 2 this episode : -18.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.7434\n",
      "Average Reward for Agent 3 this episode : -2.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.4113\n",
      "Average Reward for Agent 4 this episode : -2.28\n",
      "Saving architecture, weights, optimizer state for best agent-4\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent4_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7519\n",
      "Average Reward for Agent 5 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4865\n",
      "Average Reward for Agent 6 this episode : -3.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.1401\n",
      "Average Reward for Agent 7 this episode : -3.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7052\n",
      "Average Reward for Agent 8 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3680\n",
      "Average Reward for Agent 9 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6964\n",
      "Average Reward for Agent 10 this episode : -0.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5797\n",
      "Average Reward for Agent 11 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9100\n",
      "Average Reward for Agent 12 this episode : -4.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8242\n",
      "Average Reward for Agent 13 this episode : -5.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.6782\n",
      "Reducing exploration for all agents to 0.7323\n",
      "Episode 19 is finished\n",
      "Average Reward for Agent 0 this episode : -5.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.3277\n",
      "Average Reward for Agent 1 this episode : -11.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.6795\n",
      "Average Reward for Agent 2 this episode : -16.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.0112\n",
      "Average Reward for Agent 3 this episode : -2.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.2811\n",
      "Average Reward for Agent 4 this episode : -2.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.1842\n",
      "Average Reward for Agent 5 this episode : -0.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5985\n",
      "Average Reward for Agent 6 this episode : -3.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.2858\n",
      "Average Reward for Agent 7 this episode : -2.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2930\n",
      "Average Reward for Agent 8 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2313\n",
      "Average Reward for Agent 9 this episode : -0.33\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7480\n",
      "Average Reward for Agent 10 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9731\n",
      "Average Reward for Agent 11 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7025\n",
      "Average Reward for Agent 12 this episode : -3.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9493\n",
      "Average Reward for Agent 13 this episode : -9.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.5125\n",
      "Reducing exploration for all agents to 0.7197\n",
      "Episode 20 is finished\n",
      "Average Reward for Agent 0 this episode : -7.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.6412\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -10.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.6091\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -17.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.5545\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -2.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3822\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -2.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8374\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.3\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4729\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -3.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.0463\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -2.14\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9924\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0436\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7507\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -0.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4121\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6791\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -4.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7508\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -6.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.2099\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.7073\n",
      "Episode 21 is finished\n",
      "Average Reward for Agent 0 this episode : -7.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.9397\n",
      "Average Reward for Agent 1 this episode : -10.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.0225\n",
      "Average Reward for Agent 2 this episode : -19.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.8253\n",
      "Average Reward for Agent 3 this episode : -2.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.5244\n",
      "Average Reward for Agent 4 this episode : -2.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.3973\n",
      "Average Reward for Agent 5 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4445\n",
      "Average Reward for Agent 6 this episode : -2.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.0217\n",
      "Average Reward for Agent 7 this episode : -1.8\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6379\n",
      "Average Reward for Agent 8 this episode : -0.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2107\n",
      "Average Reward for Agent 9 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4391\n",
      "Average Reward for Agent 10 this episode : -1.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6209\n",
      "Average Reward for Agent 11 this episode : -0.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4131\n",
      "Average Reward for Agent 12 this episode : -4.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.9530\n",
      "Average Reward for Agent 13 this episode : -6.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.7550\n",
      "Reducing exploration for all agents to 0.6952\n",
      "Episode 22 is finished\n",
      "Average Reward for Agent 0 this episode : -7.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.6989\n",
      "Average Reward for Agent 1 this episode : -10.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.2131\n",
      "Average Reward for Agent 2 this episode : -20.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.7664\n",
      "Average Reward for Agent 3 this episode : -2.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3829\n",
      "Average Reward for Agent 4 this episode : -2.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.6168\n",
      "Average Reward for Agent 5 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5455\n",
      "Average Reward for Agent 6 this episode : -3.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.6375\n",
      "Average Reward for Agent 7 this episode : -2.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3847\n",
      "Average Reward for Agent 8 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1919\n",
      "Average Reward for Agent 9 this episode : -0.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4515\n",
      "Average Reward for Agent 10 this episode : -1.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5717\n",
      "Average Reward for Agent 11 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2098\n",
      "Average Reward for Agent 12 this episode : -4.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.0554\n",
      "Average Reward for Agent 13 this episode : -9.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8011\n",
      "Reducing exploration for all agents to 0.6833\n",
      "Episode 23 is finished\n",
      "Average Reward for Agent 0 this episode : -9.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.8240\n",
      "Average Reward for Agent 1 this episode : -8.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.0282\n",
      "Average Reward for Agent 2 this episode : -22.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.7644\n",
      "Average Reward for Agent 3 this episode : -2.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0919\n",
      "Average Reward for Agent 4 this episode : -2.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.9734\n",
      "Average Reward for Agent 5 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5274\n",
      "Average Reward for Agent 6 this episode : -2.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.0797\n",
      "Average Reward for Agent 7 this episode : -3.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0839\n",
      "Average Reward for Agent 8 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9686\n",
      "Average Reward for Agent 9 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6701\n",
      "Average Reward for Agent 10 this episode : -1.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5664\n",
      "Average Reward for Agent 11 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9913\n",
      "Average Reward for Agent 12 this episode : -4.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.5137\n",
      "Average Reward for Agent 13 this episode : -6.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.3607\n",
      "Reducing exploration for all agents to 0.6715\n",
      "Episode 24 is finished\n",
      "Average Reward for Agent 0 this episode : -8.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.0954\n",
      "Average Reward for Agent 1 this episode : -11.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.9897\n",
      "Average Reward for Agent 2 this episode : -20.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.4642\n",
      "Average Reward for Agent 3 this episode : -2.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6027\n",
      "Average Reward for Agent 4 this episode : -1.94\n",
      "Saving architecture, weights, optimizer state for best agent-4\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent4_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.0057\n",
      "Average Reward for Agent 5 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5715\n",
      "Average Reward for Agent 6 this episode : -2.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.4818\n",
      "Average Reward for Agent 7 this episode : -3.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1027\n",
      "Average Reward for Agent 8 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9997\n",
      "Average Reward for Agent 9 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5199\n",
      "Average Reward for Agent 10 this episode : -1.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1608\n",
      "Average Reward for Agent 11 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9349\n",
      "Average Reward for Agent 12 this episode : -3.06\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9426\n",
      "Average Reward for Agent 13 this episode : -8.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.7212\n",
      "Reducing exploration for all agents to 0.66\n",
      "Episode 25 is finished\n",
      "Average Reward for Agent 0 this episode : -11.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.7270\n",
      "Average Reward for Agent 1 this episode : -8.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.0765\n",
      "Average Reward for Agent 2 this episode : -19.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.1987\n",
      "Average Reward for Agent 3 this episode : -2.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4057\n",
      "Average Reward for Agent 4 this episode : -2.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3803\n",
      "Average Reward for Agent 5 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 6 this episode : -1.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.2347\n",
      "Average Reward for Agent 7 this episode : -3.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9547\n",
      "Average Reward for Agent 8 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7967\n",
      "Average Reward for Agent 9 this episode : -0.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7947\n",
      "Average Reward for Agent 10 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8897\n",
      "Average Reward for Agent 11 this episode : -0.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9934\n",
      "Average Reward for Agent 12 this episode : -2.95\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6725\n",
      "Average Reward for Agent 13 this episode : -4.48\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent13_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.9128\n",
      "Reducing exploration for all agents to 0.6487\n",
      "Episode 26 is finished\n",
      "Average Reward for Agent 0 this episode : -9.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.5855\n",
      "Average Reward for Agent 1 this episode : -9.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.2192\n",
      "Average Reward for Agent 2 this episode : -21.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.8108\n",
      "Average Reward for Agent 3 this episode : -3.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0948\n",
      "Average Reward for Agent 4 this episode : -2.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8743\n",
      "Average Reward for Agent 5 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4815\n",
      "Average Reward for Agent 6 this episode : -1.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.7033\n",
      "Average Reward for Agent 7 this episode : -2.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3617\n",
      "Average Reward for Agent 8 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1712\n",
      "Average Reward for Agent 9 this episode : -0.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8028\n",
      "Average Reward for Agent 10 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8707\n",
      "Average Reward for Agent 11 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0632\n",
      "Average Reward for Agent 12 this episode : -3.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2790\n",
      "Average Reward for Agent 13 this episode : -14.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.1295\n",
      "Reducing exploration for all agents to 0.6375\n",
      "Episode 27 is finished\n",
      "Average Reward for Agent 0 this episode : -8.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.3125\n",
      "Average Reward for Agent 1 this episode : -9.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.1460\n",
      "Average Reward for Agent 2 this episode : -21.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.8520\n",
      "Average Reward for Agent 3 this episode : -3.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.6467\n",
      "Average Reward for Agent 4 this episode : -2.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4452\n",
      "Average Reward for Agent 5 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4844\n",
      "Average Reward for Agent 6 this episode : -2.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.2849\n",
      "Average Reward for Agent 7 this episode : -2.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9879\n",
      "Average Reward for Agent 8 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9570\n",
      "Average Reward for Agent 9 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6786\n",
      "Average Reward for Agent 10 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9087\n",
      "Average Reward for Agent 11 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7717\n",
      "Average Reward for Agent 12 this episode : -3.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4114\n",
      "Average Reward for Agent 13 this episode : -5.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.9749\n",
      "Reducing exploration for all agents to 0.6266\n",
      "Episode 28 is finished\n",
      "Average Reward for Agent 0 this episode : -10.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.0491\n",
      "Average Reward for Agent 1 this episode : -9.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.8454\n",
      "Average Reward for Agent 2 this episode : -22.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.4178\n",
      "Average Reward for Agent 3 this episode : -4.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.4464\n",
      "Average Reward for Agent 4 this episode : -2.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.6178\n",
      "Average Reward for Agent 5 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5064\n",
      "Average Reward for Agent 6 this episode : -2.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.2854\n",
      "Average Reward for Agent 7 this episode : -3.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5079\n",
      "Average Reward for Agent 8 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8142\n",
      "Average Reward for Agent 9 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8348\n",
      "Average Reward for Agent 10 this episode : -0.52\n",
      "Saving architecture, weights, optimizer state for best agent-10\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent10_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2563\n",
      "Average Reward for Agent 11 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9022\n",
      "Average Reward for Agent 12 this episode : -3.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2063\n",
      "Average Reward for Agent 13 this episode : -11.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.7990\n",
      "Reducing exploration for all agents to 0.6158\n",
      "Episode 29 is finished\n",
      "Average Reward for Agent 0 this episode : -9.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.4440\n",
      "Average Reward for Agent 1 this episode : -11.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.4590\n",
      "Average Reward for Agent 2 this episode : -21.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.4348\n",
      "Average Reward for Agent 3 this episode : -3.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.4899\n",
      "Average Reward for Agent 4 this episode : -2.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.1776\n",
      "Average Reward for Agent 5 this episode : -0.28\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3975\n",
      "Average Reward for Agent 6 this episode : -3.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.5243\n",
      "Average Reward for Agent 7 this episode : -3.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.6608\n",
      "Average Reward for Agent 8 this episode : -0.25\n",
      "Saving architecture, weights, optimizer state for best agent-8\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7501\n",
      "Average Reward for Agent 9 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8105\n",
      "Average Reward for Agent 10 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4389\n",
      "Average Reward for Agent 11 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7781\n",
      "Average Reward for Agent 12 this episode : -2.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6724\n",
      "Average Reward for Agent 13 this episode : -16.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.2276\n",
      "Reducing exploration for all agents to 0.6053\n",
      "Episode 30 is finished\n",
      "Average Reward for Agent 0 this episode : -9.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.1379\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -11.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.0778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -21.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 89.2260\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -3.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3163\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -2.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5809\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.24\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3864\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -4.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.7459\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -3.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.7252\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6291\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6829\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1430\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7496\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -2.74\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9822\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -20.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.8526\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.5949\n",
      "Episode 31 is finished\n",
      "Average Reward for Agent 0 this episode : -11.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.4031\n",
      "Average Reward for Agent 1 this episode : -10.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.2446\n",
      "Average Reward for Agent 2 this episode : -23.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 153.7383\n",
      "Average Reward for Agent 3 this episode : -4.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.1570\n",
      "Average Reward for Agent 4 this episode : -2.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.1085\n",
      "Average Reward for Agent 5 this episode : -0.21\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1948\n",
      "Average Reward for Agent 6 this episode : -5.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.9678\n",
      "Average Reward for Agent 7 this episode : -2.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.8553\n",
      "Average Reward for Agent 8 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4591\n",
      "Average Reward for Agent 9 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8869\n",
      "Average Reward for Agent 10 this episode : -0.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2749\n",
      "Average Reward for Agent 11 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1544\n",
      "Average Reward for Agent 12 this episode : -3.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.6630\n",
      "Average Reward for Agent 13 this episode : -22.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 126.3936\n",
      "Reducing exploration for all agents to 0.5847\n",
      "Episode 32 is finished\n",
      "Average Reward for Agent 0 this episode : -9.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.4792\n",
      "Average Reward for Agent 1 this episode : -12.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.8293\n",
      "Average Reward for Agent 2 this episode : -23.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.1553\n",
      "Average Reward for Agent 3 this episode : -4.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.0344\n",
      "Average Reward for Agent 4 this episode : -2.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5205\n",
      "Average Reward for Agent 5 this episode : -0.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3357\n",
      "Average Reward for Agent 6 this episode : -5.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8027\n",
      "Average Reward for Agent 7 this episode : -2.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3956\n",
      "Average Reward for Agent 8 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5014\n",
      "Average Reward for Agent 9 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8254\n",
      "Average Reward for Agent 10 this episode : -0.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7420\n",
      "Average Reward for Agent 11 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4624\n",
      "Average Reward for Agent 12 this episode : -3.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.0875\n",
      "Average Reward for Agent 13 this episode : -19.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.5244\n",
      "Reducing exploration for all agents to 0.5746\n",
      "Episode 33 is finished\n",
      "Average Reward for Agent 0 this episode : -12.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.8971\n",
      "Average Reward for Agent 1 this episode : -12.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.8968\n",
      "Average Reward for Agent 2 this episode : -18.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 111.1715\n",
      "Average Reward for Agent 3 this episode : -3.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.3184\n",
      "Average Reward for Agent 4 this episode : -3.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.5587\n",
      "Average Reward for Agent 5 this episode : -0.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2227\n",
      "Average Reward for Agent 6 this episode : -9.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.5251\n",
      "Average Reward for Agent 7 this episode : -3.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7537\n",
      "Average Reward for Agent 8 this episode : -0.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4141\n",
      "Average Reward for Agent 9 this episode : -0.31\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8470\n",
      "Average Reward for Agent 10 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3224\n",
      "Average Reward for Agent 11 this episode : -1.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1044\n",
      "Average Reward for Agent 12 this episode : -3.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.6563\n",
      "Average Reward for Agent 13 this episode : -3.18\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent13_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.6588\n",
      "Reducing exploration for all agents to 0.5648\n",
      "Episode 34 is finished\n",
      "Average Reward for Agent 0 this episode : -11.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.0457\n",
      "Average Reward for Agent 1 this episode : -8.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.0831\n",
      "Average Reward for Agent 2 this episode : -21.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 127.8781\n",
      "Average Reward for Agent 3 this episode : -4.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.8394\n",
      "Average Reward for Agent 4 this episode : -2.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 5 this episode : -0.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1442\n",
      "Average Reward for Agent 6 this episode : -6.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.6092\n",
      "Average Reward for Agent 7 this episode : -2.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2311\n",
      "Average Reward for Agent 8 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4476\n",
      "Average Reward for Agent 9 this episode : -0.3\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6985\n",
      "Average Reward for Agent 10 this episode : -1.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1807\n",
      "Average Reward for Agent 11 this episode : -0.41\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2298\n",
      "Average Reward for Agent 12 this episode : -2.47\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1220\n",
      "Average Reward for Agent 13 this episode : -10.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.7930\n",
      "Reducing exploration for all agents to 0.5551\n",
      "Episode 35 is finished\n",
      "Average Reward for Agent 0 this episode : -11.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8109\n",
      "Average Reward for Agent 1 this episode : -9.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.1023\n",
      "Average Reward for Agent 2 this episode : -25.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.9544\n",
      "Average Reward for Agent 3 this episode : -5.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.5903\n",
      "Average Reward for Agent 4 this episode : -2.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.2057\n",
      "Average Reward for Agent 5 this episode : -0.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2412\n",
      "Average Reward for Agent 6 this episode : -3.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.3705\n",
      "Average Reward for Agent 7 this episode : -1.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2732\n",
      "Average Reward for Agent 8 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3258\n",
      "Average Reward for Agent 9 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7618\n",
      "Average Reward for Agent 10 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5818\n",
      "Average Reward for Agent 11 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4662\n",
      "Average Reward for Agent 12 this episode : -2.12\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.6572\n",
      "Average Reward for Agent 13 this episode : -9.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.1785\n",
      "Reducing exploration for all agents to 0.5456\n",
      "Episode 36 is finished\n",
      "Average Reward for Agent 0 this episode : -10.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.4092\n",
      "Average Reward for Agent 1 this episode : -10.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.5381\n",
      "Average Reward for Agent 2 this episode : -20.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.4355\n",
      "Average Reward for Agent 3 this episode : -9.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.5026\n",
      "Average Reward for Agent 4 this episode : -5.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.6664\n",
      "Average Reward for Agent 5 this episode : -0.18\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1712\n",
      "Average Reward for Agent 6 this episode : -2.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.8313\n",
      "Average Reward for Agent 7 this episode : -1.65\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.5410\n",
      "Average Reward for Agent 8 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4887\n",
      "Average Reward for Agent 9 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7192\n",
      "Average Reward for Agent 10 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9883\n",
      "Average Reward for Agent 11 this episode : -0.39\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7735\n",
      "Average Reward for Agent 12 this episode : -2.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5034\n",
      "Average Reward for Agent 13 this episode : -5.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8340\n",
      "Reducing exploration for all agents to 0.5362\n",
      "Episode 37 is finished\n",
      "Average Reward for Agent 0 this episode : -10.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.0980\n",
      "Average Reward for Agent 1 this episode : -10.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.1506\n",
      "Average Reward for Agent 2 this episode : -20.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.3325\n",
      "Average Reward for Agent 3 this episode : -10.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.6758\n",
      "Average Reward for Agent 4 this episode : -4.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.7378\n",
      "Average Reward for Agent 5 this episode : -0.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1982\n",
      "Average Reward for Agent 6 this episode : -1.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.5204\n",
      "Average Reward for Agent 7 this episode : -1.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7681\n",
      "Average Reward for Agent 8 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6077\n",
      "Average Reward for Agent 9 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5236\n",
      "Average Reward for Agent 10 this episode : -0.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0248\n",
      "Average Reward for Agent 11 this episode : -0.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8449\n",
      "Average Reward for Agent 12 this episode : -2.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3933\n",
      "Average Reward for Agent 13 this episode : -4.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.9614\n",
      "Reducing exploration for all agents to 0.527\n",
      "Episode 38 is finished\n",
      "Average Reward for Agent 0 this episode : -11.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.6783\n",
      "Average Reward for Agent 1 this episode : -12.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.7581\n",
      "Average Reward for Agent 2 this episode : -25.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.9419\n",
      "Average Reward for Agent 3 this episode : -7.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.0075\n",
      "Average Reward for Agent 4 this episode : -2.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.7355\n",
      "Average Reward for Agent 5 this episode : -0.09\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1554\n",
      "Average Reward for Agent 6 this episode : -1.48\n",
      "Saving architecture, weights, optimizer state for best agent-6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.1270\n",
      "Average Reward for Agent 7 this episode : -1.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.8199\n",
      "Average Reward for Agent 8 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6326\n",
      "Average Reward for Agent 9 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5748\n",
      "Average Reward for Agent 10 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1429\n",
      "Average Reward for Agent 11 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9640\n",
      "Average Reward for Agent 12 this episode : -2.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8368\n",
      "Average Reward for Agent 13 this episode : -8.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.6031\n",
      "Reducing exploration for all agents to 0.5179\n",
      "Episode 39 is finished\n",
      "Average Reward for Agent 0 this episode : -10.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.2367\n",
      "Average Reward for Agent 1 this episode : -11.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.9042\n",
      "Average Reward for Agent 2 this episode : -22.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.7094\n",
      "Average Reward for Agent 3 this episode : -7.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.6020\n",
      "Average Reward for Agent 4 this episode : -3.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1767\n",
      "Average Reward for Agent 5 this episode : -0.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1655\n",
      "Average Reward for Agent 6 this episode : -1.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.4344\n",
      "Average Reward for Agent 7 this episode : -1.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.0708\n",
      "Average Reward for Agent 8 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5585\n",
      "Average Reward for Agent 9 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8949\n",
      "Average Reward for Agent 10 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2671\n",
      "Average Reward for Agent 11 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9567\n",
      "Average Reward for Agent 12 this episode : -2.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4346\n",
      "Average Reward for Agent 13 this episode : -6.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.2085\n",
      "Reducing exploration for all agents to 0.5091\n",
      "Episode 40 is finished\n",
      "Average Reward for Agent 0 this episode : -9.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.1320\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -12.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.3267\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -20.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.7178\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -9.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.1254\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -3.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.8881\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3144\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.7268\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -2.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5050\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9526\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8064\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -0.5\n",
      "Saving architecture, weights, optimizer state for best agent-10\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent10_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9302\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8705\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -2.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4991\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -4.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.7483\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.5003\n",
      "Episode 41 is finished\n",
      "Average Reward for Agent 0 this episode : -12.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.7857\n",
      "Average Reward for Agent 1 this episode : -10.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.8763\n",
      "Average Reward for Agent 2 this episode : -19.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 153.3599\n",
      "Average Reward for Agent 3 this episode : -10.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.4128\n",
      "Average Reward for Agent 4 this episode : -4.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.7377\n",
      "Average Reward for Agent 5 this episode : -0.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4094\n",
      "Average Reward for Agent 6 this episode : -1.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.6943\n",
      "Average Reward for Agent 7 this episode : -2.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6375\n",
      "Average Reward for Agent 8 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6344\n",
      "Average Reward for Agent 9 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4243\n",
      "Average Reward for Agent 10 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3693\n",
      "Average Reward for Agent 11 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1212\n",
      "Average Reward for Agent 12 this episode : -2.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4337\n",
      "Average Reward for Agent 13 this episode : -15.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.9785\n",
      "Reducing exploration for all agents to 0.4917\n",
      "Episode 42 is finished\n",
      "Average Reward for Agent 0 this episode : -12.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.9652\n",
      "Average Reward for Agent 1 this episode : -11.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.5608\n",
      "Average Reward for Agent 2 this episode : -19.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 115.8164\n",
      "Average Reward for Agent 3 this episode : -8.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.9795\n",
      "Average Reward for Agent 4 this episode : -3.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.8107\n",
      "Average Reward for Agent 5 this episode : -0.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2267\n",
      "Average Reward for Agent 6 this episode : -1.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.5281\n",
      "Average Reward for Agent 7 this episode : -2.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.8151\n",
      "Average Reward for Agent 8 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5257\n",
      "Average Reward for Agent 9 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9328\n",
      "Average Reward for Agent 10 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9680\n",
      "Average Reward for Agent 11 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8431\n",
      "Average Reward for Agent 12 this episode : -2.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9056\n",
      "Average Reward for Agent 13 this episode : -4.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.1735\n",
      "Reducing exploration for all agents to 0.4833\n",
      "Episode 43 is finished\n",
      "Average Reward for Agent 0 this episode : -10.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.7190\n",
      "Average Reward for Agent 1 this episode : -10.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.8558\n",
      "Average Reward for Agent 2 this episode : -29.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 142.2884\n",
      "Average Reward for Agent 3 this episode : -8.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.2153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 4 this episode : -5.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.3875\n",
      "Average Reward for Agent 5 this episode : -0.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2412\n",
      "Average Reward for Agent 6 this episode : -2.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.9014\n",
      "Average Reward for Agent 7 this episode : -2.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9010\n",
      "Average Reward for Agent 8 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4684\n",
      "Average Reward for Agent 9 this episode : -0.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9367\n",
      "Average Reward for Agent 10 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2266\n",
      "Average Reward for Agent 11 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8588\n",
      "Average Reward for Agent 12 this episode : -2.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.8959\n",
      "Average Reward for Agent 13 this episode : -16.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.8258\n",
      "Reducing exploration for all agents to 0.475\n",
      "Episode 44 is finished\n",
      "Average Reward for Agent 0 this episode : -9.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.1554\n",
      "Average Reward for Agent 1 this episode : -10.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.0760\n",
      "Average Reward for Agent 2 this episode : -27.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.3107\n",
      "Average Reward for Agent 3 this episode : -9.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.3299\n",
      "Average Reward for Agent 4 this episode : -6.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.5158\n",
      "Average Reward for Agent 5 this episode : -0.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2810\n",
      "Average Reward for Agent 6 this episode : -2.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.8377\n",
      "Average Reward for Agent 7 this episode : -3.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3971\n",
      "Average Reward for Agent 8 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6378\n",
      "Average Reward for Agent 9 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9498\n",
      "Average Reward for Agent 10 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0922\n",
      "Average Reward for Agent 11 this episode : -0.35\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6935\n",
      "Average Reward for Agent 12 this episode : -2.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.5550\n",
      "Average Reward for Agent 13 this episode : -26.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 100.7666\n",
      "Reducing exploration for all agents to 0.4668\n",
      "Episode 45 is finished\n",
      "Average Reward for Agent 0 this episode : -9.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.7147\n",
      "Average Reward for Agent 1 this episode : -21.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.2642\n",
      "Average Reward for Agent 2 this episode : -37.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 193.5515\n",
      "Average Reward for Agent 3 this episode : -10.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.6606\n",
      "Average Reward for Agent 4 this episode : -3.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.9205\n",
      "Average Reward for Agent 5 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3391\n",
      "Average Reward for Agent 6 this episode : -3.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.3904\n",
      "Average Reward for Agent 7 this episode : -3.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.5628\n",
      "Average Reward for Agent 8 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5547\n",
      "Average Reward for Agent 9 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7830\n",
      "Average Reward for Agent 10 this episode : -0.47\n",
      "Saving architecture, weights, optimizer state for best agent-10\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent10_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2074\n",
      "Average Reward for Agent 11 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7431\n",
      "Average Reward for Agent 12 this episode : -3.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6235\n",
      "Average Reward for Agent 13 this episode : -32.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.0271\n",
      "Reducing exploration for all agents to 0.4588\n",
      "Episode 46 is finished\n",
      "Average Reward for Agent 0 this episode : -10.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.2220\n",
      "Average Reward for Agent 1 this episode : -23.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.5991\n",
      "Average Reward for Agent 2 this episode : -41.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 172.1601\n",
      "Average Reward for Agent 3 this episode : -8.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.5576\n",
      "Average Reward for Agent 4 this episode : -1.78\n",
      "Saving architecture, weights, optimizer state for best agent-4\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent4_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.7936\n",
      "Average Reward for Agent 5 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2457\n",
      "Average Reward for Agent 6 this episode : -4.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.4021\n",
      "Average Reward for Agent 7 this episode : -5.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.6160\n",
      "Average Reward for Agent 8 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7503\n",
      "Average Reward for Agent 9 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7109\n",
      "Average Reward for Agent 10 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0936\n",
      "Average Reward for Agent 11 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5926\n",
      "Average Reward for Agent 12 this episode : -2.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3191\n",
      "Average Reward for Agent 13 this episode : -28.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.8222\n",
      "Reducing exploration for all agents to 0.451\n",
      "Episode 47 is finished\n",
      "Average Reward for Agent 0 this episode : -11.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.8348\n",
      "Average Reward for Agent 1 this episode : -20.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 136.3193\n",
      "Average Reward for Agent 2 this episode : -47.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 178.0925\n",
      "Average Reward for Agent 3 this episode : -8.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.9280\n",
      "Average Reward for Agent 4 this episode : -1.35\n",
      "Saving architecture, weights, optimizer state for best agent-4\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent4_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.5064\n",
      "Average Reward for Agent 5 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2148\n",
      "Average Reward for Agent 6 this episode : -14.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 260.6093\n",
      "Average Reward for Agent 7 this episode : -3.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.2614\n",
      "Average Reward for Agent 8 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3739\n",
      "Average Reward for Agent 9 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6204\n",
      "Average Reward for Agent 10 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9655\n",
      "Average Reward for Agent 11 this episode : -1.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8933\n",
      "Average Reward for Agent 12 this episode : -4.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2576\n",
      "Average Reward for Agent 13 this episode : -33.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.2881\n",
      "Reducing exploration for all agents to 0.4432\n",
      "Episode 48 is finished\n",
      "Average Reward for Agent 0 this episode : -11.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.0204\n",
      "Average Reward for Agent 1 this episode : -16.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.5814\n",
      "Average Reward for Agent 2 this episode : -29.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 186.8422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 3 this episode : -10.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.9440\n",
      "Average Reward for Agent 4 this episode : -1.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.1943\n",
      "Average Reward for Agent 5 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2185\n",
      "Average Reward for Agent 6 this episode : -4.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.4624\n",
      "Average Reward for Agent 7 this episode : -2.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6194\n",
      "Average Reward for Agent 8 this episode : -0.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4179\n",
      "Average Reward for Agent 9 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3241\n",
      "Average Reward for Agent 10 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2328\n",
      "Average Reward for Agent 11 this episode : -1.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1671\n",
      "Average Reward for Agent 12 this episode : -2.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6406\n",
      "Average Reward for Agent 13 this episode : -25.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.2646\n",
      "Reducing exploration for all agents to 0.4356\n",
      "Episode 49 is finished\n",
      "Average Reward for Agent 0 this episode : -7.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.9264\n",
      "Average Reward for Agent 1 this episode : -17.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.0487\n",
      "Average Reward for Agent 2 this episode : -29.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.8073\n",
      "Average Reward for Agent 3 this episode : -8.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.2823\n",
      "Average Reward for Agent 4 this episode : -2.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.2292\n",
      "Average Reward for Agent 5 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1984\n",
      "Average Reward for Agent 6 this episode : -3.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.8121\n",
      "Average Reward for Agent 7 this episode : -2.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2617\n",
      "Average Reward for Agent 8 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6074\n",
      "Average Reward for Agent 9 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0768\n",
      "Average Reward for Agent 10 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1522\n",
      "Average Reward for Agent 11 this episode : -1.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5257\n",
      "Average Reward for Agent 12 this episode : -2.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4934\n",
      "Average Reward for Agent 13 this episode : -33.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.8524\n",
      "Reducing exploration for all agents to 0.4281\n",
      "Episode 50 is finished\n",
      "Average Reward for Agent 0 this episode : -10.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.2176\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -19.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 129.8694\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -23.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 156.7107\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -10.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.7932\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -7.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.3664\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2011\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -2.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.0222\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -1.62\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.6924\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5200\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8766\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7618\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6266\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -2.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.1210\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -13.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.2551\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.4208\n",
      "Episode 51 is finished\n",
      "Average Reward for Agent 0 this episode : -11.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.6255\n",
      "Average Reward for Agent 1 this episode : -13.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 192.3026\n",
      "Average Reward for Agent 2 this episode : -18.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 292.6200\n",
      "Average Reward for Agent 3 this episode : -12.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.0584\n",
      "Average Reward for Agent 4 this episode : -17.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.5030\n",
      "Average Reward for Agent 5 this episode : -0.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4954\n",
      "Average Reward for Agent 6 this episode : -1.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.6249\n",
      "Average Reward for Agent 7 this episode : -1.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9787\n",
      "Average Reward for Agent 8 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4886\n",
      "Average Reward for Agent 9 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9278\n",
      "Average Reward for Agent 10 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9165\n",
      "Average Reward for Agent 11 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1912\n",
      "Average Reward for Agent 12 this episode : -2.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.5767\n",
      "Average Reward for Agent 13 this episode : -3.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 222.6239\n",
      "Reducing exploration for all agents to 0.4136\n",
      "Episode 52 is finished\n",
      "Average Reward for Agent 0 this episode : -12.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.4650\n",
      "Average Reward for Agent 1 this episode : -10.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.8535\n",
      "Average Reward for Agent 2 this episode : -25.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 212.3273\n",
      "Average Reward for Agent 3 this episode : -9.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.5273\n",
      "Average Reward for Agent 4 this episode : -4.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.3488\n",
      "Average Reward for Agent 5 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4161\n",
      "Average Reward for Agent 6 this episode : -2.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.8371\n",
      "Average Reward for Agent 7 this episode : -1.59\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6241\n",
      "Average Reward for Agent 8 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5859\n",
      "Average Reward for Agent 9 this episode : -1.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1374\n",
      "Average Reward for Agent 10 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0216\n",
      "Average Reward for Agent 11 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6293\n",
      "Average Reward for Agent 12 this episode : -2.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3675\n",
      "Average Reward for Agent 13 this episode : -4.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.7481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.4065\n",
      "Episode 53 is finished\n",
      "Average Reward for Agent 0 this episode : -12.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.3458\n",
      "Average Reward for Agent 1 this episode : -9.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.9371\n",
      "Average Reward for Agent 2 this episode : -31.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 149.7209\n",
      "Average Reward for Agent 3 this episode : -9.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.4396\n",
      "Average Reward for Agent 4 this episode : -1.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.0562\n",
      "Average Reward for Agent 5 this episode : -0.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3508\n",
      "Average Reward for Agent 6 this episode : -2.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.7345\n",
      "Average Reward for Agent 7 this episode : -1.58\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3836\n",
      "Average Reward for Agent 8 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4787\n",
      "Average Reward for Agent 9 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4480\n",
      "Average Reward for Agent 10 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8625\n",
      "Average Reward for Agent 11 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3135\n",
      "Average Reward for Agent 12 this episode : -3.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6464\n",
      "Average Reward for Agent 13 this episode : -5.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.9759\n",
      "Reducing exploration for all agents to 0.3995\n",
      "Episode 54 is finished\n",
      "Average Reward for Agent 0 this episode : -10.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.9388\n",
      "Average Reward for Agent 1 this episode : -13.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 223.3953\n",
      "Average Reward for Agent 2 this episode : -24.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 213.2782\n",
      "Average Reward for Agent 3 this episode : -10.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.1826\n",
      "Average Reward for Agent 4 this episode : -7.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.1030\n",
      "Average Reward for Agent 5 this episode : -0.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2349\n",
      "Average Reward for Agent 6 this episode : -2.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.1132\n",
      "Average Reward for Agent 7 this episode : -1.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6152\n",
      "Average Reward for Agent 8 this episode : -0.19\n",
      "Saving architecture, weights, optimizer state for best agent-8\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3857\n",
      "Average Reward for Agent 9 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1926\n",
      "Average Reward for Agent 10 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5761\n",
      "Average Reward for Agent 11 this episode : -1.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5393\n",
      "Average Reward for Agent 12 this episode : -4.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0597\n",
      "Average Reward for Agent 13 this episode : -18.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.0030\n",
      "Reducing exploration for all agents to 0.3926\n",
      "Episode 55 is finished\n",
      "Average Reward for Agent 0 this episode : -11.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.9425\n",
      "Average Reward for Agent 1 this episode : -11.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.7806\n",
      "Average Reward for Agent 2 this episode : -26.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 258.6655\n",
      "Average Reward for Agent 3 this episode : -9.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.2684\n",
      "Average Reward for Agent 4 this episode : -7.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.1861\n",
      "Average Reward for Agent 5 this episode : -0.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2853\n",
      "Average Reward for Agent 6 this episode : -1.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.8400\n",
      "Average Reward for Agent 7 this episode : -1.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1852\n",
      "Average Reward for Agent 8 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5815\n",
      "Average Reward for Agent 9 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8289\n",
      "Average Reward for Agent 10 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5921\n",
      "Average Reward for Agent 11 this episode : -2.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3312\n",
      "Average Reward for Agent 12 this episode : -2.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3510\n",
      "Average Reward for Agent 13 this episode : -4.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.0519\n",
      "Reducing exploration for all agents to 0.3859\n",
      "Episode 56 is finished\n",
      "Average Reward for Agent 0 this episode : -10.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.8503\n",
      "Average Reward for Agent 1 this episode : -10.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.7282\n",
      "Average Reward for Agent 2 this episode : -31.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 217.2217\n",
      "Average Reward for Agent 3 this episode : -8.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.3794\n",
      "Average Reward for Agent 4 this episode : -7.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.8801\n",
      "Average Reward for Agent 5 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3408\n",
      "Average Reward for Agent 6 this episode : -2.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.0777\n",
      "Average Reward for Agent 7 this episode : -1.35\n",
      "Saving architecture, weights, optimizer state for best agent-7\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent7_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6580\n",
      "Average Reward for Agent 8 this episode : -0.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6492\n",
      "Average Reward for Agent 9 this episode : -1.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8588\n",
      "Average Reward for Agent 10 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6991\n",
      "Average Reward for Agent 11 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9183\n",
      "Average Reward for Agent 12 this episode : -2.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3575\n",
      "Average Reward for Agent 13 this episode : -17.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 100.7156\n",
      "Reducing exploration for all agents to 0.3793\n",
      "Episode 57 is finished\n",
      "Average Reward for Agent 0 this episode : -11.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.2463\n",
      "Average Reward for Agent 1 this episode : -11.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.7524\n",
      "Average Reward for Agent 2 this episode : -28.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 235.7295\n",
      "Average Reward for Agent 3 this episode : -8.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.4242\n",
      "Average Reward for Agent 4 this episode : -15.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.1282\n",
      "Average Reward for Agent 5 this episode : -0.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2763\n",
      "Average Reward for Agent 6 this episode : -3.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.8419\n",
      "Average Reward for Agent 7 this episode : -2.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.8416\n",
      "Average Reward for Agent 8 this episode : -0.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4829\n",
      "Average Reward for Agent 9 this episode : -1.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6567\n",
      "Average Reward for Agent 10 this episode : -0.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7821\n",
      "Average Reward for Agent 11 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1406\n",
      "Average Reward for Agent 12 this episode : -2.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5468\n",
      "Average Reward for Agent 13 this episode : -15.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.0805\n",
      "Reducing exploration for all agents to 0.3728\n",
      "Episode 58 is finished\n",
      "Average Reward for Agent 0 this episode : -13.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.8895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 1 this episode : -12.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.7687\n",
      "Average Reward for Agent 2 this episode : -34.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 236.8978\n",
      "Average Reward for Agent 3 this episode : -8.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.5013\n",
      "Average Reward for Agent 4 this episode : -5.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2722\n",
      "Average Reward for Agent 5 this episode : -0.09\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3733\n",
      "Average Reward for Agent 6 this episode : -2.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.3083\n",
      "Average Reward for Agent 7 this episode : -1.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6963\n",
      "Average Reward for Agent 8 this episode : -0.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4693\n",
      "Average Reward for Agent 9 this episode : -1.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8937\n",
      "Average Reward for Agent 10 this episode : -0.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7791\n",
      "Average Reward for Agent 11 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8163\n",
      "Average Reward for Agent 12 this episode : -3.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6160\n",
      "Average Reward for Agent 13 this episode : -17.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.0950\n",
      "Reducing exploration for all agents to 0.3664\n",
      "Episode 59 is finished\n",
      "Average Reward for Agent 0 this episode : -10.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.6070\n",
      "Average Reward for Agent 1 this episode : -13.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 116.2994\n",
      "Average Reward for Agent 2 this episode : -47.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 260.8953\n",
      "Average Reward for Agent 3 this episode : -8.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.0276\n",
      "Average Reward for Agent 4 this episode : -22.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.9815\n",
      "Average Reward for Agent 5 this episode : -0.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1724\n",
      "Average Reward for Agent 6 this episode : -2.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.4225\n",
      "Average Reward for Agent 7 this episode : -2.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5798\n",
      "Average Reward for Agent 8 this episode : -0.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3823\n",
      "Average Reward for Agent 9 this episode : -1.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7093\n",
      "Average Reward for Agent 10 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9742\n",
      "Average Reward for Agent 11 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5009\n",
      "Average Reward for Agent 12 this episode : -3.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0583\n",
      "Average Reward for Agent 13 this episode : -13.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.3905\n",
      "Reducing exploration for all agents to 0.3601\n",
      "Episode 60 is finished\n",
      "Average Reward for Agent 0 this episode : -11.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.3828\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -12.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.4705\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -50.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 214.3582\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -8.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.5919\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -3.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1199\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1945\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -4.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.3217\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -1.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0425\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4508\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9403\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8538\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9432\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -3.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4235\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -16.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.2455\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.3539\n",
      "Episode 61 is finished\n",
      "Average Reward for Agent 0 this episode : -12.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.7195\n",
      "Average Reward for Agent 1 this episode : -9.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 121.5265\n",
      "Average Reward for Agent 2 this episode : -49.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 569.3131\n",
      "Average Reward for Agent 3 this episode : -10.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.6987\n",
      "Average Reward for Agent 4 this episode : -9.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.9542\n",
      "Average Reward for Agent 5 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2081\n",
      "Average Reward for Agent 6 this episode : -7.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.1836\n",
      "Average Reward for Agent 7 this episode : -2.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7030\n",
      "Average Reward for Agent 8 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4023\n",
      "Average Reward for Agent 9 this episode : -0.23\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.2839\n",
      "Average Reward for Agent 10 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8149\n",
      "Average Reward for Agent 11 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6254\n",
      "Average Reward for Agent 12 this episode : -4.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.0263\n",
      "Average Reward for Agent 13 this episode : -18.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.9789\n",
      "Reducing exploration for all agents to 0.3478\n",
      "Episode 62 is finished\n",
      "Average Reward for Agent 0 this episode : -12.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.8670\n",
      "Average Reward for Agent 1 this episode : -10.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.3025\n",
      "Average Reward for Agent 2 this episode : -47.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 307.8611\n",
      "Average Reward for Agent 3 this episode : -9.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.6625\n",
      "Average Reward for Agent 4 this episode : -4.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.0326\n",
      "Average Reward for Agent 5 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1925\n",
      "Average Reward for Agent 6 this episode : -8.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.4963\n",
      "Average Reward for Agent 7 this episode : -3.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0797\n",
      "Average Reward for Agent 8 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6961\n",
      "Average Reward for Agent 9 this episode : -0.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3000\n",
      "Average Reward for Agent 10 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9037\n",
      "Average Reward for Agent 11 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 12 this episode : -3.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5499\n",
      "Average Reward for Agent 13 this episode : -7.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.2441\n",
      "Reducing exploration for all agents to 0.3418\n",
      "Episode 63 is finished\n",
      "Average Reward for Agent 0 this episode : -12.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.3810\n",
      "Average Reward for Agent 1 this episode : -11.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 89.5265\n",
      "Average Reward for Agent 2 this episode : -24.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 751.3621\n",
      "Average Reward for Agent 3 this episode : -9.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.0901\n",
      "Average Reward for Agent 4 this episode : -8.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.3409\n",
      "Average Reward for Agent 5 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1601\n",
      "Average Reward for Agent 6 this episode : -14.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 126.5343\n",
      "Average Reward for Agent 7 this episode : -4.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9899\n",
      "Average Reward for Agent 8 this episode : -0.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7477\n",
      "Average Reward for Agent 9 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1034\n",
      "Average Reward for Agent 10 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6270\n",
      "Average Reward for Agent 11 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1160\n",
      "Average Reward for Agent 12 this episode : -2.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3277\n",
      "Average Reward for Agent 13 this episode : -9.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.5996\n",
      "Reducing exploration for all agents to 0.336\n",
      "Episode 64 is finished\n",
      "Average Reward for Agent 0 this episode : -12.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.3977\n",
      "Average Reward for Agent 1 this episode : -10.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 148.0994\n",
      "Average Reward for Agent 2 this episode : -25.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 466.9989\n",
      "Average Reward for Agent 3 this episode : -9.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.2566\n",
      "Average Reward for Agent 4 this episode : -17.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.2627\n",
      "Average Reward for Agent 5 this episode : -0.08\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2844\n",
      "Average Reward for Agent 6 this episode : -11.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 199.3979\n",
      "Average Reward for Agent 7 this episode : -7.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1830\n",
      "Average Reward for Agent 8 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6696\n",
      "Average Reward for Agent 9 this episode : -1.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0580\n",
      "Average Reward for Agent 10 this episode : -0.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5552\n",
      "Average Reward for Agent 11 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9682\n",
      "Average Reward for Agent 12 this episode : -2.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.7850\n",
      "Average Reward for Agent 13 this episode : -6.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.1972\n",
      "Reducing exploration for all agents to 0.3302\n",
      "Episode 65 is finished\n",
      "Average Reward for Agent 0 this episode : -10.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.2585\n",
      "Average Reward for Agent 1 this episode : -10.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 111.8518\n",
      "Average Reward for Agent 2 this episode : -49.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 385.6602\n",
      "Average Reward for Agent 3 this episode : -11.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.7832\n",
      "Average Reward for Agent 4 this episode : -5.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.3598\n",
      "Average Reward for Agent 5 this episode : -0.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1402\n",
      "Average Reward for Agent 6 this episode : -8.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 161.9142\n",
      "Average Reward for Agent 7 this episode : -8.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1612\n",
      "Average Reward for Agent 8 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4876\n",
      "Average Reward for Agent 9 this episode : -1.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6823\n",
      "Average Reward for Agent 10 this episode : -1.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8722\n",
      "Average Reward for Agent 11 this episode : -0.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1912\n",
      "Average Reward for Agent 12 this episode : -2.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7589\n",
      "Average Reward for Agent 13 this episode : -22.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 136.8909\n",
      "Reducing exploration for all agents to 0.3245\n",
      "Episode 66 is finished\n",
      "Average Reward for Agent 0 this episode : -11.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.1849\n",
      "Average Reward for Agent 1 this episode : -10.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.4081\n",
      "Average Reward for Agent 2 this episode : -43.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 444.4622\n",
      "Average Reward for Agent 3 this episode : -9.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.6013\n",
      "Average Reward for Agent 4 this episode : -7.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.1052\n",
      "Average Reward for Agent 5 this episode : -0.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2401\n",
      "Average Reward for Agent 6 this episode : -4.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 212.6611\n",
      "Average Reward for Agent 7 this episode : -6.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.3394\n",
      "Average Reward for Agent 8 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5224\n",
      "Average Reward for Agent 9 this episode : -1.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5495\n",
      "Average Reward for Agent 10 this episode : -1.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9521\n",
      "Average Reward for Agent 11 this episode : -1.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3411\n",
      "Average Reward for Agent 12 this episode : -2.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3462\n",
      "Average Reward for Agent 13 this episode : -19.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 122.3022\n",
      "Reducing exploration for all agents to 0.319\n",
      "Episode 67 is finished\n",
      "Average Reward for Agent 0 this episode : -13.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.1093\n",
      "Average Reward for Agent 1 this episode : -11.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.9039\n",
      "Average Reward for Agent 2 this episode : -48.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 494.4794\n",
      "Average Reward for Agent 3 this episode : -9.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.7411\n",
      "Average Reward for Agent 4 this episode : -4.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.7509\n",
      "Average Reward for Agent 5 this episode : -0.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2276\n",
      "Average Reward for Agent 6 this episode : -9.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 212.2350\n",
      "Average Reward for Agent 7 this episode : -10.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.3149\n",
      "Average Reward for Agent 8 this episode : -0.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5035\n",
      "Average Reward for Agent 9 this episode : -1.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7401\n",
      "Average Reward for Agent 10 this episode : -1.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4347\n",
      "Average Reward for Agent 11 this episode : -1.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0801\n",
      "Average Reward for Agent 12 this episode : -3.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3178\n",
      "Average Reward for Agent 13 this episode : -26.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.4173\n",
      "Reducing exploration for all agents to 0.3135\n",
      "Episode 68 is finished\n",
      "Average Reward for Agent 0 this episode : -15.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.1802\n",
      "Average Reward for Agent 1 this episode : -11.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 111.8314\n",
      "Average Reward for Agent 2 this episode : -45.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 486.4888\n",
      "Average Reward for Agent 3 this episode : -8.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.1459\n",
      "Average Reward for Agent 4 this episode : -5.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.1883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 5 this episode : -0.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1742\n",
      "Average Reward for Agent 6 this episode : -8.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 160.5002\n",
      "Average Reward for Agent 7 this episode : -10.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.2319\n",
      "Average Reward for Agent 8 this episode : -0.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6245\n",
      "Average Reward for Agent 9 this episode : -1.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3522\n",
      "Average Reward for Agent 10 this episode : -1.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4014\n",
      "Average Reward for Agent 11 this episode : -0.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5022\n",
      "Average Reward for Agent 12 this episode : -2.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4792\n",
      "Average Reward for Agent 13 this episode : -22.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 176.7242\n",
      "Reducing exploration for all agents to 0.3081\n",
      "Episode 69 is finished\n",
      "Average Reward for Agent 0 this episode : -15.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.3858\n",
      "Average Reward for Agent 1 this episode : -10.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.4859\n",
      "Average Reward for Agent 2 this episode : -37.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 340.1132\n",
      "Average Reward for Agent 3 this episode : -11.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.4192\n",
      "Average Reward for Agent 4 this episode : -6.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.9041\n",
      "Average Reward for Agent 5 this episode : -0.07\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1590\n",
      "Average Reward for Agent 6 this episode : -17.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 201.9056\n",
      "Average Reward for Agent 7 this episode : -12.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.6936\n",
      "Average Reward for Agent 8 this episode : -0.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6154\n",
      "Average Reward for Agent 9 this episode : -0.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9966\n",
      "Average Reward for Agent 10 this episode : -1.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7680\n",
      "Average Reward for Agent 11 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3363\n",
      "Average Reward for Agent 12 this episode : -4.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4327\n",
      "Average Reward for Agent 13 this episode : -24.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 207.5449\n",
      "Reducing exploration for all agents to 0.3028\n",
      "Episode 70 is finished\n",
      "Average Reward for Agent 0 this episode : -14.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.1936\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -11.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.7008\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -32.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 478.1192\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -10.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.7993\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -3.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.3106\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1663\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -13.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 155.1474\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -12.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.4749\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5504\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -1.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4980\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -1.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6249\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3379\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -3.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3398\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -36.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 215.2323\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.2976\n",
      "Episode 71 is finished\n",
      "Average Reward for Agent 0 this episode : -17.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.9660\n",
      "Average Reward for Agent 1 this episode : -9.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 111.3465\n",
      "Average Reward for Agent 2 this episode : -27.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 587.5759\n",
      "Average Reward for Agent 3 this episode : -11.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.1749\n",
      "Average Reward for Agent 4 this episode : -6.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.0850\n",
      "Average Reward for Agent 5 this episode : -0.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1160\n",
      "Average Reward for Agent 6 this episode : -25.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 251.7942\n",
      "Average Reward for Agent 7 this episode : -11.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.4411\n",
      "Average Reward for Agent 8 this episode : -0.18\n",
      "Saving architecture, weights, optimizer state for best agent-8\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5155\n",
      "Average Reward for Agent 9 this episode : -1.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5381\n",
      "Average Reward for Agent 10 this episode : -1.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7321\n",
      "Average Reward for Agent 11 this episode : -0.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3295\n",
      "Average Reward for Agent 12 this episode : -4.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3802\n",
      "Average Reward for Agent 13 this episode : -26.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 346.7922\n",
      "Reducing exploration for all agents to 0.2925\n",
      "Episode 72 is finished\n",
      "Average Reward for Agent 0 this episode : -12.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.7832\n",
      "Average Reward for Agent 1 this episode : -7.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.4388\n",
      "Average Reward for Agent 2 this episode : -25.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 286.9981\n",
      "Average Reward for Agent 3 this episode : -9.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.1314\n",
      "Average Reward for Agent 4 this episode : -10.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.4251\n",
      "Average Reward for Agent 5 this episode : -0.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0810\n",
      "Average Reward for Agent 6 this episode : -8.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 225.7781\n",
      "Average Reward for Agent 7 this episode : -12.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.0243\n",
      "Average Reward for Agent 8 this episode : -0.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3690\n",
      "Average Reward for Agent 9 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8176\n",
      "Average Reward for Agent 10 this episode : -0.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1940\n",
      "Average Reward for Agent 11 this episode : -2.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9341\n",
      "Average Reward for Agent 12 this episode : -3.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0436\n",
      "Average Reward for Agent 13 this episode : -44.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 201.4530\n",
      "Reducing exploration for all agents to 0.2875\n",
      "Episode 73 is finished\n",
      "Average Reward for Agent 0 this episode : -10.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.4292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 1 this episode : -12.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.5686\n",
      "Average Reward for Agent 2 this episode : -23.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 287.3917\n",
      "Average Reward for Agent 3 this episode : -10.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.4717\n",
      "Average Reward for Agent 4 this episode : -17.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.4581\n",
      "Average Reward for Agent 5 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1670\n",
      "Average Reward for Agent 6 this episode : -19.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 134.4130\n",
      "Average Reward for Agent 7 this episode : -13.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.2484\n",
      "Average Reward for Agent 8 this episode : -0.14\n",
      "Saving architecture, weights, optimizer state for best agent-8\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4050\n",
      "Average Reward for Agent 9 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9145\n",
      "Average Reward for Agent 10 this episode : -0.44\n",
      "Saving architecture, weights, optimizer state for best agent-10\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent10_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0036\n",
      "Average Reward for Agent 11 this episode : -4.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2672\n",
      "Average Reward for Agent 12 this episode : -5.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.7670\n",
      "Average Reward for Agent 13 this episode : -47.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 345.2883\n",
      "Reducing exploration for all agents to 0.2826\n",
      "Episode 74 is finished\n",
      "Average Reward for Agent 0 this episode : -9.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.2621\n",
      "Average Reward for Agent 1 this episode : -14.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.3752\n",
      "Average Reward for Agent 2 this episode : -21.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 380.2667\n",
      "Average Reward for Agent 3 this episode : -8.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.3448\n",
      "Average Reward for Agent 4 this episode : -14.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.4292\n",
      "Average Reward for Agent 5 this episode : -0.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2780\n",
      "Average Reward for Agent 6 this episode : -5.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.9109\n",
      "Average Reward for Agent 7 this episode : -13.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.3942\n",
      "Average Reward for Agent 8 this episode : -0.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4551\n",
      "Average Reward for Agent 9 this episode : -1.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8515\n",
      "Average Reward for Agent 10 this episode : -1.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0761\n",
      "Average Reward for Agent 11 this episode : -0.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3276\n",
      "Average Reward for Agent 12 this episode : -4.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4890\n",
      "Average Reward for Agent 13 this episode : -22.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 243.4071\n",
      "Reducing exploration for all agents to 0.2777\n",
      "Episode 75 is finished\n",
      "Average Reward for Agent 0 this episode : -9.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.9799\n",
      "Average Reward for Agent 1 this episode : -9.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.9323\n",
      "Average Reward for Agent 2 this episode : -25.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 504.1471\n",
      "Average Reward for Agent 3 this episode : -10.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.8908\n",
      "Average Reward for Agent 4 this episode : -16.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.4792\n",
      "Average Reward for Agent 5 this episode : -0.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0793\n",
      "Average Reward for Agent 6 this episode : -3.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 115.0819\n",
      "Average Reward for Agent 7 this episode : -13.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.4536\n",
      "Average Reward for Agent 8 this episode : -0.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4013\n",
      "Average Reward for Agent 9 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6644\n",
      "Average Reward for Agent 10 this episode : -1.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8497\n",
      "Average Reward for Agent 11 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0660\n",
      "Average Reward for Agent 12 this episode : -3.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.0755\n",
      "Average Reward for Agent 13 this episode : -49.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 187.4326\n",
      "Reducing exploration for all agents to 0.273\n",
      "Episode 76 is finished\n",
      "Average Reward for Agent 0 this episode : -11.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.5614\n",
      "Average Reward for Agent 1 this episode : -14.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.4486\n",
      "Average Reward for Agent 2 this episode : -20.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 484.0334\n",
      "Average Reward for Agent 3 this episode : -11.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.3033\n",
      "Average Reward for Agent 4 this episode : -13.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.7358\n",
      "Average Reward for Agent 5 this episode : -0.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2675\n",
      "Average Reward for Agent 6 this episode : -2.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 186.0912\n",
      "Average Reward for Agent 7 this episode : -13.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.8910\n",
      "Average Reward for Agent 8 this episode : -0.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4734\n",
      "Average Reward for Agent 9 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1678\n",
      "Average Reward for Agent 10 this episode : -1.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7327\n",
      "Average Reward for Agent 11 this episode : -0.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0652\n",
      "Average Reward for Agent 12 this episode : -2.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.9050\n",
      "Average Reward for Agent 13 this episode : -3.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 238.1291\n",
      "Reducing exploration for all agents to 0.2683\n",
      "Episode 77 is finished\n",
      "Average Reward for Agent 0 this episode : -14.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.1865\n",
      "Average Reward for Agent 1 this episode : -11.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.5381\n",
      "Average Reward for Agent 2 this episode : -24.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 231.6999\n",
      "Average Reward for Agent 3 this episode : -9.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.7882\n",
      "Average Reward for Agent 4 this episode : -16.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.4861\n",
      "Average Reward for Agent 5 this episode : -0.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3322\n",
      "Average Reward for Agent 6 this episode : -3.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 100.6278\n",
      "Average Reward for Agent 7 this episode : -13.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.9315\n",
      "Average Reward for Agent 8 this episode : -0.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0331\n",
      "Average Reward for Agent 9 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1577\n",
      "Average Reward for Agent 10 this episode : -0.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1994\n",
      "Average Reward for Agent 11 this episode : -1.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7393\n",
      "Average Reward for Agent 12 this episode : -6.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.4429\n",
      "Average Reward for Agent 13 this episode : -7.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.7340\n",
      "Reducing exploration for all agents to 0.2637\n",
      "Episode 78 is finished\n",
      "Average Reward for Agent 0 this episode : -12.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.7397\n",
      "Average Reward for Agent 1 this episode : -15.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.4030\n",
      "Average Reward for Agent 2 this episode : -27.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 283.7814\n",
      "Average Reward for Agent 3 this episode : -12.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.5238\n",
      "Average Reward for Agent 4 this episode : -12.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 89.9158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 5 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3859\n",
      "Average Reward for Agent 6 this episode : -2.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.0384\n",
      "Average Reward for Agent 7 this episode : -16.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.9905\n",
      "Average Reward for Agent 8 this episode : -1.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3073\n",
      "Average Reward for Agent 9 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.8874\n",
      "Average Reward for Agent 10 this episode : -1.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1786\n",
      "Average Reward for Agent 11 this episode : -2.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8557\n",
      "Average Reward for Agent 12 this episode : -4.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9646\n",
      "Average Reward for Agent 13 this episode : -4.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.8458\n",
      "Reducing exploration for all agents to 0.2591\n",
      "Episode 79 is finished\n",
      "Average Reward for Agent 0 this episode : -12.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.3620\n",
      "Average Reward for Agent 1 this episode : -15.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.6531\n",
      "Average Reward for Agent 2 this episode : -28.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 411.0974\n",
      "Average Reward for Agent 3 this episode : -8.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.1810\n",
      "Average Reward for Agent 4 this episode : -12.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.6900\n",
      "Average Reward for Agent 5 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4950\n",
      "Average Reward for Agent 6 this episode : -1.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.5056\n",
      "Average Reward for Agent 7 this episode : -12.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.3351\n",
      "Average Reward for Agent 8 this episode : -1.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2758\n",
      "Average Reward for Agent 9 this episode : -1.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.3020\n",
      "Average Reward for Agent 10 this episode : -0.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5109\n",
      "Average Reward for Agent 11 this episode : -6.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3046\n",
      "Average Reward for Agent 12 this episode : -4.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.6144\n",
      "Average Reward for Agent 13 this episode : -16.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 129.5518\n",
      "Reducing exploration for all agents to 0.2547\n",
      "Episode 80 is finished\n",
      "Average Reward for Agent 0 this episode : -13.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 129.2135\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -14.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 132.2108\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -30.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 404.5280\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -12.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.0288\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -10.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.8867\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4370\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -9.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.5497\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -14.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.7048\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3002\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -1.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4666\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3435\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -1.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4728\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -1.44\n",
      "Saving architecture, weights, optimizer state for best agent-12\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent12_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.8614\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -5.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.2917\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.2503\n",
      "Episode 81 is finished\n",
      "Average Reward for Agent 0 this episode : -8.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.1631\n",
      "Average Reward for Agent 1 this episode : -14.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 143.4521\n",
      "Average Reward for Agent 2 this episode : -30.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 383.3384\n",
      "Average Reward for Agent 3 this episode : -14.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.1440\n",
      "Average Reward for Agent 4 this episode : -11.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.7410\n",
      "Average Reward for Agent 5 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5775\n",
      "Average Reward for Agent 6 this episode : -2.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.2277\n",
      "Average Reward for Agent 7 this episode : -12.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.1752\n",
      "Average Reward for Agent 8 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.9588\n",
      "Average Reward for Agent 9 this episode : -1.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3070\n",
      "Average Reward for Agent 10 this episode : -3.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8458\n",
      "Average Reward for Agent 11 this episode : -0.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.0776\n",
      "Average Reward for Agent 12 this episode : -3.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4399\n",
      "Average Reward for Agent 13 this episode : -13.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.2104\n",
      "Reducing exploration for all agents to 0.246\n",
      "Episode 82 is finished\n",
      "Average Reward for Agent 0 this episode : -7.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.4452\n",
      "Average Reward for Agent 1 this episode : -15.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 128.7930\n",
      "Average Reward for Agent 2 this episode : -33.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 426.4365\n",
      "Average Reward for Agent 3 this episode : -9.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.2873\n",
      "Average Reward for Agent 4 this episode : -7.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.9785\n",
      "Average Reward for Agent 5 this episode : -0.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3802\n",
      "Average Reward for Agent 6 this episode : -2.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.2110\n",
      "Average Reward for Agent 7 this episode : -13.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.4370\n",
      "Average Reward for Agent 8 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1664\n",
      "Average Reward for Agent 9 this episode : -0.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1499\n",
      "Average Reward for Agent 10 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7793\n",
      "Average Reward for Agent 11 this episode : -0.32\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.0316\n",
      "Average Reward for Agent 12 this episode : -26.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.2223\n",
      "Average Reward for Agent 13 this episode : -18.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 150.2686\n",
      "Reducing exploration for all agents to 0.2418\n",
      "Episode 83 is finished\n",
      "Average Reward for Agent 0 this episode : -7.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.8490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 1 this episode : -18.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.1514\n",
      "Average Reward for Agent 2 this episode : -24.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 319.1600\n",
      "Average Reward for Agent 3 this episode : -10.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.7151\n",
      "Average Reward for Agent 4 this episode : -13.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.0120\n",
      "Average Reward for Agent 5 this episode : -0.07\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3179\n",
      "Average Reward for Agent 6 this episode : -6.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 115.8354\n",
      "Average Reward for Agent 7 this episode : -14.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.8872\n",
      "Average Reward for Agent 8 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1155\n",
      "Average Reward for Agent 9 this episode : -0.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.8776\n",
      "Average Reward for Agent 10 this episode : -1.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5165\n",
      "Average Reward for Agent 11 this episode : -1.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3639\n",
      "Average Reward for Agent 12 this episode : -9.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.1882\n",
      "Average Reward for Agent 13 this episode : -6.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.9806\n",
      "Reducing exploration for all agents to 0.2377\n",
      "Episode 84 is finished\n",
      "Average Reward for Agent 0 this episode : -7.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.7563\n",
      "Average Reward for Agent 1 this episode : -15.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.9687\n",
      "Average Reward for Agent 2 this episode : -24.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 294.6349\n",
      "Average Reward for Agent 3 this episode : -11.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.0566\n",
      "Average Reward for Agent 4 this episode : -8.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.8019\n",
      "Average Reward for Agent 5 this episode : -0.05\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2202\n",
      "Average Reward for Agent 6 this episode : -4.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 139.6499\n",
      "Average Reward for Agent 7 this episode : -13.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.2226\n",
      "Average Reward for Agent 8 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0150\n",
      "Average Reward for Agent 9 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1488\n",
      "Average Reward for Agent 10 this episode : -1.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.9188\n",
      "Average Reward for Agent 11 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.5594\n",
      "Average Reward for Agent 12 this episode : -29.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.0401\n",
      "Average Reward for Agent 13 this episode : -9.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.9416\n",
      "Reducing exploration for all agents to 0.2336\n",
      "Episode 85 is finished\n",
      "Average Reward for Agent 0 this episode : -8.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.2679\n",
      "Average Reward for Agent 1 this episode : -21.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 144.1210\n",
      "Average Reward for Agent 2 this episode : -22.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 238.4385\n",
      "Average Reward for Agent 3 this episode : -9.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.2673\n",
      "Average Reward for Agent 4 this episode : -10.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.6238\n",
      "Average Reward for Agent 5 this episode : -0.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2755\n",
      "Average Reward for Agent 6 this episode : -8.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.9383\n",
      "Average Reward for Agent 7 this episode : -13.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.7247\n",
      "Average Reward for Agent 8 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1726\n",
      "Average Reward for Agent 9 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5775\n",
      "Average Reward for Agent 10 this episode : -2.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.4064\n",
      "Average Reward for Agent 11 this episode : -3.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.1969\n",
      "Average Reward for Agent 12 this episode : -18.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.8015\n",
      "Average Reward for Agent 13 this episode : -10.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 128.7266\n",
      "Reducing exploration for all agents to 0.2296\n",
      "Episode 86 is finished\n",
      "Average Reward for Agent 0 this episode : -7.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.3523\n",
      "Average Reward for Agent 1 this episode : -22.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.1621\n",
      "Average Reward for Agent 2 this episode : -26.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 241.9471\n",
      "Average Reward for Agent 3 this episode : -6.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.1475\n",
      "Average Reward for Agent 4 this episode : -18.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.2602\n",
      "Average Reward for Agent 5 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1502\n",
      "Average Reward for Agent 6 this episode : -20.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 148.9628\n",
      "Average Reward for Agent 7 this episode : -13.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.6767\n",
      "Average Reward for Agent 8 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7360\n",
      "Average Reward for Agent 9 this episode : -1.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1762\n",
      "Average Reward for Agent 10 this episode : -1.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6063\n",
      "Average Reward for Agent 11 this episode : -9.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.7834\n",
      "Average Reward for Agent 12 this episode : -20.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.6897\n",
      "Average Reward for Agent 13 this episode : -15.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 143.3687\n",
      "Reducing exploration for all agents to 0.2256\n",
      "Episode 87 is finished\n",
      "Average Reward for Agent 0 this episode : -8.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.6064\n",
      "Average Reward for Agent 1 this episode : -24.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 132.6003\n",
      "Average Reward for Agent 2 this episode : -31.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 313.8816\n",
      "Average Reward for Agent 3 this episode : -9.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.5581\n",
      "Average Reward for Agent 4 this episode : -18.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.0427\n",
      "Average Reward for Agent 5 this episode : -0.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1473\n",
      "Average Reward for Agent 6 this episode : -25.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 149.6140\n",
      "Average Reward for Agent 7 this episode : -13.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.8418\n",
      "Average Reward for Agent 8 this episode : -0.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1956\n",
      "Average Reward for Agent 9 this episode : -1.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6230\n",
      "Average Reward for Agent 10 this episode : -1.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3710\n",
      "Average Reward for Agent 11 this episode : -7.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.8871\n",
      "Average Reward for Agent 12 this episode : -21.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.6010\n",
      "Average Reward for Agent 13 this episode : -7.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 111.7257\n",
      "Reducing exploration for all agents to 0.2218\n",
      "Episode 88 is finished\n",
      "Average Reward for Agent 0 this episode : -8.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.0731\n",
      "Average Reward for Agent 1 this episode : -15.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.5801\n",
      "Average Reward for Agent 2 this episode : -33.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 234.3640\n",
      "Average Reward for Agent 3 this episode : -10.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.1989\n",
      "Average Reward for Agent 4 this episode : -18.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.5303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 5 this episode : -0.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1663\n",
      "Average Reward for Agent 6 this episode : -9.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 127.8065\n",
      "Average Reward for Agent 7 this episode : -12.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.8809\n",
      "Average Reward for Agent 8 this episode : -2.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7962\n",
      "Average Reward for Agent 9 this episode : -1.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.4988\n",
      "Average Reward for Agent 10 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1673\n",
      "Average Reward for Agent 11 this episode : -15.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.7426\n",
      "Average Reward for Agent 12 this episode : -11.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.5888\n",
      "Average Reward for Agent 13 this episode : -2.24\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent13_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.4924\n",
      "Reducing exploration for all agents to 0.2179\n",
      "Episode 89 is finished\n",
      "Average Reward for Agent 0 this episode : -8.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.2050\n",
      "Average Reward for Agent 1 this episode : -11.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 141.0080\n",
      "Average Reward for Agent 2 this episode : -62.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 404.8332\n",
      "Average Reward for Agent 3 this episode : -9.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.1800\n",
      "Average Reward for Agent 4 this episode : -16.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.7205\n",
      "Average Reward for Agent 5 this episode : -0.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2104\n",
      "Average Reward for Agent 6 this episode : -8.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 187.8987\n",
      "Average Reward for Agent 7 this episode : -11.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.8792\n",
      "Average Reward for Agent 8 this episode : -8.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.2347\n",
      "Average Reward for Agent 9 this episode : -1.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1419\n",
      "Average Reward for Agent 10 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2871\n",
      "Average Reward for Agent 11 this episode : -1.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.6692\n",
      "Average Reward for Agent 12 this episode : -24.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.2642\n",
      "Average Reward for Agent 13 this episode : -12.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.3811\n",
      "Reducing exploration for all agents to 0.2142\n",
      "Episode 90 is finished\n",
      "Average Reward for Agent 0 this episode : -9.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.9404\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -12.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 169.9436\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -44.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 443.1603\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -9.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.5214\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -37.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.8644\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -4.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4101\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -10.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 184.0966\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -12.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.4106\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6620\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -1.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2599\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3085\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -31.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.6639\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -7.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.4210\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -4.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.8341\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.2105\n",
      "Episode 91 is finished\n",
      "Average Reward for Agent 0 this episode : -8.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.8430\n",
      "Average Reward for Agent 1 this episode : -11.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 176.1935\n",
      "Average Reward for Agent 2 this episode : -44.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 605.3844\n",
      "Average Reward for Agent 3 this episode : -11.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.2007\n",
      "Average Reward for Agent 4 this episode : -19.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 179.1351\n",
      "Average Reward for Agent 5 this episode : -0.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.9526\n",
      "Average Reward for Agent 6 this episode : -8.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 176.4241\n",
      "Average Reward for Agent 7 this episode : -13.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.6549\n",
      "Average Reward for Agent 8 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.8928\n",
      "Average Reward for Agent 9 this episode : -1.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.1233\n",
      "Average Reward for Agent 10 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5144\n",
      "Average Reward for Agent 11 this episode : -0.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 153.7889\n",
      "Average Reward for Agent 12 this episode : -14.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.9381\n",
      "Average Reward for Agent 13 this episode : -32.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 142.4135\n",
      "Reducing exploration for all agents to 0.2069\n",
      "Episode 92 is finished\n",
      "Average Reward for Agent 0 this episode : -8.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.7230\n",
      "Average Reward for Agent 1 this episode : -9.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 151.1082\n",
      "Average Reward for Agent 2 this episode : -48.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 500.8323\n",
      "Average Reward for Agent 3 this episode : -11.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.6890\n",
      "Average Reward for Agent 4 this episode : -11.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.1078\n",
      "Average Reward for Agent 5 this episode : -0.05\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2092\n",
      "Average Reward for Agent 6 this episode : -7.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 151.0928\n",
      "Average Reward for Agent 7 this episode : -12.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.5719\n",
      "Average Reward for Agent 8 this episode : -1.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0927\n",
      "Average Reward for Agent 9 this episode : -1.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6058\n",
      "Average Reward for Agent 10 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8212\n",
      "Average Reward for Agent 11 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.2163\n",
      "Average Reward for Agent 12 this episode : -14.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.3142\n",
      "Average Reward for Agent 13 this episode : -15.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.3223\n",
      "Reducing exploration for all agents to 0.2034\n",
      "Episode 93 is finished\n",
      "Average Reward for Agent 0 this episode : -9.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.1799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 1 this episode : -9.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 142.8898\n",
      "Average Reward for Agent 2 this episode : -29.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 772.3987\n",
      "Average Reward for Agent 3 this episode : -14.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.9298\n",
      "Average Reward for Agent 4 this episode : -10.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 121.7427\n",
      "Average Reward for Agent 5 this episode : -3.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0408\n",
      "Average Reward for Agent 6 this episode : -9.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 149.3531\n",
      "Average Reward for Agent 7 this episode : -13.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.0032\n",
      "Average Reward for Agent 8 this episode : -1.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2177\n",
      "Average Reward for Agent 9 this episode : -1.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8104\n",
      "Average Reward for Agent 10 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.4813\n",
      "Average Reward for Agent 11 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.4915\n",
      "Average Reward for Agent 12 this episode : -27.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.3917\n",
      "Average Reward for Agent 13 this episode : -14.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.3860\n",
      "Reducing exploration for all agents to 0.1999\n",
      "Episode 94 is finished\n",
      "Average Reward for Agent 0 this episode : -9.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.6055\n",
      "Average Reward for Agent 1 this episode : -11.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 160.3887\n",
      "Average Reward for Agent 2 this episode : -29.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 445.8951\n",
      "Average Reward for Agent 3 this episode : -9.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.9861\n",
      "Average Reward for Agent 4 this episode : -8.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.3563\n",
      "Average Reward for Agent 5 this episode : -5.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9299\n",
      "Average Reward for Agent 6 this episode : -3.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.1188\n",
      "Average Reward for Agent 7 this episode : -12.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.8512\n",
      "Average Reward for Agent 8 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5438\n",
      "Average Reward for Agent 9 this episode : -1.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.1635\n",
      "Average Reward for Agent 10 this episode : -1.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.8124\n",
      "Average Reward for Agent 11 this episode : -1.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.0046\n",
      "Average Reward for Agent 12 this episode : -31.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.1260\n",
      "Average Reward for Agent 13 this episode : -40.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.4155\n",
      "Reducing exploration for all agents to 0.1964\n",
      "Episode 95 is finished\n",
      "Average Reward for Agent 0 this episode : -8.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.8574\n",
      "Average Reward for Agent 1 this episode : -10.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 176.5822\n",
      "Average Reward for Agent 2 this episode : -33.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 414.8715\n",
      "Average Reward for Agent 3 this episode : -12.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.7488\n",
      "Average Reward for Agent 4 this episode : -12.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.0741\n",
      "Average Reward for Agent 5 this episode : -6.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6600\n",
      "Average Reward for Agent 6 this episode : -9.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 136.9325\n",
      "Average Reward for Agent 7 this episode : -15.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.9197\n",
      "Average Reward for Agent 8 this episode : -1.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3171\n",
      "Average Reward for Agent 9 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3736\n",
      "Average Reward for Agent 10 this episode : -0.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1001\n",
      "Average Reward for Agent 11 this episode : -1.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.9214\n",
      "Average Reward for Agent 12 this episode : -8.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.8355\n",
      "Average Reward for Agent 13 this episode : -34.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.2525\n",
      "Reducing exploration for all agents to 0.1931\n",
      "Episode 96 is finished\n",
      "Average Reward for Agent 0 this episode : -7.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.0062\n",
      "Average Reward for Agent 1 this episode : -41.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 178.2047\n",
      "Average Reward for Agent 2 this episode : -47.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 290.9095\n",
      "Average Reward for Agent 3 this episode : -9.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.8829\n",
      "Average Reward for Agent 4 this episode : -8.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.6754\n",
      "Average Reward for Agent 5 this episode : -1.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6101\n",
      "Average Reward for Agent 6 this episode : -1.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.5864\n",
      "Average Reward for Agent 7 this episode : -13.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.1112\n",
      "Average Reward for Agent 8 this episode : -0.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2047\n",
      "Average Reward for Agent 9 this episode : -0.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2394\n",
      "Average Reward for Agent 10 this episode : -0.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3076\n",
      "Average Reward for Agent 11 this episode : -2.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9920\n",
      "Average Reward for Agent 12 this episode : -17.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.1905\n",
      "Average Reward for Agent 13 this episode : -38.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.8929\n",
      "Reducing exploration for all agents to 0.1898\n",
      "Episode 97 is finished\n",
      "Average Reward for Agent 0 this episode : -9.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.9846\n",
      "Average Reward for Agent 1 this episode : -42.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 132.8383\n",
      "Average Reward for Agent 2 this episode : -46.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 411.3826\n",
      "Average Reward for Agent 3 this episode : -11.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.4444\n",
      "Average Reward for Agent 4 this episode : -10.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.3659\n",
      "Average Reward for Agent 5 this episode : -5.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3206\n",
      "Average Reward for Agent 6 this episode : -1.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.3493\n",
      "Average Reward for Agent 7 this episode : -9.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.4288\n",
      "Average Reward for Agent 8 this episode : -2.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1707\n",
      "Average Reward for Agent 9 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2253\n",
      "Average Reward for Agent 10 this episode : -0.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7305\n",
      "Average Reward for Agent 11 this episode : -5.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.9407\n",
      "Average Reward for Agent 12 this episode : -21.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.2315\n",
      "Average Reward for Agent 13 this episode : -42.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.8273\n",
      "Reducing exploration for all agents to 0.1865\n",
      "Episode 98 is finished\n",
      "Average Reward for Agent 0 this episode : -10.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.9213\n",
      "Average Reward for Agent 1 this episode : -17.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.4556\n",
      "Average Reward for Agent 2 this episode : -46.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 489.9577\n",
      "Average Reward for Agent 3 this episode : -10.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.5560\n",
      "Average Reward for Agent 4 this episode : -7.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.6664\n",
      "Average Reward for Agent 5 this episode : -8.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6155\n",
      "Average Reward for Agent 6 this episode : -1.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.3955\n",
      "Average Reward for Agent 7 this episode : -12.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.8884\n",
      "Average Reward for Agent 8 this episode : -1.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3546\n",
      "Average Reward for Agent 9 this episode : -1.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2150\n",
      "Average Reward for Agent 10 this episode : -12.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.2846\n",
      "Average Reward for Agent 11 this episode : -13.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.8341\n",
      "Average Reward for Agent 12 this episode : -19.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.9239\n",
      "Average Reward for Agent 13 this episode : -11.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 132.7499\n",
      "Reducing exploration for all agents to 0.1833\n",
      "Episode 99 is finished\n",
      "Average Reward for Agent 0 this episode : -9.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.1495\n",
      "Average Reward for Agent 1 this episode : -39.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 210.9156\n",
      "Average Reward for Agent 2 this episode : -38.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 582.2767\n",
      "Average Reward for Agent 3 this episode : -12.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.3068\n",
      "Average Reward for Agent 4 this episode : -5.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.8274\n",
      "Average Reward for Agent 5 this episode : -1.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5338\n",
      "Average Reward for Agent 6 this episode : -1.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.2735\n",
      "Average Reward for Agent 7 this episode : -13.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.4608\n",
      "Average Reward for Agent 8 this episode : -1.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7032\n",
      "Average Reward for Agent 9 this episode : -2.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3166\n",
      "Average Reward for Agent 10 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.1616\n",
      "Average Reward for Agent 11 this episode : -6.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.0405\n",
      "Average Reward for Agent 12 this episode : -22.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.7938\n",
      "Average Reward for Agent 13 this episode : -1.27\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent13_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.8470\n",
      "Reducing exploration for all agents to 0.1802\n",
      "Episode 100 is finished\n",
      "Average Reward for Agent 0 this episode : -8.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.7292\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -36.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 256.7587\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -31.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 273.6862\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -10.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.8458\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -10.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.2152\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6584\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.0457\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -13.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.9797\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6822\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -1.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7531\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -5.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2748\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -2.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.7641\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -15.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.9198\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -14.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.5185\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.1771\n",
      "Episode 101 is finished\n",
      "Average Reward for Agent 0 this episode : -9.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.7247\n",
      "Average Reward for Agent 1 this episode : -46.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 443.9505\n",
      "Average Reward for Agent 2 this episode : -36.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 515.9100\n",
      "Average Reward for Agent 3 this episode : -9.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.1766\n",
      "Average Reward for Agent 4 this episode : -8.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.1306\n",
      "Average Reward for Agent 5 this episode : -0.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.9220\n",
      "Average Reward for Agent 6 this episode : -1.27\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.5525\n",
      "Average Reward for Agent 7 this episode : -12.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.3380\n",
      "Average Reward for Agent 8 this episode : -1.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9940\n",
      "Average Reward for Agent 9 this episode : -0.21\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.4272\n",
      "Average Reward for Agent 10 this episode : -2.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.4934\n",
      "Average Reward for Agent 11 this episode : -3.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.7974\n",
      "Average Reward for Agent 12 this episode : -24.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 126.5033\n",
      "Average Reward for Agent 13 this episode : -0.61\n",
      "Saving architecture, weights, optimizer state for best agent-13\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent13_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.8967\n",
      "Reducing exploration for all agents to 0.174\n",
      "Episode 102 is finished\n",
      "Average Reward for Agent 0 this episode : -8.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.3274\n",
      "Average Reward for Agent 1 this episode : -14.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 777.5656\n",
      "Average Reward for Agent 2 this episode : -38.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 305.9509\n",
      "Average Reward for Agent 3 this episode : -10.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.6860\n",
      "Average Reward for Agent 4 this episode : -13.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.1919\n",
      "Average Reward for Agent 5 this episode : -0.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7176\n",
      "Average Reward for Agent 6 this episode : -2.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.4406\n",
      "Average Reward for Agent 7 this episode : -16.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.8078\n",
      "Average Reward for Agent 8 this episode : -1.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3792\n",
      "Average Reward for Agent 9 this episode : -3.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4623\n",
      "Average Reward for Agent 10 this episode : -4.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.4414\n",
      "Average Reward for Agent 11 this episode : -1.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.4078\n",
      "Average Reward for Agent 12 this episode : -21.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.4602\n",
      "Average Reward for Agent 13 this episode : -11.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.7989\n",
      "Reducing exploration for all agents to 0.171\n",
      "Episode 103 is finished\n",
      "Average Reward for Agent 0 this episode : -7.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.8873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 1 this episode : -20.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 275.6100\n",
      "Average Reward for Agent 2 this episode : -34.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 657.7899\n",
      "Average Reward for Agent 3 this episode : -11.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.4869\n",
      "Average Reward for Agent 4 this episode : -9.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.8808\n",
      "Average Reward for Agent 5 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4436\n",
      "Average Reward for Agent 6 this episode : -2.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.4187\n",
      "Average Reward for Agent 7 this episode : -17.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.9753\n",
      "Average Reward for Agent 8 this episode : -0.12\n",
      "Saving architecture, weights, optimizer state for best agent-8\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2627\n",
      "Average Reward for Agent 9 this episode : -13.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.6926\n",
      "Average Reward for Agent 10 this episode : -3.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.9212\n",
      "Average Reward for Agent 11 this episode : -0.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.8341\n",
      "Average Reward for Agent 12 this episode : -21.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.4547\n",
      "Average Reward for Agent 13 this episode : -7.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.4933\n",
      "Reducing exploration for all agents to 0.1681\n",
      "Episode 104 is finished\n",
      "Average Reward for Agent 0 this episode : -8.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.2878\n",
      "Average Reward for Agent 1 this episode : -28.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 234.7563\n",
      "Average Reward for Agent 2 this episode : -27.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 626.7217\n",
      "Average Reward for Agent 3 this episode : -10.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.1975\n",
      "Average Reward for Agent 4 this episode : -11.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 117.5786\n",
      "Average Reward for Agent 5 this episode : -0.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8596\n",
      "Average Reward for Agent 6 this episode : -2.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.2059\n",
      "Average Reward for Agent 7 this episode : -17.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.3499\n",
      "Average Reward for Agent 8 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6637\n",
      "Average Reward for Agent 9 this episode : -0.2\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8247\n",
      "Average Reward for Agent 10 this episode : -31.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.6633\n",
      "Average Reward for Agent 11 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.6593\n",
      "Average Reward for Agent 12 this episode : -6.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.1335\n",
      "Average Reward for Agent 13 this episode : -3.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.9439\n",
      "Reducing exploration for all agents to 0.1652\n",
      "Episode 105 is finished\n",
      "Average Reward for Agent 0 this episode : -8.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.1367\n",
      "Average Reward for Agent 1 this episode : -24.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 314.7083\n",
      "Average Reward for Agent 2 this episode : -29.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 411.4998\n",
      "Average Reward for Agent 3 this episode : -13.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.8435\n",
      "Average Reward for Agent 4 this episode : -12.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.3437\n",
      "Average Reward for Agent 5 this episode : -0.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6808\n",
      "Average Reward for Agent 6 this episode : -4.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.8489\n",
      "Average Reward for Agent 7 this episode : -13.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.2312\n",
      "Average Reward for Agent 8 this episode : -11.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6494\n",
      "Average Reward for Agent 9 this episode : -1.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.7301\n",
      "Average Reward for Agent 10 this episode : -11.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.5310\n",
      "Average Reward for Agent 11 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.4304\n",
      "Average Reward for Agent 12 this episode : -15.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.8421\n",
      "Average Reward for Agent 13 this episode : -3.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.0696\n",
      "Reducing exploration for all agents to 0.1624\n",
      "Episode 106 is finished\n",
      "Average Reward for Agent 0 this episode : -8.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.5531\n",
      "Average Reward for Agent 1 this episode : -27.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 281.9355\n",
      "Average Reward for Agent 2 this episode : -45.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 454.7802\n",
      "Average Reward for Agent 3 this episode : -12.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.4284\n",
      "Average Reward for Agent 4 this episode : -12.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.8942\n",
      "Average Reward for Agent 5 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1956\n",
      "Average Reward for Agent 6 this episode : -2.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.4248\n",
      "Average Reward for Agent 7 this episode : -17.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.7350\n",
      "Average Reward for Agent 8 this episode : -5.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.9568\n",
      "Average Reward for Agent 9 this episode : -1.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.1736\n",
      "Average Reward for Agent 10 this episode : -6.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.4526\n",
      "Average Reward for Agent 11 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3566\n",
      "Average Reward for Agent 12 this episode : -10.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.0267\n",
      "Average Reward for Agent 13 this episode : -4.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.2761\n",
      "Reducing exploration for all agents to 0.1596\n",
      "Episode 107 is finished\n",
      "Average Reward for Agent 0 this episode : -9.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.7504\n",
      "Average Reward for Agent 1 this episode : -18.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 248.6133\n",
      "Average Reward for Agent 2 this episode : -47.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 478.6785\n",
      "Average Reward for Agent 3 this episode : -13.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.5196\n",
      "Average Reward for Agent 4 this episode : -11.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.3845\n",
      "Average Reward for Agent 5 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9064\n",
      "Average Reward for Agent 6 this episode : -2.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.4644\n",
      "Average Reward for Agent 7 this episode : -14.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.0920\n",
      "Average Reward for Agent 8 this episode : -19.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.8822\n",
      "Average Reward for Agent 9 this episode : -0.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.5503\n",
      "Average Reward for Agent 10 this episode : -7.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.7024\n",
      "Average Reward for Agent 11 this episode : -0.26\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.1211\n",
      "Average Reward for Agent 12 this episode : -22.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.5996\n",
      "Average Reward for Agent 13 this episode : -9.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.0825\n",
      "Reducing exploration for all agents to 0.1569\n",
      "Episode 108 is finished\n",
      "Average Reward for Agent 0 this episode : -9.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.4089\n",
      "Average Reward for Agent 1 this episode : -17.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.1480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 2 this episode : -25.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 496.1916\n",
      "Average Reward for Agent 3 this episode : -11.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.0211\n",
      "Average Reward for Agent 4 this episode : -9.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.9175\n",
      "Average Reward for Agent 5 this episode : -0.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4250\n",
      "Average Reward for Agent 6 this episode : -1.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.7786\n",
      "Average Reward for Agent 7 this episode : -19.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.2845\n",
      "Average Reward for Agent 8 this episode : -13.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.9949\n",
      "Average Reward for Agent 9 this episode : -0.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.0371\n",
      "Average Reward for Agent 10 this episode : -7.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.9510\n",
      "Average Reward for Agent 11 this episode : -0.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.1047\n",
      "Average Reward for Agent 12 this episode : -20.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.3482\n",
      "Average Reward for Agent 13 this episode : -10.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.5930\n",
      "Reducing exploration for all agents to 0.1542\n",
      "Episode 109 is finished\n",
      "Average Reward for Agent 0 this episode : -9.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.9739\n",
      "Average Reward for Agent 1 this episode : -11.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 345.4435\n",
      "Average Reward for Agent 2 this episode : -28.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 451.2758\n",
      "Average Reward for Agent 3 this episode : -12.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.0856\n",
      "Average Reward for Agent 4 this episode : -13.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.5045\n",
      "Average Reward for Agent 5 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1914\n",
      "Average Reward for Agent 6 this episode : -1.13\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.7945\n",
      "Average Reward for Agent 7 this episode : -15.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.4867\n",
      "Average Reward for Agent 8 this episode : -2.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.4285\n",
      "Average Reward for Agent 9 this episode : -1.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.2762\n",
      "Average Reward for Agent 10 this episode : -5.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.8089\n",
      "Average Reward for Agent 11 this episode : -0.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.8675\n",
      "Average Reward for Agent 12 this episode : -17.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.2831\n",
      "Average Reward for Agent 13 this episode : -10.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.0885\n",
      "Reducing exploration for all agents to 0.1515\n",
      "Episode 110 is finished\n",
      "Average Reward for Agent 0 this episode : -8.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.1596\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -12.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 250.0621\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -28.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 331.2689\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -10.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.7776\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -9.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.2120\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8063\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.4255\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -16.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.5046\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -2.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.3937\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.3018\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -10.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.3476\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.2793\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -9.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.2993\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -11.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.2213\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.1489\n",
      "Episode 111 is finished\n",
      "Average Reward for Agent 0 this episode : -9.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.1971\n",
      "Average Reward for Agent 1 this episode : -12.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 300.1627\n",
      "Average Reward for Agent 2 this episode : -24.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 480.3204\n",
      "Average Reward for Agent 3 this episode : -12.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.1811\n",
      "Average Reward for Agent 4 this episode : -11.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.2712\n",
      "Average Reward for Agent 5 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7234\n",
      "Average Reward for Agent 6 this episode : -1.06\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.4114\n",
      "Average Reward for Agent 7 this episode : -13.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.8745\n",
      "Average Reward for Agent 8 this episode : -5.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.0515\n",
      "Average Reward for Agent 9 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3240\n",
      "Average Reward for Agent 10 this episode : -27.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 156.6723\n",
      "Average Reward for Agent 11 this episode : -0.26\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.5939\n",
      "Average Reward for Agent 12 this episode : -24.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 123.4932\n",
      "Average Reward for Agent 13 this episode : -13.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 157.9725\n",
      "Reducing exploration for all agents to 0.1464\n",
      "Episode 112 is finished\n",
      "Average Reward for Agent 0 this episode : -9.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.5919\n",
      "Average Reward for Agent 1 this episode : -14.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 173.1122\n",
      "Average Reward for Agent 2 this episode : -24.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 344.7686\n",
      "Average Reward for Agent 3 this episode : -12.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.5210\n",
      "Average Reward for Agent 4 this episode : -7.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.6640\n",
      "Average Reward for Agent 5 this episode : -0.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.4429\n",
      "Average Reward for Agent 6 this episode : -1.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.4908\n",
      "Average Reward for Agent 7 this episode : -17.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.6770\n",
      "Average Reward for Agent 8 this episode : -7.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.3301\n",
      "Average Reward for Agent 9 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 10 this episode : -8.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.7953\n",
      "Average Reward for Agent 11 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.1295\n",
      "Average Reward for Agent 12 this episode : -26.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.5867\n",
      "Average Reward for Agent 13 this episode : -19.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.4882\n",
      "Reducing exploration for all agents to 0.1438\n",
      "Episode 113 is finished\n",
      "Average Reward for Agent 0 this episode : -9.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.4275\n",
      "Average Reward for Agent 1 this episode : -12.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 163.5827\n",
      "Average Reward for Agent 2 this episode : -20.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 241.2021\n",
      "Average Reward for Agent 3 this episode : -13.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.8288\n",
      "Average Reward for Agent 4 this episode : -9.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.0076\n",
      "Average Reward for Agent 5 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2766\n",
      "Average Reward for Agent 6 this episode : -5.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.5862\n",
      "Average Reward for Agent 7 this episode : -15.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.2084\n",
      "Average Reward for Agent 8 this episode : -0.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.8843\n",
      "Average Reward for Agent 9 this episode : -0.19\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2039\n",
      "Average Reward for Agent 10 this episode : -8.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.8773\n",
      "Average Reward for Agent 11 this episode : -0.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.6805\n",
      "Average Reward for Agent 12 this episode : -8.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.3472\n",
      "Average Reward for Agent 13 this episode : -12.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.9238\n",
      "Reducing exploration for all agents to 0.1414\n",
      "Episode 114 is finished\n",
      "Average Reward for Agent 0 this episode : -10.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.7989\n",
      "Average Reward for Agent 1 this episode : -13.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 200.4087\n",
      "Average Reward for Agent 2 this episode : -18.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 450.7555\n",
      "Average Reward for Agent 3 this episode : -11.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.7651\n",
      "Average Reward for Agent 4 this episode : -12.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.8821\n",
      "Average Reward for Agent 5 this episode : -0.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5424\n",
      "Average Reward for Agent 6 this episode : -16.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 138.2957\n",
      "Average Reward for Agent 7 this episode : -19.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.2563\n",
      "Average Reward for Agent 8 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.1542\n",
      "Average Reward for Agent 9 this episode : -0.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1737\n",
      "Average Reward for Agent 10 this episode : -9.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 133.9257\n",
      "Average Reward for Agent 11 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.4096\n",
      "Average Reward for Agent 12 this episode : -16.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.9460\n",
      "Average Reward for Agent 13 this episode : -14.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.9304\n",
      "Reducing exploration for all agents to 0.1389\n",
      "Episode 115 is finished\n",
      "Average Reward for Agent 0 this episode : -10.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.9924\n",
      "Average Reward for Agent 1 this episode : -11.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 152.7609\n",
      "Average Reward for Agent 2 this episode : -21.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 427.4049\n",
      "Average Reward for Agent 3 this episode : -10.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.9349\n",
      "Average Reward for Agent 4 this episode : -10.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.8487\n",
      "Average Reward for Agent 5 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4105\n",
      "Average Reward for Agent 6 this episode : -13.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 139.3986\n",
      "Average Reward for Agent 7 this episode : -17.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.4377\n",
      "Average Reward for Agent 8 this episode : -0.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9995\n",
      "Average Reward for Agent 9 this episode : -10.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.1518\n",
      "Average Reward for Agent 10 this episode : -5.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.7222\n",
      "Average Reward for Agent 11 this episode : -0.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.1731\n",
      "Average Reward for Agent 12 this episode : -6.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.8069\n",
      "Average Reward for Agent 13 this episode : -25.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 148.5088\n",
      "Reducing exploration for all agents to 0.1366\n",
      "Episode 116 is finished\n",
      "Average Reward for Agent 0 this episode : -9.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.1498\n",
      "Average Reward for Agent 1 this episode : -14.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.1288\n",
      "Average Reward for Agent 2 this episode : -19.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 327.5681\n",
      "Average Reward for Agent 3 this episode : -11.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.6469\n",
      "Average Reward for Agent 4 this episode : -19.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.0099\n",
      "Average Reward for Agent 5 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5957\n",
      "Average Reward for Agent 6 this episode : -4.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.5341\n",
      "Average Reward for Agent 7 this episode : -16.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.8053\n",
      "Average Reward for Agent 8 this episode : -1.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.6691\n",
      "Average Reward for Agent 9 this episode : -0.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.5767\n",
      "Average Reward for Agent 10 this episode : -26.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.0004\n",
      "Average Reward for Agent 11 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.3973\n",
      "Average Reward for Agent 12 this episode : -9.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.1276\n",
      "Average Reward for Agent 13 this episode : -11.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.8553\n",
      "Reducing exploration for all agents to 0.1342\n",
      "Episode 117 is finished\n",
      "Average Reward for Agent 0 this episode : -9.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.2757\n",
      "Average Reward for Agent 1 this episode : -14.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 154.6555\n",
      "Average Reward for Agent 2 this episode : -27.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 358.7051\n",
      "Average Reward for Agent 3 this episode : -11.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.8511\n",
      "Average Reward for Agent 4 this episode : -10.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.6099\n",
      "Average Reward for Agent 5 this episode : -0.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6932\n",
      "Average Reward for Agent 6 this episode : -19.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 115.2683\n",
      "Average Reward for Agent 7 this episode : -14.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.9581\n",
      "Average Reward for Agent 8 this episode : -2.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.8256\n",
      "Average Reward for Agent 9 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.6914\n",
      "Average Reward for Agent 10 this episode : -13.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.2145\n",
      "Average Reward for Agent 11 this episode : -0.22\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.5702\n",
      "Average Reward for Agent 12 this episode : -22.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.9177\n",
      "Average Reward for Agent 13 this episode : -4.71\n",
      "Train on 128 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 - 0s - loss: 111.0916\n",
      "Reducing exploration for all agents to 0.1319\n",
      "Episode 118 is finished\n",
      "Average Reward for Agent 0 this episode : -9.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.4038\n",
      "Average Reward for Agent 1 this episode : -14.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 240.4838\n",
      "Average Reward for Agent 2 this episode : -22.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 338.6398\n",
      "Average Reward for Agent 3 this episode : -9.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.9163\n",
      "Average Reward for Agent 4 this episode : -10.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.6828\n",
      "Average Reward for Agent 5 this episode : -0.05\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8418\n",
      "Average Reward for Agent 6 this episode : -8.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 124.3615\n",
      "Average Reward for Agent 7 this episode : -18.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.5600\n",
      "Average Reward for Agent 8 this episode : -2.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4481\n",
      "Average Reward for Agent 9 this episode : -2.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.6753\n",
      "Average Reward for Agent 10 this episode : -19.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.9623\n",
      "Average Reward for Agent 11 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.6743\n",
      "Average Reward for Agent 12 this episode : -34.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 121.0684\n",
      "Average Reward for Agent 13 this episode : -10.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.4311\n",
      "Reducing exploration for all agents to 0.1297\n",
      "Episode 119 is finished\n",
      "Average Reward for Agent 0 this episode : -10.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.0500\n",
      "Average Reward for Agent 1 this episode : -11.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 134.5589\n",
      "Average Reward for Agent 2 this episode : -24.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 355.4579\n",
      "Average Reward for Agent 3 this episode : -11.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.0218\n",
      "Average Reward for Agent 4 this episode : -13.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.8388\n",
      "Average Reward for Agent 5 this episode : -0.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3705\n",
      "Average Reward for Agent 6 this episode : -4.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.0537\n",
      "Average Reward for Agent 7 this episode : -21.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.6479\n",
      "Average Reward for Agent 8 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5148\n",
      "Average Reward for Agent 9 this episode : -2.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.4797\n",
      "Average Reward for Agent 10 this episode : -33.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.7569\n",
      "Average Reward for Agent 11 this episode : -0.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2289\n",
      "Average Reward for Agent 12 this episode : -8.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.4214\n",
      "Average Reward for Agent 13 this episode : -5.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.6924\n",
      "Reducing exploration for all agents to 0.1274\n",
      "Episode 120 is finished\n",
      "Average Reward for Agent 0 this episode : -9.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.5277\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -15.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 243.7924\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -20.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 252.5289\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -10.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.3449\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -13.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.0953\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6480\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -17.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.3422\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -16.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.4857\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3621\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -1.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0891\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -18.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.1189\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.5859\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -8.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.9498\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -9.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.6649\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.1252\n",
      "Episode 121 is finished\n",
      "Average Reward for Agent 0 this episode : -9.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.7842\n",
      "Average Reward for Agent 1 this episode : -15.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 272.7328\n",
      "Average Reward for Agent 2 this episode : -25.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 259.7020\n",
      "Average Reward for Agent 3 this episode : -11.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.5329\n",
      "Average Reward for Agent 4 this episode : -13.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.1075\n",
      "Average Reward for Agent 5 this episode : -0.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3409\n",
      "Average Reward for Agent 6 this episode : -2.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 159.8240\n",
      "Average Reward for Agent 7 this episode : -19.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.2504\n",
      "Average Reward for Agent 8 this episode : -0.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2346\n",
      "Average Reward for Agent 9 this episode : -2.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7198\n",
      "Average Reward for Agent 10 this episode : -10.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 268.8700\n",
      "Average Reward for Agent 11 this episode : -0.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7142\n",
      "Average Reward for Agent 12 this episode : -14.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.4628\n",
      "Average Reward for Agent 13 this episode : -24.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 201.3158\n",
      "Reducing exploration for all agents to 0.1231\n",
      "Episode 122 is finished\n",
      "Average Reward for Agent 0 this episode : -9.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.7221\n",
      "Average Reward for Agent 1 this episode : -9.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.0439\n",
      "Average Reward for Agent 2 this episode : -26.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 254.0466\n",
      "Average Reward for Agent 3 this episode : -11.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.9907\n",
      "Average Reward for Agent 4 this episode : -14.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.5651\n",
      "Average Reward for Agent 5 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8094\n",
      "Average Reward for Agent 6 this episode : -3.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.2324\n",
      "Average Reward for Agent 7 this episode : -12.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.4580\n",
      "Average Reward for Agent 8 this episode : -3.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5474\n",
      "Average Reward for Agent 9 this episode : -6.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.6637\n",
      "Average Reward for Agent 10 this episode : -37.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.2553\n",
      "Average Reward for Agent 11 this episode : -0.21\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.8047\n",
      "Average Reward for Agent 12 this episode : -23.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.4569\n",
      "Average Reward for Agent 13 this episode : -33.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 140.8428\n",
      "Reducing exploration for all agents to 0.121\n",
      "Episode 123 is finished\n",
      "Average Reward for Agent 0 this episode : -12.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.0586\n",
      "Average Reward for Agent 1 this episode : -13.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.4013\n",
      "Average Reward for Agent 2 this episode : -31.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 217.4885\n",
      "Average Reward for Agent 3 this episode : -8.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.2137\n",
      "Average Reward for Agent 4 this episode : -10.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.0285\n",
      "Average Reward for Agent 5 this episode : -0.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0058\n",
      "Average Reward for Agent 6 this episode : -17.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.1698\n",
      "Average Reward for Agent 7 this episode : -16.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 155.5508\n",
      "Average Reward for Agent 8 this episode : -2.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3001\n",
      "Average Reward for Agent 9 this episode : -1.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.0965\n",
      "Average Reward for Agent 10 this episode : -4.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 132.5509\n",
      "Average Reward for Agent 11 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1929\n",
      "Average Reward for Agent 12 this episode : -14.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.4543\n",
      "Average Reward for Agent 13 this episode : -15.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.1019\n",
      "Reducing exploration for all agents to 0.1189\n",
      "Episode 124 is finished\n",
      "Average Reward for Agent 0 this episode : -15.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.9922\n",
      "Average Reward for Agent 1 this episode : -10.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 195.6081\n",
      "Average Reward for Agent 2 this episode : -36.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 253.5570\n",
      "Average Reward for Agent 3 this episode : -11.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.2434\n",
      "Average Reward for Agent 4 this episode : -14.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 167.7141\n",
      "Average Reward for Agent 5 this episode : -0.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8258\n",
      "Average Reward for Agent 6 this episode : -31.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 232.4911\n",
      "Average Reward for Agent 7 this episode : -13.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.1202\n",
      "Average Reward for Agent 8 this episode : -3.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.6540\n",
      "Average Reward for Agent 9 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6133\n",
      "Average Reward for Agent 10 this episode : -1.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 158.5572\n",
      "Average Reward for Agent 11 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1185\n",
      "Average Reward for Agent 12 this episode : -11.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.1118\n",
      "Average Reward for Agent 13 this episode : -26.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 144.4500\n",
      "Reducing exploration for all agents to 0.1169\n",
      "Episode 125 is finished\n",
      "Average Reward for Agent 0 this episode : -11.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.2149\n",
      "Average Reward for Agent 1 this episode : -8.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 162.7382\n",
      "Average Reward for Agent 2 this episode : -39.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 321.5215\n",
      "Average Reward for Agent 3 this episode : -10.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.8299\n",
      "Average Reward for Agent 4 this episode : -19.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 177.1325\n",
      "Average Reward for Agent 5 this episode : -0.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7100\n",
      "Average Reward for Agent 6 this episode : -21.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.6462\n",
      "Average Reward for Agent 7 this episode : -14.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.0597\n",
      "Average Reward for Agent 8 this episode : -1.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2115\n",
      "Average Reward for Agent 9 this episode : -3.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6614\n",
      "Average Reward for Agent 10 this episode : -4.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.6152\n",
      "Average Reward for Agent 11 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6843\n",
      "Average Reward for Agent 12 this episode : -10.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.1967\n",
      "Average Reward for Agent 13 this episode : -46.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 297.6099\n",
      "Reducing exploration for all agents to 0.1149\n",
      "Episode 126 is finished\n",
      "Average Reward for Agent 0 this episode : -10.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.7406\n",
      "Average Reward for Agent 1 this episode : -8.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 201.0051\n",
      "Average Reward for Agent 2 this episode : -34.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 306.1263\n",
      "Average Reward for Agent 3 this episode : -12.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.5994\n",
      "Average Reward for Agent 4 this episode : -15.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 122.8290\n",
      "Average Reward for Agent 5 this episode : -0.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4724\n",
      "Average Reward for Agent 6 this episode : -18.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.9194\n",
      "Average Reward for Agent 7 this episode : -18.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.3099\n",
      "Average Reward for Agent 8 this episode : -1.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.3559\n",
      "Average Reward for Agent 9 this episode : -4.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.9007\n",
      "Average Reward for Agent 10 this episode : -14.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.1657\n",
      "Average Reward for Agent 11 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3718\n",
      "Average Reward for Agent 12 this episode : -9.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.3736\n",
      "Average Reward for Agent 13 this episode : -36.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.0430\n",
      "Reducing exploration for all agents to 0.1129\n",
      "Episode 127 is finished\n",
      "Average Reward for Agent 0 this episode : -11.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.5913\n",
      "Average Reward for Agent 1 this episode : -10.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 164.3027\n",
      "Average Reward for Agent 2 this episode : -36.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 241.9022\n",
      "Average Reward for Agent 3 this episode : -12.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.4746\n",
      "Average Reward for Agent 4 this episode : -14.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 172.8215\n",
      "Average Reward for Agent 5 this episode : -0.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6509\n",
      "Average Reward for Agent 6 this episode : -13.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.6254\n",
      "Average Reward for Agent 7 this episode : -16.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.5834\n",
      "Average Reward for Agent 8 this episode : -2.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6961\n",
      "Average Reward for Agent 9 this episode : -1.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.7261\n",
      "Average Reward for Agent 10 this episode : -16.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.9430\n",
      "Average Reward for Agent 11 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.2000\n",
      "Average Reward for Agent 12 this episode : -13.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.5738\n",
      "Average Reward for Agent 13 this episode : -63.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 338.8625\n",
      "Reducing exploration for all agents to 0.1109\n",
      "Episode 128 is finished\n",
      "Average Reward for Agent 0 this episode : -12.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.0928\n",
      "Average Reward for Agent 1 this episode : -12.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 173.8273\n",
      "Average Reward for Agent 2 this episode : -40.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 364.6086\n",
      "Average Reward for Agent 3 this episode : -7.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.7457\n",
      "Average Reward for Agent 4 this episode : -15.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 209.3601\n",
      "Average Reward for Agent 5 this episode : -0.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 6 this episode : -12.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.3181\n",
      "Average Reward for Agent 7 this episode : -16.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.4150\n",
      "Average Reward for Agent 8 this episode : -2.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6721\n",
      "Average Reward for Agent 9 this episode : -1.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.9562\n",
      "Average Reward for Agent 10 this episode : -22.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 122.8817\n",
      "Average Reward for Agent 11 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6961\n",
      "Average Reward for Agent 12 this episode : -20.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.9244\n",
      "Average Reward for Agent 13 this episode : -25.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 233.5449\n",
      "Reducing exploration for all agents to 0.109\n",
      "Episode 129 is finished\n",
      "Average Reward for Agent 0 this episode : -12.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.3337\n",
      "Average Reward for Agent 1 this episode : -14.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 178.3675\n",
      "Average Reward for Agent 2 this episode : -30.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 539.2883\n",
      "Average Reward for Agent 3 this episode : -13.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.4700\n",
      "Average Reward for Agent 4 this episode : -10.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 151.7285\n",
      "Average Reward for Agent 5 this episode : -0.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7786\n",
      "Average Reward for Agent 6 this episode : -43.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.6520\n",
      "Average Reward for Agent 7 this episode : -17.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.4130\n",
      "Average Reward for Agent 8 this episode : -4.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.0947\n",
      "Average Reward for Agent 9 this episode : -1.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5173\n",
      "Average Reward for Agent 10 this episode : -25.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 187.5630\n",
      "Average Reward for Agent 11 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7494\n",
      "Average Reward for Agent 12 this episode : -12.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.3526\n",
      "Average Reward for Agent 13 this episode : -11.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 199.9781\n",
      "Reducing exploration for all agents to 0.1072\n",
      "Episode 130 is finished\n",
      "Average Reward for Agent 0 this episode : -13.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.8640\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -15.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 188.0603\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -45.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 554.7793\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -11.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.4765\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -10.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.8462\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0096\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -25.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 184.5352\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -19.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.8876\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0420\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -2.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.3624\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -8.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.3658\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.5223\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -16.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.0676\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -21.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.7944\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.1053\n",
      "Episode 131 is finished\n",
      "Average Reward for Agent 0 this episode : -15.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.2003\n",
      "Average Reward for Agent 1 this episode : -13.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 209.7044\n",
      "Average Reward for Agent 2 this episode : -40.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 816.4365\n",
      "Average Reward for Agent 3 this episode : -10.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.6545\n",
      "Average Reward for Agent 4 this episode : -12.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.1606\n",
      "Average Reward for Agent 5 this episode : -0.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5856\n",
      "Average Reward for Agent 6 this episode : -11.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 347.2599\n",
      "Average Reward for Agent 7 this episode : -13.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.7036\n",
      "Average Reward for Agent 8 this episode : -10.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.8147\n",
      "Average Reward for Agent 9 this episode : -3.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.5860\n",
      "Average Reward for Agent 10 this episode : -22.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 242.1912\n",
      "Average Reward for Agent 11 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5818\n",
      "Average Reward for Agent 12 this episode : -13.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.8863\n",
      "Average Reward for Agent 13 this episode : -36.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 361.5804\n",
      "Reducing exploration for all agents to 0.1035\n",
      "Episode 132 is finished\n",
      "Average Reward for Agent 0 this episode : -17.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.4874\n",
      "Average Reward for Agent 1 this episode : -16.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 217.5872\n",
      "Average Reward for Agent 2 this episode : -35.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 454.0334\n",
      "Average Reward for Agent 3 this episode : -11.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.9429\n",
      "Average Reward for Agent 4 this episode : -11.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 117.6877\n",
      "Average Reward for Agent 5 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3403\n",
      "Average Reward for Agent 6 this episode : -14.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 217.0648\n",
      "Average Reward for Agent 7 this episode : -14.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.4116\n",
      "Average Reward for Agent 8 this episode : -3.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.1464\n",
      "Average Reward for Agent 9 this episode : -2.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.2419\n",
      "Average Reward for Agent 10 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.9790\n",
      "Average Reward for Agent 11 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2775\n",
      "Average Reward for Agent 12 this episode : -15.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.3260\n",
      "Average Reward for Agent 13 this episode : -19.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.5580\n",
      "Reducing exploration for all agents to 0.1017\n",
      "Episode 133 is finished\n",
      "Average Reward for Agent 0 this episode : -20.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.0838\n",
      "Average Reward for Agent 1 this episode : -20.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 200.4754\n",
      "Average Reward for Agent 2 this episode : -45.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 607.2479\n",
      "Average Reward for Agent 3 this episode : -9.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.0577\n",
      "Average Reward for Agent 4 this episode : -12.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.6930\n",
      "Average Reward for Agent 5 this episode : -0.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9115\n",
      "Average Reward for Agent 6 this episode : -20.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 142.3904\n",
      "Average Reward for Agent 7 this episode : -18.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.4442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 8 this episode : -2.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.5067\n",
      "Average Reward for Agent 9 this episode : -3.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.2159\n",
      "Average Reward for Agent 10 this episode : -9.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 199.8507\n",
      "Average Reward for Agent 11 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1381\n",
      "Average Reward for Agent 12 this episode : -15.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.1842\n",
      "Average Reward for Agent 13 this episode : -8.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 186.1469\n",
      "Reducing exploration for all agents to 0.1\n",
      "Episode 134 is finished\n",
      "Average Reward for Agent 0 this episode : -34.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.7716\n",
      "Average Reward for Agent 1 this episode : -18.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 269.9113\n",
      "Average Reward for Agent 2 this episode : -39.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 555.9478\n",
      "Average Reward for Agent 3 this episode : -9.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.1607\n",
      "Average Reward for Agent 4 this episode : -9.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.6542\n",
      "Average Reward for Agent 5 this episode : -0.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5934\n",
      "Average Reward for Agent 6 this episode : -27.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 222.9645\n",
      "Average Reward for Agent 7 this episode : -23.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.8542\n",
      "Average Reward for Agent 8 this episode : -1.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.9918\n",
      "Average Reward for Agent 9 this episode : -1.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.8317\n",
      "Average Reward for Agent 10 this episode : -3.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 226.5485\n",
      "Average Reward for Agent 11 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7960\n",
      "Average Reward for Agent 12 this episode : -17.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.1319\n",
      "Average Reward for Agent 13 this episode : -3.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 271.0791\n",
      "Reducing exploration for all agents to 0.0983\n",
      "Episode 135 is finished\n",
      "Average Reward for Agent 0 this episode : -25.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 124.5821\n",
      "Average Reward for Agent 1 this episode : -19.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 195.3840\n",
      "Average Reward for Agent 2 this episode : -56.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 440.4575\n",
      "Average Reward for Agent 3 this episode : -9.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.4623\n",
      "Average Reward for Agent 4 this episode : -13.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.2531\n",
      "Average Reward for Agent 5 this episode : -0.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7556\n",
      "Average Reward for Agent 6 this episode : -5.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 243.1618\n",
      "Average Reward for Agent 7 this episode : -13.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.0922\n",
      "Average Reward for Agent 8 this episode : -1.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.3633\n",
      "Average Reward for Agent 9 this episode : -2.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.9084\n",
      "Average Reward for Agent 10 this episode : -14.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.6552\n",
      "Average Reward for Agent 11 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5592\n",
      "Average Reward for Agent 12 this episode : -23.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.7058\n",
      "Average Reward for Agent 13 this episode : -12.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 116.3850\n",
      "Reducing exploration for all agents to 0.0966\n",
      "Episode 136 is finished\n",
      "Average Reward for Agent 0 this episode : -27.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.2780\n",
      "Average Reward for Agent 1 this episode : -17.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 279.2607\n",
      "Average Reward for Agent 2 this episode : -32.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 618.7347\n",
      "Average Reward for Agent 3 this episode : -13.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.7784\n",
      "Average Reward for Agent 4 this episode : -14.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.2576\n",
      "Average Reward for Agent 5 this episode : -0.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.4208\n",
      "Average Reward for Agent 6 this episode : -3.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.6112\n",
      "Average Reward for Agent 7 this episode : -20.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.3927\n",
      "Average Reward for Agent 8 this episode : -1.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0568\n",
      "Average Reward for Agent 9 this episode : -3.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3507\n",
      "Average Reward for Agent 10 this episode : -26.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 117.8879\n",
      "Average Reward for Agent 11 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2059\n",
      "Average Reward for Agent 12 this episode : -45.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 89.8862\n",
      "Average Reward for Agent 13 this episode : -10.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.2828\n",
      "Reducing exploration for all agents to 0.0949\n",
      "Episode 137 is finished\n",
      "Average Reward for Agent 0 this episode : -40.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.7119\n",
      "Average Reward for Agent 1 this episode : -20.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 167.8420\n",
      "Average Reward for Agent 2 this episode : -32.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 660.7380\n",
      "Average Reward for Agent 3 this episode : -9.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 115.0016\n",
      "Average Reward for Agent 4 this episode : -12.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.8732\n",
      "Average Reward for Agent 5 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7130\n",
      "Average Reward for Agent 6 this episode : -4.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 117.6939\n",
      "Average Reward for Agent 7 this episode : -16.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.0918\n",
      "Average Reward for Agent 8 this episode : -6.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9398\n",
      "Average Reward for Agent 9 this episode : -5.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2458\n",
      "Average Reward for Agent 10 this episode : -0.33\n",
      "Saving architecture, weights, optimizer state for best agent-10\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent10_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.8619\n",
      "Average Reward for Agent 11 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5647\n",
      "Average Reward for Agent 12 this episode : -19.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.2724\n",
      "Average Reward for Agent 13 this episode : -34.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.6410\n",
      "Reducing exploration for all agents to 0.0933\n",
      "Episode 138 is finished\n",
      "Average Reward for Agent 0 this episode : -32.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.3810\n",
      "Average Reward for Agent 1 this episode : -23.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.4537\n",
      "Average Reward for Agent 2 this episode : -40.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 594.4152\n",
      "Average Reward for Agent 3 this episode : -8.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.0068\n",
      "Average Reward for Agent 4 this episode : -22.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.4168\n",
      "Average Reward for Agent 5 this episode : -1.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.9277\n",
      "Average Reward for Agent 6 this episode : -6.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.0925\n",
      "Average Reward for Agent 7 this episode : -12.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.2283\n",
      "Average Reward for Agent 8 this episode : -4.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.8164\n",
      "Average Reward for Agent 9 this episode : -2.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.4612\n",
      "Average Reward for Agent 10 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 115.5463\n",
      "Average Reward for Agent 11 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0411\n",
      "Average Reward for Agent 12 this episode : -26.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.2586\n",
      "Average Reward for Agent 13 this episode : -7.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.0818\n",
      "Reducing exploration for all agents to 0.0917\n",
      "Episode 139 is finished\n",
      "Average Reward for Agent 0 this episode : -16.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 123.0996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 1 this episode : -22.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 220.4361\n",
      "Average Reward for Agent 2 this episode : -40.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 514.3062\n",
      "Average Reward for Agent 3 this episode : -9.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 100.8752\n",
      "Average Reward for Agent 4 this episode : -24.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.9813\n",
      "Average Reward for Agent 5 this episode : -4.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.9278\n",
      "Average Reward for Agent 6 this episode : -10.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 137.8856\n",
      "Average Reward for Agent 7 this episode : -13.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.2487\n",
      "Average Reward for Agent 8 this episode : -1.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.7879\n",
      "Average Reward for Agent 9 this episode : -5.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.7489\n",
      "Average Reward for Agent 10 this episode : -0.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.5300\n",
      "Average Reward for Agent 11 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.1720\n",
      "Average Reward for Agent 12 this episode : -16.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.0182\n",
      "Average Reward for Agent 13 this episode : -4.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.7816\n",
      "Reducing exploration for all agents to 0.0901\n",
      "Episode 140 is finished\n",
      "Average Reward for Agent 0 this episode : -13.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.2213\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -22.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 261.8577\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -39.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 207.5575\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -9.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.4563\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -18.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.9321\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -1.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.3554\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.9827\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -13.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.2933\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5718\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -2.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.0217\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.5684\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7742\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -16.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.8135\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -7.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.1974\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0886\n",
      "Episode 141 is finished\n",
      "Average Reward for Agent 0 this episode : -21.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.4820\n",
      "Average Reward for Agent 1 this episode : -20.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 276.6506\n",
      "Average Reward for Agent 2 this episode : -43.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 635.2786\n",
      "Average Reward for Agent 3 this episode : -8.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 117.0836\n",
      "Average Reward for Agent 4 this episode : -17.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 172.4916\n",
      "Average Reward for Agent 5 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1328\n",
      "Average Reward for Agent 6 this episode : -19.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 140.0569\n",
      "Average Reward for Agent 7 this episode : -12.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.3497\n",
      "Average Reward for Agent 8 this episode : -2.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.5164\n",
      "Average Reward for Agent 9 this episode : -3.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.4454\n",
      "Average Reward for Agent 10 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.5669\n",
      "Average Reward for Agent 11 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8526\n",
      "Average Reward for Agent 12 this episode : -14.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.2121\n",
      "Average Reward for Agent 13 this episode : -12.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.1836\n",
      "Reducing exploration for all agents to 0.0871\n",
      "Episode 142 is finished\n",
      "Average Reward for Agent 0 this episode : -15.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.7914\n",
      "Average Reward for Agent 1 this episode : -15.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 302.6029\n",
      "Average Reward for Agent 2 this episode : -20.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 270.8188\n",
      "Average Reward for Agent 3 this episode : -10.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.2847\n",
      "Average Reward for Agent 4 this episode : -21.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.6452\n",
      "Average Reward for Agent 5 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.9087\n",
      "Average Reward for Agent 6 this episode : -3.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 124.7460\n",
      "Average Reward for Agent 7 this episode : -12.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.5089\n",
      "Average Reward for Agent 8 this episode : -1.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.8219\n",
      "Average Reward for Agent 9 this episode : -1.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.9639\n",
      "Average Reward for Agent 10 this episode : -0.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.9628\n",
      "Average Reward for Agent 11 this episode : -0.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9264\n",
      "Average Reward for Agent 12 this episode : -13.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.0753\n",
      "Average Reward for Agent 13 this episode : -44.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.8513\n",
      "Reducing exploration for all agents to 0.0856\n",
      "Episode 143 is finished\n",
      "Average Reward for Agent 0 this episode : -10.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.1525\n",
      "Average Reward for Agent 1 this episode : -22.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 262.0148\n",
      "Average Reward for Agent 2 this episode : -21.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 257.1731\n",
      "Average Reward for Agent 3 this episode : -8.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.0769\n",
      "Average Reward for Agent 4 this episode : -19.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 190.1788\n",
      "Average Reward for Agent 5 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.2429\n",
      "Average Reward for Agent 6 this episode : -5.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.4044\n",
      "Average Reward for Agent 7 this episode : -14.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.5975\n",
      "Average Reward for Agent 8 this episode : -4.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0891\n",
      "Average Reward for Agent 9 this episode : -3.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.4222\n",
      "Average Reward for Agent 10 this episode : -1.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.3415\n",
      "Average Reward for Agent 11 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9361\n",
      "Average Reward for Agent 12 this episode : -14.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.5725\n",
      "Average Reward for Agent 13 this episode : -59.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.6206\n",
      "Reducing exploration for all agents to 0.0841\n",
      "Episode 144 is finished\n",
      "Average Reward for Agent 0 this episode : -14.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 133.5361\n",
      "Average Reward for Agent 1 this episode : -17.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 219.5840\n",
      "Average Reward for Agent 2 this episode : -19.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 423.0189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 3 this episode : -7.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.1420\n",
      "Average Reward for Agent 4 this episode : -19.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 178.0154\n",
      "Average Reward for Agent 5 this episode : -0.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.7133\n",
      "Average Reward for Agent 6 this episode : -2.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 136.0339\n",
      "Average Reward for Agent 7 this episode : -14.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 116.9850\n",
      "Average Reward for Agent 8 this episode : -1.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.3791\n",
      "Average Reward for Agent 9 this episode : -5.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7992\n",
      "Average Reward for Agent 10 this episode : -1.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.5067\n",
      "Average Reward for Agent 11 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2290\n",
      "Average Reward for Agent 12 this episode : -25.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.9158\n",
      "Average Reward for Agent 13 this episode : -47.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 208.8654\n",
      "Reducing exploration for all agents to 0.0827\n",
      "Episode 145 is finished\n",
      "Average Reward for Agent 0 this episode : -11.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.1024\n",
      "Average Reward for Agent 1 this episode : -22.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 274.8244\n",
      "Average Reward for Agent 2 this episode : -10.66\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent2_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 471.1725\n",
      "Average Reward for Agent 3 this episode : -8.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.0931\n",
      "Average Reward for Agent 4 this episode : -19.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 129.3636\n",
      "Average Reward for Agent 5 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2761\n",
      "Average Reward for Agent 6 this episode : -1.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 123.8703\n",
      "Average Reward for Agent 7 this episode : -13.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.2753\n",
      "Average Reward for Agent 8 this episode : -1.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9413\n",
      "Average Reward for Agent 9 this episode : -11.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.6409\n",
      "Average Reward for Agent 10 this episode : -5.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.7689\n",
      "Average Reward for Agent 11 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7949\n",
      "Average Reward for Agent 12 this episode : -14.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 100.1470\n",
      "Average Reward for Agent 13 this episode : -55.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 199.0208\n",
      "Reducing exploration for all agents to 0.0812\n",
      "Episode 146 is finished\n",
      "Average Reward for Agent 0 this episode : -12.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.4557\n",
      "Average Reward for Agent 1 this episode : -10.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 298.1434\n",
      "Average Reward for Agent 2 this episode : -15.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 177.4566\n",
      "Average Reward for Agent 3 this episode : -11.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.3265\n",
      "Average Reward for Agent 4 this episode : -19.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 216.9553\n",
      "Average Reward for Agent 5 this episode : -0.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3102\n",
      "Average Reward for Agent 6 this episode : -3.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 137.2385\n",
      "Average Reward for Agent 7 this episode : -11.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.0478\n",
      "Average Reward for Agent 8 this episode : -3.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5721\n",
      "Average Reward for Agent 9 this episode : -3.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.1878\n",
      "Average Reward for Agent 10 this episode : -2.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.2629\n",
      "Average Reward for Agent 11 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5526\n",
      "Average Reward for Agent 12 this episode : -24.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.6683\n",
      "Average Reward for Agent 13 this episode : -65.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.8778\n",
      "Reducing exploration for all agents to 0.0798\n",
      "Episode 147 is finished\n",
      "Average Reward for Agent 0 this episode : -10.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.6707\n",
      "Average Reward for Agent 1 this episode : -24.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 242.1986\n",
      "Average Reward for Agent 2 this episode : -28.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 232.5282\n",
      "Average Reward for Agent 3 this episode : -12.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.2065\n",
      "Average Reward for Agent 4 this episode : -25.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.8296\n",
      "Average Reward for Agent 5 this episode : -0.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0961\n",
      "Average Reward for Agent 6 this episode : -0.78\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.0793\n",
      "Average Reward for Agent 7 this episode : -14.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.8969\n",
      "Average Reward for Agent 8 this episode : -2.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0417\n",
      "Average Reward for Agent 9 this episode : -6.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.7146\n",
      "Average Reward for Agent 10 this episode : -7.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.2151\n",
      "Average Reward for Agent 11 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1174\n",
      "Average Reward for Agent 12 this episode : -30.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.3932\n",
      "Average Reward for Agent 13 this episode : -34.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 523.2546\n",
      "Reducing exploration for all agents to 0.0785\n",
      "Episode 148 is finished\n",
      "Average Reward for Agent 0 this episode : -10.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.0088\n",
      "Average Reward for Agent 1 this episode : -16.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 267.1527\n",
      "Average Reward for Agent 2 this episode : -32.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 297.4489\n",
      "Average Reward for Agent 3 this episode : -8.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.9499\n",
      "Average Reward for Agent 4 this episode : -17.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.9422\n",
      "Average Reward for Agent 5 this episode : -0.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7995\n",
      "Average Reward for Agent 6 this episode : -1.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.6523\n",
      "Average Reward for Agent 7 this episode : -12.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.0842\n",
      "Average Reward for Agent 8 this episode : -5.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6367\n",
      "Average Reward for Agent 9 this episode : -7.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.1622\n",
      "Average Reward for Agent 10 this episode : -0.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.5175\n",
      "Average Reward for Agent 11 this episode : -0.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4048\n",
      "Average Reward for Agent 12 this episode : -15.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 136.2436\n",
      "Average Reward for Agent 13 this episode : -53.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 140.1347\n",
      "Reducing exploration for all agents to 0.0771\n",
      "Episode 149 is finished\n",
      "Average Reward for Agent 0 this episode : -10.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.0119\n",
      "Average Reward for Agent 1 this episode : -22.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 258.9028\n",
      "Average Reward for Agent 2 this episode : -9.69\n",
      "Saving architecture, weights, optimizer state for best agent-2\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent2_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 282.0949\n",
      "Average Reward for Agent 3 this episode : -3.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.5517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 4 this episode : -16.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.8605\n",
      "Average Reward for Agent 5 this episode : -0.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9480\n",
      "Average Reward for Agent 6 this episode : -2.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.8341\n",
      "Average Reward for Agent 7 this episode : -13.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.7591\n",
      "Average Reward for Agent 8 this episode : -3.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3872\n",
      "Average Reward for Agent 9 this episode : -2.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2377\n",
      "Average Reward for Agent 10 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.0617\n",
      "Average Reward for Agent 11 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0040\n",
      "Average Reward for Agent 12 this episode : -17.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 124.3013\n",
      "Average Reward for Agent 13 this episode : -46.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 230.5553\n",
      "Reducing exploration for all agents to 0.0758\n",
      "Episode 150 is finished\n",
      "Average Reward for Agent 0 this episode : -18.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.4191\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -19.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 276.3064\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -30.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 206.7897\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -8.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.3049\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -18.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.1511\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.4550\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -4.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.1582\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -14.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 147.5610\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7861\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -5.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.8103\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -1.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.6799\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -1.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6262\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -28.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.9004\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -36.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 204.4188\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0745\n",
      "Episode 151 is finished\n",
      "Average Reward for Agent 0 this episode : -13.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.7882\n",
      "Average Reward for Agent 1 this episode : -17.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 152.6155\n",
      "Average Reward for Agent 2 this episode : -22.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 263.6378\n",
      "Average Reward for Agent 3 this episode : -7.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.6192\n",
      "Average Reward for Agent 4 this episode : -24.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.7366\n",
      "Average Reward for Agent 5 this episode : -0.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6443\n",
      "Average Reward for Agent 6 this episode : -4.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.8668\n",
      "Average Reward for Agent 7 this episode : -14.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.3525\n",
      "Average Reward for Agent 8 this episode : -1.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.0070\n",
      "Average Reward for Agent 9 this episode : -4.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.3176\n",
      "Average Reward for Agent 10 this episode : -1.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.7573\n",
      "Average Reward for Agent 11 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3918\n",
      "Average Reward for Agent 12 this episode : -21.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.3505\n",
      "Average Reward for Agent 13 this episode : -48.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 301.9410\n",
      "Reducing exploration for all agents to 0.0732\n",
      "Episode 152 is finished\n",
      "Average Reward for Agent 0 this episode : -11.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.3719\n",
      "Average Reward for Agent 1 this episode : -18.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 148.8160\n",
      "Average Reward for Agent 2 this episode : -17.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 250.4957\n",
      "Average Reward for Agent 3 this episode : -6.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.5246\n",
      "Average Reward for Agent 4 this episode : -19.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 145.9533\n",
      "Average Reward for Agent 5 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7475\n",
      "Average Reward for Agent 6 this episode : -11.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.6287\n",
      "Average Reward for Agent 7 this episode : -12.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.4382\n",
      "Average Reward for Agent 8 this episode : -6.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2464\n",
      "Average Reward for Agent 9 this episode : -3.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.8582\n",
      "Average Reward for Agent 10 this episode : -1.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.2978\n",
      "Average Reward for Agent 11 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7372\n",
      "Average Reward for Agent 12 this episode : -16.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 185.6357\n",
      "Average Reward for Agent 13 this episode : -37.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 144.6906\n",
      "Reducing exploration for all agents to 0.072\n",
      "Episode 153 is finished\n",
      "Average Reward for Agent 0 this episode : -13.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.5772\n",
      "Average Reward for Agent 1 this episode : -19.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 233.0427\n",
      "Average Reward for Agent 2 this episode : -21.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 235.1276\n",
      "Average Reward for Agent 3 this episode : -8.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.7796\n",
      "Average Reward for Agent 4 this episode : -18.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 161.3777\n",
      "Average Reward for Agent 5 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1758\n",
      "Average Reward for Agent 6 this episode : -4.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.7727\n",
      "Average Reward for Agent 7 this episode : -14.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.7111\n",
      "Average Reward for Agent 8 this episode : -2.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3060\n",
      "Average Reward for Agent 9 this episode : -5.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.0643\n",
      "Average Reward for Agent 10 this episode : -7.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.5473\n",
      "Average Reward for Agent 11 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1328\n",
      "Average Reward for Agent 12 this episode : -19.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.4607\n",
      "Average Reward for Agent 13 this episode : -40.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 372.9155\n",
      "Reducing exploration for all agents to 0.0707\n",
      "Episode 154 is finished\n",
      "Average Reward for Agent 0 this episode : -12.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.1171\n",
      "Average Reward for Agent 1 this episode : -16.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 229.8139\n",
      "Average Reward for Agent 2 this episode : -21.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 236.5393\n",
      "Average Reward for Agent 3 this episode : -12.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.1346\n",
      "Average Reward for Agent 4 this episode : -15.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 183.4592\n",
      "Average Reward for Agent 5 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 6 this episode : -0.71\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.9083\n",
      "Average Reward for Agent 7 this episode : -14.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.7201\n",
      "Average Reward for Agent 8 this episode : -3.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.3901\n",
      "Average Reward for Agent 9 this episode : -3.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.5950\n",
      "Average Reward for Agent 10 this episode : -11.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.8272\n",
      "Average Reward for Agent 11 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3611\n",
      "Average Reward for Agent 12 this episode : -14.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.6425\n",
      "Average Reward for Agent 13 this episode : -31.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.0499\n",
      "Reducing exploration for all agents to 0.0695\n",
      "Episode 155 is finished\n",
      "Average Reward for Agent 0 this episode : -11.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.8922\n",
      "Average Reward for Agent 1 this episode : -15.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 274.0343\n",
      "Average Reward for Agent 2 this episode : -18.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 164.2493\n",
      "Average Reward for Agent 3 this episode : -12.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 140.4069\n",
      "Average Reward for Agent 4 this episode : -14.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.4139\n",
      "Average Reward for Agent 5 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1418\n",
      "Average Reward for Agent 6 this episode : -1.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.5084\n",
      "Average Reward for Agent 7 this episode : -13.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.1065\n",
      "Average Reward for Agent 8 this episode : -3.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.9020\n",
      "Average Reward for Agent 9 this episode : -4.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.0342\n",
      "Average Reward for Agent 10 this episode : -19.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.0968\n",
      "Average Reward for Agent 11 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8442\n",
      "Average Reward for Agent 12 this episode : -44.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 191.0829\n",
      "Average Reward for Agent 13 this episode : -46.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 347.8455\n",
      "Reducing exploration for all agents to 0.0683\n",
      "Episode 156 is finished\n",
      "Average Reward for Agent 0 this episode : -14.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.9713\n",
      "Average Reward for Agent 1 this episode : -13.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 233.3898\n",
      "Average Reward for Agent 2 this episode : -21.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.0093\n",
      "Average Reward for Agent 3 this episode : -10.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.6360\n",
      "Average Reward for Agent 4 this episode : -18.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.9543\n",
      "Average Reward for Agent 5 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2292\n",
      "Average Reward for Agent 6 this episode : -0.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.7725\n",
      "Average Reward for Agent 7 this episode : -11.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.5262\n",
      "Average Reward for Agent 8 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.3973\n",
      "Average Reward for Agent 9 this episode : -9.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.0326\n",
      "Average Reward for Agent 10 this episode : -7.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.8693\n",
      "Average Reward for Agent 11 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8657\n",
      "Average Reward for Agent 12 this episode : -30.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.0555\n",
      "Average Reward for Agent 13 this episode : -35.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 188.4673\n",
      "Reducing exploration for all agents to 0.0672\n",
      "Episode 157 is finished\n",
      "Average Reward for Agent 0 this episode : -14.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.7041\n",
      "Average Reward for Agent 1 this episode : -21.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 158.4648\n",
      "Average Reward for Agent 2 this episode : -18.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 140.6715\n",
      "Average Reward for Agent 3 this episode : -10.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.6954\n",
      "Average Reward for Agent 4 this episode : -16.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.8211\n",
      "Average Reward for Agent 5 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0353\n",
      "Average Reward for Agent 6 this episode : -0.68\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.7375\n",
      "Average Reward for Agent 7 this episode : -14.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.1600\n",
      "Average Reward for Agent 8 this episode : -1.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.9931\n",
      "Average Reward for Agent 9 this episode : -8.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.1438\n",
      "Average Reward for Agent 10 this episode : -1.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.4315\n",
      "Average Reward for Agent 11 this episode : -0.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7171\n",
      "Average Reward for Agent 12 this episode : -20.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.7137\n",
      "Average Reward for Agent 13 this episode : -45.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.7361\n",
      "Reducing exploration for all agents to 0.066\n",
      "Episode 158 is finished\n",
      "Average Reward for Agent 0 this episode : -15.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.4995\n",
      "Average Reward for Agent 1 this episode : -18.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 208.7967\n",
      "Average Reward for Agent 2 this episode : -19.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 224.1400\n",
      "Average Reward for Agent 3 this episode : -5.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.7118\n",
      "Average Reward for Agent 4 this episode : -14.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.3945\n",
      "Average Reward for Agent 5 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.1034\n",
      "Average Reward for Agent 6 this episode : -0.67\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.9970\n",
      "Average Reward for Agent 7 this episode : -12.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 121.2799\n",
      "Average Reward for Agent 8 this episode : -1.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.8106\n",
      "Average Reward for Agent 9 this episode : -7.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.0651\n",
      "Average Reward for Agent 10 this episode : -0.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.2903\n",
      "Average Reward for Agent 11 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4380\n",
      "Average Reward for Agent 12 this episode : -22.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.9547\n",
      "Average Reward for Agent 13 this episode : -36.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 297.8316\n",
      "Reducing exploration for all agents to 0.0649\n",
      "Episode 159 is finished\n",
      "Average Reward for Agent 0 this episode : -9.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.4759\n",
      "Average Reward for Agent 1 this episode : -25.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 306.2592\n",
      "Average Reward for Agent 2 this episode : -20.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 233.7230\n",
      "Average Reward for Agent 3 this episode : -9.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.8025\n",
      "Average Reward for Agent 4 this episode : -11.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.4353\n",
      "Average Reward for Agent 5 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0673\n",
      "Average Reward for Agent 6 this episode : -0.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.2995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 7 this episode : -12.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.6643\n",
      "Average Reward for Agent 8 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.3488\n",
      "Average Reward for Agent 9 this episode : -9.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.4438\n",
      "Average Reward for Agent 10 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.3214\n",
      "Average Reward for Agent 11 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1138\n",
      "Average Reward for Agent 12 this episode : -32.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.9238\n",
      "Average Reward for Agent 13 this episode : -43.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.5245\n",
      "Reducing exploration for all agents to 0.0638\n",
      "Episode 160 is finished\n",
      "Average Reward for Agent 0 this episode : -24.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.0161\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -18.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 233.5363\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -14.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 273.5960\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -7.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.9788\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -16.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.4980\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -1.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.5229\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -0.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.2716\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -13.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.5329\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.5125\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -9.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.5673\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -1.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.9654\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0623\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -11.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.2126\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -43.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 257.5831\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0627\n",
      "Episode 161 is finished\n",
      "Average Reward for Agent 0 this episode : -19.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.3044\n",
      "Average Reward for Agent 1 this episode : -16.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 155.0058\n",
      "Average Reward for Agent 2 this episode : -10.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 184.4330\n",
      "Average Reward for Agent 3 this episode : -10.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.2144\n",
      "Average Reward for Agent 4 this episode : -17.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.6621\n",
      "Average Reward for Agent 5 this episode : -1.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.7129\n",
      "Average Reward for Agent 6 this episode : -0.67\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.7206\n",
      "Average Reward for Agent 7 this episode : -14.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.0071\n",
      "Average Reward for Agent 8 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.7095\n",
      "Average Reward for Agent 9 this episode : -3.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.7551\n",
      "Average Reward for Agent 10 this episode : -1.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.8116\n",
      "Average Reward for Agent 11 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9826\n",
      "Average Reward for Agent 12 this episode : -23.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 125.5495\n",
      "Average Reward for Agent 13 this episode : -44.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 247.7288\n",
      "Reducing exploration for all agents to 0.0616\n",
      "Episode 162 is finished\n",
      "Average Reward for Agent 0 this episode : -12.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.7374\n",
      "Average Reward for Agent 1 this episode : -21.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 158.2500\n",
      "Average Reward for Agent 2 this episode : -22.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 256.8298\n",
      "Average Reward for Agent 3 this episode : -13.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.3157\n",
      "Average Reward for Agent 4 this episode : -22.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 111.4479\n",
      "Average Reward for Agent 5 this episode : -1.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5298\n",
      "Average Reward for Agent 6 this episode : -2.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.7659\n",
      "Average Reward for Agent 7 this episode : -14.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.4471\n",
      "Average Reward for Agent 8 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.4683\n",
      "Average Reward for Agent 9 this episode : -13.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.1627\n",
      "Average Reward for Agent 10 this episode : -5.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.0218\n",
      "Average Reward for Agent 11 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3774\n",
      "Average Reward for Agent 12 this episode : -11.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.4791\n",
      "Average Reward for Agent 13 this episode : -21.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.7527\n",
      "Reducing exploration for all agents to 0.0605\n",
      "Episode 163 is finished\n",
      "Average Reward for Agent 0 this episode : -14.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 137.6764\n",
      "Average Reward for Agent 1 this episode : -22.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 226.1348\n",
      "Average Reward for Agent 2 this episode : -33.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 226.5347\n",
      "Average Reward for Agent 3 this episode : -7.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.9846\n",
      "Average Reward for Agent 4 this episode : -20.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 269.7645\n",
      "Average Reward for Agent 5 this episode : -2.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.0539\n",
      "Average Reward for Agent 6 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.9576\n",
      "Average Reward for Agent 7 this episode : -14.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.2540\n",
      "Average Reward for Agent 8 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.0157\n",
      "Average Reward for Agent 9 this episode : -2.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.5302\n",
      "Average Reward for Agent 10 this episode : -20.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 121.3114\n",
      "Average Reward for Agent 11 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3207\n",
      "Average Reward for Agent 12 this episode : -15.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 141.4496\n",
      "Average Reward for Agent 13 this episode : -13.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 367.4000\n",
      "Reducing exploration for all agents to 0.0595\n",
      "Episode 164 is finished\n",
      "Average Reward for Agent 0 this episode : -10.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.1096\n",
      "Average Reward for Agent 1 this episode : -20.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 284.4751\n",
      "Average Reward for Agent 2 this episode : -32.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 192.7763\n",
      "Average Reward for Agent 3 this episode : -6.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.0298\n",
      "Average Reward for Agent 4 this episode : -16.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.2794\n",
      "Average Reward for Agent 5 this episode : -1.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 6 this episode : -1.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.9934\n",
      "Average Reward for Agent 7 this episode : -14.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.8340\n",
      "Average Reward for Agent 8 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.6809\n",
      "Average Reward for Agent 9 this episode : -6.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.7720\n",
      "Average Reward for Agent 10 this episode : -3.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.8811\n",
      "Average Reward for Agent 11 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6563\n",
      "Average Reward for Agent 12 this episode : -8.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.1973\n",
      "Average Reward for Agent 13 this episode : -8.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.8685\n",
      "Reducing exploration for all agents to 0.0585\n",
      "Episode 165 is finished\n",
      "Average Reward for Agent 0 this episode : -13.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.3380\n",
      "Average Reward for Agent 1 this episode : -18.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 222.6125\n",
      "Average Reward for Agent 2 this episode : -31.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 218.5067\n",
      "Average Reward for Agent 3 this episode : -13.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.6870\n",
      "Average Reward for Agent 4 this episode : -16.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 256.2220\n",
      "Average Reward for Agent 5 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8982\n",
      "Average Reward for Agent 6 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.1689\n",
      "Average Reward for Agent 7 this episode : -14.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.4838\n",
      "Average Reward for Agent 8 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.4678\n",
      "Average Reward for Agent 9 this episode : -2.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.7635\n",
      "Average Reward for Agent 10 this episode : -1.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.5179\n",
      "Average Reward for Agent 11 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6129\n",
      "Average Reward for Agent 12 this episode : -11.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.6220\n",
      "Average Reward for Agent 13 this episode : -10.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.0608\n",
      "Reducing exploration for all agents to 0.0575\n",
      "Episode 166 is finished\n",
      "Average Reward for Agent 0 this episode : -14.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.8073\n",
      "Average Reward for Agent 1 this episode : -17.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 163.0593\n",
      "Average Reward for Agent 2 this episode : -29.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 267.1552\n",
      "Average Reward for Agent 3 this episode : -10.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.1807\n",
      "Average Reward for Agent 4 this episode : -15.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 191.2004\n",
      "Average Reward for Agent 5 this episode : -0.02\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.8631\n",
      "Average Reward for Agent 6 this episode : -0.41\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.8980\n",
      "Average Reward for Agent 7 this episode : -11.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.0004\n",
      "Average Reward for Agent 8 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2776\n",
      "Average Reward for Agent 9 this episode : -14.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.9617\n",
      "Average Reward for Agent 10 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.4254\n",
      "Average Reward for Agent 11 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9825\n",
      "Average Reward for Agent 12 this episode : -21.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.9092\n",
      "Average Reward for Agent 13 this episode : -20.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 151.0814\n",
      "Reducing exploration for all agents to 0.0565\n",
      "Episode 167 is finished\n",
      "Average Reward for Agent 0 this episode : -13.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.7587\n",
      "Average Reward for Agent 1 this episode : -18.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 243.4170\n",
      "Average Reward for Agent 2 this episode : -35.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.6166\n",
      "Average Reward for Agent 3 this episode : -6.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.4279\n",
      "Average Reward for Agent 4 this episode : -20.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 178.0239\n",
      "Average Reward for Agent 5 this episode : -0.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3488\n",
      "Average Reward for Agent 6 this episode : -0.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.6418\n",
      "Average Reward for Agent 7 this episode : -13.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.0373\n",
      "Average Reward for Agent 8 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.7184\n",
      "Average Reward for Agent 9 this episode : -8.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.0114\n",
      "Average Reward for Agent 10 this episode : -32.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.1726\n",
      "Average Reward for Agent 11 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8493\n",
      "Average Reward for Agent 12 this episode : -14.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.6958\n",
      "Average Reward for Agent 13 this episode : -6.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 166.4492\n",
      "Reducing exploration for all agents to 0.0555\n",
      "Episode 168 is finished\n",
      "Average Reward for Agent 0 this episode : -31.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 143.8265\n",
      "Average Reward for Agent 1 this episode : -13.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 320.0261\n",
      "Average Reward for Agent 2 this episode : -21.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 115.5485\n",
      "Average Reward for Agent 3 this episode : -15.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.7792\n",
      "Average Reward for Agent 4 this episode : -16.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.7163\n",
      "Average Reward for Agent 5 this episode : -0.02\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7235\n",
      "Average Reward for Agent 6 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.5986\n",
      "Average Reward for Agent 7 this episode : -13.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.1584\n",
      "Average Reward for Agent 8 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.9314\n",
      "Average Reward for Agent 9 this episode : -3.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.3461\n",
      "Average Reward for Agent 10 this episode : -54.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.4147\n",
      "Average Reward for Agent 11 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6974\n",
      "Average Reward for Agent 12 this episode : -10.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.1064\n",
      "Average Reward for Agent 13 this episode : -8.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.6132\n",
      "Reducing exploration for all agents to 0.0546\n",
      "Episode 169 is finished\n",
      "Average Reward for Agent 0 this episode : -14.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.7532\n",
      "Average Reward for Agent 1 this episode : -18.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 206.2719\n",
      "Average Reward for Agent 2 this episode : -36.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.1575\n",
      "Average Reward for Agent 3 this episode : -6.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.8636\n",
      "Average Reward for Agent 4 this episode : -22.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.9236\n",
      "Average Reward for Agent 5 this episode : -0.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8253\n",
      "Average Reward for Agent 6 this episode : -1.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.7563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 7 this episode : -14.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.1190\n",
      "Average Reward for Agent 8 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.1885\n",
      "Average Reward for Agent 9 this episode : -10.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.9754\n",
      "Average Reward for Agent 10 this episode : -40.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.0654\n",
      "Average Reward for Agent 11 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0707\n",
      "Average Reward for Agent 12 this episode : -14.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.4226\n",
      "Average Reward for Agent 13 this episode : -13.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.9390\n",
      "Reducing exploration for all agents to 0.0536\n",
      "Episode 170 is finished\n",
      "Average Reward for Agent 0 this episode : -12.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.0571\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -20.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 167.0252\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -38.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 127.5517\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -9.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.6797\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -19.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.0223\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8022\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.8272\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -14.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.9476\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.5357\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -4.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.2573\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -36.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.2162\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8725\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -14.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.5088\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -17.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.5017\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0527\n",
      "Episode 171 is finished\n",
      "Average Reward for Agent 0 this episode : -21.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.5906\n",
      "Average Reward for Agent 1 this episode : -13.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 164.1079\n",
      "Average Reward for Agent 2 this episode : -24.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 275.8481\n",
      "Average Reward for Agent 3 this episode : -12.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.5230\n",
      "Average Reward for Agent 4 this episode : -15.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 233.9356\n",
      "Average Reward for Agent 5 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4476\n",
      "Average Reward for Agent 6 this episode : -0.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.0084\n",
      "Average Reward for Agent 7 this episode : -13.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.6243\n",
      "Average Reward for Agent 8 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.1737\n",
      "Average Reward for Agent 9 this episode : -6.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.4718\n",
      "Average Reward for Agent 10 this episode : -34.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 596.2598\n",
      "Average Reward for Agent 11 this episode : -0.15\n",
      "Saving architecture, weights, optimizer state for best agent-11\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent11_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7479\n",
      "Average Reward for Agent 12 this episode : -9.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.8027\n",
      "Average Reward for Agent 13 this episode : -4.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.7684\n",
      "Reducing exploration for all agents to 0.0518\n",
      "Episode 172 is finished\n",
      "Average Reward for Agent 0 this episode : -11.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.5995\n",
      "Average Reward for Agent 1 this episode : -18.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.6346\n",
      "Average Reward for Agent 2 this episode : -20.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 295.8344\n",
      "Average Reward for Agent 3 this episode : -8.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.0504\n",
      "Average Reward for Agent 4 this episode : -17.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 153.9189\n",
      "Average Reward for Agent 5 this episode : -0.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4487\n",
      "Average Reward for Agent 6 this episode : -0.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.5913\n",
      "Average Reward for Agent 7 this episode : -13.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.9544\n",
      "Average Reward for Agent 8 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.1403\n",
      "Average Reward for Agent 9 this episode : -11.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.6278\n",
      "Average Reward for Agent 10 this episode : -1.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 379.2169\n",
      "Average Reward for Agent 11 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4966\n",
      "Average Reward for Agent 12 this episode : -28.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.6022\n",
      "Average Reward for Agent 13 this episode : -38.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.8040\n",
      "Reducing exploration for all agents to 0.0509\n",
      "Episode 173 is finished\n",
      "Average Reward for Agent 0 this episode : -12.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.9709\n",
      "Average Reward for Agent 1 this episode : -17.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.3614\n",
      "Average Reward for Agent 2 this episode : -16.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 163.4277\n",
      "Average Reward for Agent 3 this episode : -9.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.1828\n",
      "Average Reward for Agent 4 this episode : -17.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 89.1792\n",
      "Average Reward for Agent 5 this episode : -0.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2069\n",
      "Average Reward for Agent 6 this episode : -1.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.1955\n",
      "Average Reward for Agent 7 this episode : -14.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.8451\n",
      "Average Reward for Agent 8 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.4129\n",
      "Average Reward for Agent 9 this episode : -13.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.6345\n",
      "Average Reward for Agent 10 this episode : -1.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 442.5150\n",
      "Average Reward for Agent 11 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5160\n",
      "Average Reward for Agent 12 this episode : -27.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.3033\n",
      "Average Reward for Agent 13 this episode : -44.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.1304\n",
      "Reducing exploration for all agents to 0.05\n",
      "Episode 174 is finished\n",
      "Average Reward for Agent 0 this episode : -10.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 123.1753\n",
      "Average Reward for Agent 1 this episode : -22.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 198.2453\n",
      "Average Reward for Agent 2 this episode : -12.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 167.8840\n",
      "Average Reward for Agent 3 this episode : -14.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 127.0371\n",
      "Average Reward for Agent 4 this episode : -19.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 153.3631\n",
      "Average Reward for Agent 5 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 6 this episode : -1.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.1684\n",
      "Average Reward for Agent 7 this episode : -14.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 141.3760\n",
      "Average Reward for Agent 8 this episode : -1.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.0159\n",
      "Average Reward for Agent 9 this episode : -7.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.3573\n",
      "Average Reward for Agent 10 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 139.1870\n",
      "Average Reward for Agent 11 this episode : -0.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3828\n",
      "Average Reward for Agent 12 this episode : -15.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.3222\n",
      "Average Reward for Agent 13 this episode : -48.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.2286\n",
      "Reducing exploration for all agents to 0.0492\n",
      "Episode 175 is finished\n",
      "Average Reward for Agent 0 this episode : -11.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.2595\n",
      "Average Reward for Agent 1 this episode : -19.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 190.2941\n",
      "Average Reward for Agent 2 this episode : -21.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.6896\n",
      "Average Reward for Agent 3 this episode : -6.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.9402\n",
      "Average Reward for Agent 4 this episode : -20.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 354.5342\n",
      "Average Reward for Agent 5 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8661\n",
      "Average Reward for Agent 6 this episode : -1.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.0187\n",
      "Average Reward for Agent 7 this episode : -14.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.0674\n",
      "Average Reward for Agent 8 this episode : -1.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.9879\n",
      "Average Reward for Agent 9 this episode : -7.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.8533\n",
      "Average Reward for Agent 10 this episode : -46.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.3234\n",
      "Average Reward for Agent 11 this episode : -4.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.7229\n",
      "Average Reward for Agent 12 this episode : -9.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.0220\n",
      "Average Reward for Agent 13 this episode : -42.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.5154\n",
      "Reducing exploration for all agents to 0.0483\n",
      "Episode 176 is finished\n",
      "Average Reward for Agent 0 this episode : -14.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.6658\n",
      "Average Reward for Agent 1 this episode : -15.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 206.1068\n",
      "Average Reward for Agent 2 this episode : -15.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.5509\n",
      "Average Reward for Agent 3 this episode : -10.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.0830\n",
      "Average Reward for Agent 4 this episode : -20.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 324.9373\n",
      "Average Reward for Agent 5 this episode : -0.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7636\n",
      "Average Reward for Agent 6 this episode : -1.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.3096\n",
      "Average Reward for Agent 7 this episode : -14.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.5526\n",
      "Average Reward for Agent 8 this episode : -2.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.9315\n",
      "Average Reward for Agent 9 this episode : -12.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.4717\n",
      "Average Reward for Agent 10 this episode : -26.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 183.1278\n",
      "Average Reward for Agent 11 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.7533\n",
      "Average Reward for Agent 12 this episode : -14.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.8980\n",
      "Average Reward for Agent 13 this episode : -46.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 111.9827\n",
      "Reducing exploration for all agents to 0.0475\n",
      "Episode 177 is finished\n",
      "Average Reward for Agent 0 this episode : -14.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.0980\n",
      "Average Reward for Agent 1 this episode : -21.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 282.8571\n",
      "Average Reward for Agent 2 this episode : -16.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.9251\n",
      "Average Reward for Agent 3 this episode : -10.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.1460\n",
      "Average Reward for Agent 4 this episode : -17.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 274.8062\n",
      "Average Reward for Agent 5 this episode : -0.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5183\n",
      "Average Reward for Agent 6 this episode : -2.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.4435\n",
      "Average Reward for Agent 7 this episode : -14.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 117.0476\n",
      "Average Reward for Agent 8 this episode : -1.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.5600\n",
      "Average Reward for Agent 9 this episode : -21.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.2603\n",
      "Average Reward for Agent 10 this episode : -51.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 535.8359\n",
      "Average Reward for Agent 11 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2032\n",
      "Average Reward for Agent 12 this episode : -12.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.1014\n",
      "Average Reward for Agent 13 this episode : -37.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.5695\n",
      "Reducing exploration for all agents to 0.0467\n",
      "Episode 178 is finished\n",
      "Average Reward for Agent 0 this episode : -35.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.9946\n",
      "Average Reward for Agent 1 this episode : -16.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 274.7206\n",
      "Average Reward for Agent 2 this episode : -13.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 179.6663\n",
      "Average Reward for Agent 3 this episode : -5.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.9991\n",
      "Average Reward for Agent 4 this episode : -19.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 233.1597\n",
      "Average Reward for Agent 5 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0871\n",
      "Average Reward for Agent 6 this episode : -1.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.8709\n",
      "Average Reward for Agent 7 this episode : -15.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.9756\n",
      "Average Reward for Agent 8 this episode : -0.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.1600\n",
      "Average Reward for Agent 9 this episode : -13.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.5887\n",
      "Average Reward for Agent 10 this episode : -10.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.0553\n",
      "Average Reward for Agent 11 this episode : -1.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3265\n",
      "Average Reward for Agent 12 this episode : -26.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.8986\n",
      "Average Reward for Agent 13 this episode : -53.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.8567\n",
      "Reducing exploration for all agents to 0.0459\n",
      "Episode 179 is finished\n",
      "Average Reward for Agent 0 this episode : -14.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.3389\n",
      "Average Reward for Agent 1 this episode : -9.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 163.0145\n",
      "Average Reward for Agent 2 this episode : -17.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.6721\n",
      "Average Reward for Agent 3 this episode : -3.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.7330\n",
      "Average Reward for Agent 4 this episode : -19.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 252.2169\n",
      "Average Reward for Agent 5 this episode : -1.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7773\n",
      "Average Reward for Agent 6 this episode : -1.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.9395\n",
      "Average Reward for Agent 7 this episode : -11.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.3212\n",
      "Average Reward for Agent 8 this episode : -0.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.0093\n",
      "Average Reward for Agent 9 this episode : -9.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.4662\n",
      "Average Reward for Agent 10 this episode : -51.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 265.9777\n",
      "Average Reward for Agent 11 this episode : -3.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3611\n",
      "Average Reward for Agent 12 this episode : -15.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.0541\n",
      "Average Reward for Agent 13 this episode : -53.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.7246\n",
      "Reducing exploration for all agents to 0.0451\n",
      "Episode 180 is finished\n",
      "Average Reward for Agent 0 this episode : -10.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.0020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -17.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 240.3597\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -21.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 176.2568\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -1.31\n",
      "Saving architecture, weights, optimizer state for best agent-3\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent3_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.2622\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -20.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 169.0627\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5249\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -3.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8646\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -14.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.6907\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.5889\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -9.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.3163\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -49.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 395.3576\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -11.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.1603\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -15.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.6050\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -27.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.2963\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0443\n",
      "Episode 181 is finished\n",
      "Average Reward for Agent 0 this episode : -13.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.9233\n",
      "Average Reward for Agent 1 this episode : -15.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 185.2955\n",
      "Average Reward for Agent 2 this episode : -19.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 157.3848\n",
      "Average Reward for Agent 3 this episode : -1.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.0221\n",
      "Average Reward for Agent 4 this episode : -18.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 164.7210\n",
      "Average Reward for Agent 5 this episode : -1.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4591\n",
      "Average Reward for Agent 6 this episode : -2.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.1788\n",
      "Average Reward for Agent 7 this episode : -14.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.7093\n",
      "Average Reward for Agent 8 this episode : -1.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2030\n",
      "Average Reward for Agent 9 this episode : -15.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.2194\n",
      "Average Reward for Agent 10 this episode : -53.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 645.4186\n",
      "Average Reward for Agent 11 this episode : -22.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.6108\n",
      "Average Reward for Agent 12 this episode : -12.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.9207\n",
      "Average Reward for Agent 13 this episode : -44.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 164.0527\n",
      "Reducing exploration for all agents to 0.0436\n",
      "Episode 182 is finished\n",
      "Average Reward for Agent 0 this episode : -12.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.1458\n",
      "Average Reward for Agent 1 this episode : -13.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.0485\n",
      "Average Reward for Agent 2 this episode : -20.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.9069\n",
      "Average Reward for Agent 3 this episode : -1.28\n",
      "Saving architecture, weights, optimizer state for best agent-3\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent3_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.9345\n",
      "Average Reward for Agent 4 this episode : -18.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 127.6049\n",
      "Average Reward for Agent 5 this episode : -0.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1383\n",
      "Average Reward for Agent 6 this episode : -2.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.0773\n",
      "Average Reward for Agent 7 this episode : -13.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.9682\n",
      "Average Reward for Agent 8 this episode : -1.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1511\n",
      "Average Reward for Agent 9 this episode : -10.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.3617\n",
      "Average Reward for Agent 10 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 178.0217\n",
      "Average Reward for Agent 11 this episode : -3.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.7757\n",
      "Average Reward for Agent 12 this episode : -7.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.1453\n",
      "Average Reward for Agent 13 this episode : -36.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.4864\n",
      "Reducing exploration for all agents to 0.0428\n",
      "Episode 183 is finished\n",
      "Average Reward for Agent 0 this episode : -9.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.0546\n",
      "Average Reward for Agent 1 this episode : -17.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 237.4065\n",
      "Average Reward for Agent 2 this episode : -16.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 193.8459\n",
      "Average Reward for Agent 3 this episode : -3.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.7748\n",
      "Average Reward for Agent 4 this episode : -22.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.7573\n",
      "Average Reward for Agent 5 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3417\n",
      "Average Reward for Agent 6 this episode : -1.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.8884\n",
      "Average Reward for Agent 7 this episode : -9.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.1275\n",
      "Average Reward for Agent 8 this episode : -2.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1419\n",
      "Average Reward for Agent 9 this episode : -1.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.2886\n",
      "Average Reward for Agent 10 this episode : -48.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 452.7743\n",
      "Average Reward for Agent 11 this episode : -1.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 121.2428\n",
      "Average Reward for Agent 12 this episode : -20.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 123.7931\n",
      "Average Reward for Agent 13 this episode : -47.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.0860\n",
      "Reducing exploration for all agents to 0.0421\n",
      "Episode 184 is finished\n",
      "Average Reward for Agent 0 this episode : -12.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 188.1232\n",
      "Average Reward for Agent 1 this episode : -18.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 229.8662\n",
      "Average Reward for Agent 2 this episode : -20.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 223.0735\n",
      "Average Reward for Agent 3 this episode : -2.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.5786\n",
      "Average Reward for Agent 4 this episode : -19.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 356.1767\n",
      "Average Reward for Agent 5 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4587\n",
      "Average Reward for Agent 6 this episode : -3.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.2453\n",
      "Average Reward for Agent 7 this episode : -12.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.9759\n",
      "Average Reward for Agent 8 this episode : -6.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.5768\n",
      "Average Reward for Agent 9 this episode : -0.14\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 - 0s - loss: 75.1766\n",
      "Average Reward for Agent 10 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.5539\n",
      "Average Reward for Agent 11 this episode : -3.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.2287\n",
      "Average Reward for Agent 12 this episode : -14.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.5262\n",
      "Average Reward for Agent 13 this episode : -39.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.6699\n",
      "Reducing exploration for all agents to 0.0414\n",
      "Episode 185 is finished\n",
      "Average Reward for Agent 0 this episode : -9.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 186.2825\n",
      "Average Reward for Agent 1 this episode : -18.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 284.6110\n",
      "Average Reward for Agent 2 this episode : -19.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.5940\n",
      "Average Reward for Agent 3 this episode : -1.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.2095\n",
      "Average Reward for Agent 4 this episode : -18.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 232.1071\n",
      "Average Reward for Agent 5 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2541\n",
      "Average Reward for Agent 6 this episode : -4.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.7521\n",
      "Average Reward for Agent 7 this episode : -15.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.0017\n",
      "Average Reward for Agent 8 this episode : -9.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.9308\n",
      "Average Reward for Agent 9 this episode : -0.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.2459\n",
      "Average Reward for Agent 10 this episode : -5.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 128.7447\n",
      "Average Reward for Agent 11 this episode : -33.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.0894\n",
      "Average Reward for Agent 12 this episode : -14.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.5670\n",
      "Average Reward for Agent 13 this episode : -53.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.4496\n",
      "Reducing exploration for all agents to 0.0406\n",
      "Episode 186 is finished\n",
      "Average Reward for Agent 0 this episode : -9.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.2338\n",
      "Average Reward for Agent 1 this episode : -17.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 147.7139\n",
      "Average Reward for Agent 2 this episode : -17.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 188.7000\n",
      "Average Reward for Agent 3 this episode : -3.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.9786\n",
      "Average Reward for Agent 4 this episode : -22.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 191.5673\n",
      "Average Reward for Agent 5 this episode : -0.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0817\n",
      "Average Reward for Agent 6 this episode : -1.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.6007\n",
      "Average Reward for Agent 7 this episode : -14.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.2382\n",
      "Average Reward for Agent 8 this episode : -1.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.5557\n",
      "Average Reward for Agent 9 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.6275\n",
      "Average Reward for Agent 10 this episode : -54.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 132.7727\n",
      "Average Reward for Agent 11 this episode : -17.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.7776\n",
      "Average Reward for Agent 12 this episode : -13.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.9114\n",
      "Average Reward for Agent 13 this episode : -50.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 155.4625\n",
      "Reducing exploration for all agents to 0.0399\n",
      "Episode 187 is finished\n",
      "Average Reward for Agent 0 this episode : -8.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.8110\n",
      "Average Reward for Agent 1 this episode : -16.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 203.8381\n",
      "Average Reward for Agent 2 this episode : -21.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.6483\n",
      "Average Reward for Agent 3 this episode : -3.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.6874\n",
      "Average Reward for Agent 4 this episode : -17.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 200.7315\n",
      "Average Reward for Agent 5 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6087\n",
      "Average Reward for Agent 6 this episode : -2.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.5023\n",
      "Average Reward for Agent 7 this episode : -13.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.1409\n",
      "Average Reward for Agent 8 this episode : -3.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.3828\n",
      "Average Reward for Agent 9 this episode : -0.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.4850\n",
      "Average Reward for Agent 10 this episode : -37.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 168.9910\n",
      "Average Reward for Agent 11 this episode : -18.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.2744\n",
      "Average Reward for Agent 12 this episode : -17.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.1275\n",
      "Average Reward for Agent 13 this episode : -45.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.3501\n",
      "Reducing exploration for all agents to 0.0393\n",
      "Episode 188 is finished\n",
      "Average Reward for Agent 0 this episode : -9.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.3599\n",
      "Average Reward for Agent 1 this episode : -17.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 438.3466\n",
      "Average Reward for Agent 2 this episode : -17.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 266.2917\n",
      "Average Reward for Agent 3 this episode : -4.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.4537\n",
      "Average Reward for Agent 4 this episode : -15.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 177.0496\n",
      "Average Reward for Agent 5 this episode : -0.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6960\n",
      "Average Reward for Agent 6 this episode : -7.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.5467\n",
      "Average Reward for Agent 7 this episode : -13.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.2320\n",
      "Average Reward for Agent 8 this episode : -4.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.0283\n",
      "Average Reward for Agent 9 this episode : -1.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.7674\n",
      "Average Reward for Agent 10 this episode : -53.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.8791\n",
      "Average Reward for Agent 11 this episode : -12.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.3295\n",
      "Average Reward for Agent 12 this episode : -11.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.4191\n",
      "Average Reward for Agent 13 this episode : -42.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 116.9983\n",
      "Reducing exploration for all agents to 0.0386\n",
      "Episode 189 is finished\n",
      "Average Reward for Agent 0 this episode : -9.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.0488\n",
      "Average Reward for Agent 1 this episode : -22.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 216.1536\n",
      "Average Reward for Agent 2 this episode : -10.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.0192\n",
      "Average Reward for Agent 3 this episode : -1.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.8957\n",
      "Average Reward for Agent 4 this episode : -16.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 319.4866\n",
      "Average Reward for Agent 5 this episode : -0.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2176\n",
      "Average Reward for Agent 6 this episode : -2.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.7525\n",
      "Average Reward for Agent 7 this episode : -13.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.6582\n",
      "Average Reward for Agent 8 this episode : -7.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.0273\n",
      "Average Reward for Agent 9 this episode : -7.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.2055\n",
      "Average Reward for Agent 10 this episode : -39.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 183.7159\n",
      "Average Reward for Agent 11 this episode : -19.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.6058\n",
      "Average Reward for Agent 12 this episode : -22.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.5220\n",
      "Average Reward for Agent 13 this episode : -49.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.0130\n",
      "Reducing exploration for all agents to 0.0379\n",
      "Episode 190 is finished\n",
      "Average Reward for Agent 0 this episode : -12.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 100.4709\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -20.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 202.4282\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -22.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 150.4194\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -3.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.4112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -15.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 369.9913\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -1.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2704\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -5.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.0296\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -14.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.6075\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -9.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.7683\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -10.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.9615\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -48.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 141.2132\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -19.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.9763\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -10.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.4626\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -36.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.0409\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0373\n",
      "Episode 191 is finished\n",
      "Average Reward for Agent 0 this episode : -8.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.2402\n",
      "Average Reward for Agent 1 this episode : -20.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 198.0692\n",
      "Average Reward for Agent 2 this episode : -16.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 243.2850\n",
      "Average Reward for Agent 3 this episode : -1.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.3082\n",
      "Average Reward for Agent 4 this episode : -20.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 166.4149\n",
      "Average Reward for Agent 5 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6582\n",
      "Average Reward for Agent 6 this episode : -3.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.0355\n",
      "Average Reward for Agent 7 this episode : -14.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8433\n",
      "Average Reward for Agent 8 this episode : -3.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.7583\n",
      "Average Reward for Agent 9 this episode : -20.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.4851\n",
      "Average Reward for Agent 10 this episode : -45.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 600.6089\n",
      "Average Reward for Agent 11 this episode : -21.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 174.5336\n",
      "Average Reward for Agent 12 this episode : -13.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.5502\n",
      "Average Reward for Agent 13 this episode : -51.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 207.9772\n",
      "Reducing exploration for all agents to 0.0366\n",
      "Episode 192 is finished\n",
      "Average Reward for Agent 0 this episode : -10.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.1127\n",
      "Average Reward for Agent 1 this episode : -18.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 282.8097\n",
      "Average Reward for Agent 2 this episode : -19.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 174.9912\n",
      "Average Reward for Agent 3 this episode : -2.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.5560\n",
      "Average Reward for Agent 4 this episode : -19.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 166.3270\n",
      "Average Reward for Agent 5 this episode : -0.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0526\n",
      "Average Reward for Agent 6 this episode : -1.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.6952\n",
      "Average Reward for Agent 7 this episode : -14.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.6429\n",
      "Average Reward for Agent 8 this episode : -1.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.0464\n",
      "Average Reward for Agent 9 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.9368\n",
      "Average Reward for Agent 10 this episode : -42.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 132.5866\n",
      "Average Reward for Agent 11 this episode : -28.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.1865\n",
      "Average Reward for Agent 12 this episode : -13.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.6254\n",
      "Average Reward for Agent 13 this episode : -47.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.4222\n",
      "Reducing exploration for all agents to 0.036\n",
      "Episode 193 is finished\n",
      "Average Reward for Agent 0 this episode : -7.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.7420\n",
      "Average Reward for Agent 1 this episode : -19.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 299.9771\n",
      "Average Reward for Agent 2 this episode : -23.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.9153\n",
      "Average Reward for Agent 3 this episode : -5.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.4067\n",
      "Average Reward for Agent 4 this episode : -17.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 115.0959\n",
      "Average Reward for Agent 5 this episode : -1.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9772\n",
      "Average Reward for Agent 6 this episode : -6.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.7766\n",
      "Average Reward for Agent 7 this episode : -14.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.7042\n",
      "Average Reward for Agent 8 this episode : -1.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.4114\n",
      "Average Reward for Agent 9 this episode : -0.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.1068\n",
      "Average Reward for Agent 10 this episode : -43.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 337.5015\n",
      "Average Reward for Agent 11 this episode : -4.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.3803\n",
      "Average Reward for Agent 12 this episode : -18.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.8759\n",
      "Average Reward for Agent 13 this episode : -41.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 218.6205\n",
      "Reducing exploration for all agents to 0.0354\n",
      "Episode 194 is finished\n",
      "Average Reward for Agent 0 this episode : -8.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.9746\n",
      "Average Reward for Agent 1 this episode : -25.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 326.2140\n",
      "Average Reward for Agent 2 this episode : -20.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.3810\n",
      "Average Reward for Agent 3 this episode : -8.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.8716\n",
      "Average Reward for Agent 4 this episode : -15.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 157.1734\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3307\n",
      "Average Reward for Agent 6 this episode : -1.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.1433\n",
      "Average Reward for Agent 7 this episode : -14.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.3888\n",
      "Average Reward for Agent 8 this episode : -1.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4961\n",
      "Average Reward for Agent 9 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.8221\n",
      "Average Reward for Agent 10 this episode : -28.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 332.0683\n",
      "Average Reward for Agent 11 this episode : -2.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.9857\n",
      "Average Reward for Agent 12 this episode : -10.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.5086\n",
      "Average Reward for Agent 13 this episode : -56.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.9008\n",
      "Reducing exploration for all agents to 0.0348\n",
      "Episode 195 is finished\n",
      "Average Reward for Agent 0 this episode : -8.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.2451\n",
      "Average Reward for Agent 1 this episode : -26.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 139.2747\n",
      "Average Reward for Agent 2 this episode : -18.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.7087\n",
      "Average Reward for Agent 3 this episode : -3.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.3292\n",
      "Average Reward for Agent 4 this episode : -18.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 263.9111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 5 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1338\n",
      "Average Reward for Agent 6 this episode : -0.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.7051\n",
      "Average Reward for Agent 7 this episode : -15.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.7706\n",
      "Average Reward for Agent 8 this episode : -1.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4524\n",
      "Average Reward for Agent 9 this episode : -0.11\n",
      "Saving architecture, weights, optimizer state for best agent-9\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent9_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.5597\n",
      "Average Reward for Agent 10 this episode : -54.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.2034\n",
      "Average Reward for Agent 11 this episode : -3.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.3981\n",
      "Average Reward for Agent 12 this episode : -16.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.5788\n",
      "Average Reward for Agent 13 this episode : -39.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 218.4546\n",
      "Reducing exploration for all agents to 0.0342\n",
      "Episode 196 is finished\n",
      "Average Reward for Agent 0 this episode : -9.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.0154\n",
      "Average Reward for Agent 1 this episode : -30.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 157.4891\n",
      "Average Reward for Agent 2 this episode : -24.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 153.2927\n",
      "Average Reward for Agent 3 this episode : -1.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.0668\n",
      "Average Reward for Agent 4 this episode : -19.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 208.5209\n",
      "Average Reward for Agent 5 this episode : -0.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0550\n",
      "Average Reward for Agent 6 this episode : -15.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.8076\n",
      "Average Reward for Agent 7 this episode : -15.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.2153\n",
      "Average Reward for Agent 8 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.0105\n",
      "Average Reward for Agent 9 this episode : -0.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.1399\n",
      "Average Reward for Agent 10 this episode : -43.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 254.2433\n",
      "Average Reward for Agent 11 this episode : -8.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.6420\n",
      "Average Reward for Agent 12 this episode : -11.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.8151\n",
      "Average Reward for Agent 13 this episode : -19.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.4571\n",
      "Reducing exploration for all agents to 0.0336\n",
      "Episode 197 is finished\n",
      "Average Reward for Agent 0 this episode : -8.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.2491\n",
      "Average Reward for Agent 1 this episode : -33.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 197.6069\n",
      "Average Reward for Agent 2 this episode : -22.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 213.1070\n",
      "Average Reward for Agent 3 this episode : -3.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.2272\n",
      "Average Reward for Agent 4 this episode : -17.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 339.9320\n",
      "Average Reward for Agent 5 this episode : -0.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0334\n",
      "Average Reward for Agent 6 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.3601\n",
      "Average Reward for Agent 7 this episode : -13.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.1469\n",
      "Average Reward for Agent 8 this episode : -1.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6792\n",
      "Average Reward for Agent 9 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.7569\n",
      "Average Reward for Agent 10 this episode : -56.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 222.1094\n",
      "Average Reward for Agent 11 this episode : -20.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.4196\n",
      "Average Reward for Agent 12 this episode : -11.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.9279\n",
      "Average Reward for Agent 13 this episode : -7.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.7316\n",
      "Reducing exploration for all agents to 0.033\n",
      "Episode 198 is finished\n",
      "Average Reward for Agent 0 this episode : -7.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.5769\n",
      "Average Reward for Agent 1 this episode : -29.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 193.9994\n",
      "Average Reward for Agent 2 this episode : -19.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 200.5036\n",
      "Average Reward for Agent 3 this episode : -1.17\n",
      "Saving architecture, weights, optimizer state for best agent-3\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent3_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.5915\n",
      "Average Reward for Agent 4 this episode : -21.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 198.1483\n",
      "Average Reward for Agent 5 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.0874\n",
      "Average Reward for Agent 6 this episode : -1.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.6293\n",
      "Average Reward for Agent 7 this episode : -13.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 116.3109\n",
      "Average Reward for Agent 8 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.8997\n",
      "Average Reward for Agent 9 this episode : -3.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.5080\n",
      "Average Reward for Agent 10 this episode : -40.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.8186\n",
      "Average Reward for Agent 11 this episode : -25.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 162.3805\n",
      "Average Reward for Agent 12 this episode : -20.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.7822\n",
      "Average Reward for Agent 13 this episode : -3.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.2167\n",
      "Reducing exploration for all agents to 0.0325\n",
      "Episode 199 is finished\n",
      "Average Reward for Agent 0 this episode : -7.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.4353\n",
      "Average Reward for Agent 1 this episode : -29.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 205.6276\n",
      "Average Reward for Agent 2 this episode : -25.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.1098\n",
      "Average Reward for Agent 3 this episode : -1.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.7613\n",
      "Average Reward for Agent 4 this episode : -17.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.6031\n",
      "Average Reward for Agent 5 this episode : -0.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6524\n",
      "Average Reward for Agent 6 this episode : -4.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.8437\n",
      "Average Reward for Agent 7 this episode : -15.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.4555\n",
      "Average Reward for Agent 8 this episode : -1.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7581\n",
      "Average Reward for Agent 9 this episode : -11.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.9821\n",
      "Average Reward for Agent 10 this episode : -39.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 282.9647\n",
      "Average Reward for Agent 11 this episode : -18.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 137.5149\n",
      "Average Reward for Agent 12 this episode : -13.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.1892\n",
      "Average Reward for Agent 13 this episode : -7.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.0197\n",
      "Reducing exploration for all agents to 0.0319\n",
      "Episode 200 is finished\n",
      "Average Reward for Agent 0 this episode : -7.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.7307\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -26.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.8164\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -23.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 166.3925\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -1.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.8641\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -17.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.0197\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -2.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.4379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -27.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.1873\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -14.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.7673\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.4721\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -3.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.0192\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -33.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 239.8092\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -5.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.6001\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -14.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.1901\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -14.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.4604\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0314\n",
      "Episode 201 is finished\n",
      "Average Reward for Agent 0 this episode : -9.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.5785\n",
      "Average Reward for Agent 1 this episode : -24.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 328.2210\n",
      "Average Reward for Agent 2 this episode : -24.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.3140\n",
      "Average Reward for Agent 3 this episode : -0.69\n",
      "Saving architecture, weights, optimizer state for best agent-3\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent3_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.9839\n",
      "Average Reward for Agent 4 this episode : -21.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.8764\n",
      "Average Reward for Agent 5 this episode : -0.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9902\n",
      "Average Reward for Agent 6 this episode : -37.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 201.6131\n",
      "Average Reward for Agent 7 this episode : -14.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.9738\n",
      "Average Reward for Agent 8 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.9265\n",
      "Average Reward for Agent 9 this episode : -6.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.0773\n",
      "Average Reward for Agent 10 this episode : -47.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 497.6796\n",
      "Average Reward for Agent 11 this episode : -1.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.7529\n",
      "Average Reward for Agent 12 this episode : -5.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.3395\n",
      "Average Reward for Agent 13 this episode : -8.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.2264\n",
      "Reducing exploration for all agents to 0.0308\n",
      "Episode 202 is finished\n",
      "Average Reward for Agent 0 this episode : -8.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.4280\n",
      "Average Reward for Agent 1 this episode : -21.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.9818\n",
      "Average Reward for Agent 2 this episode : -21.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.3506\n",
      "Average Reward for Agent 3 this episode : -3.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.0727\n",
      "Average Reward for Agent 4 this episode : -24.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 248.9380\n",
      "Average Reward for Agent 5 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3716\n",
      "Average Reward for Agent 6 this episode : -2.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.5725\n",
      "Average Reward for Agent 7 this episode : -14.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.6996\n",
      "Average Reward for Agent 8 this episode : -1.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9311\n",
      "Average Reward for Agent 9 this episode : -0.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.9351\n",
      "Average Reward for Agent 10 this episode : -62.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 497.1552\n",
      "Average Reward for Agent 11 this episode : -0.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.1212\n",
      "Average Reward for Agent 12 this episode : -12.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.9263\n",
      "Average Reward for Agent 13 this episode : -10.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 121.0965\n",
      "Reducing exploration for all agents to 0.0303\n",
      "Episode 203 is finished\n",
      "Average Reward for Agent 0 this episode : -7.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.7880\n",
      "Average Reward for Agent 1 this episode : -22.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 249.5484\n",
      "Average Reward for Agent 2 this episode : -25.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.7581\n",
      "Average Reward for Agent 3 this episode : -1.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.2493\n",
      "Average Reward for Agent 4 this episode : -20.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 121.1165\n",
      "Average Reward for Agent 5 this episode : -1.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6281\n",
      "Average Reward for Agent 6 this episode : -1.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 145.0807\n",
      "Average Reward for Agent 7 this episode : -14.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.5004\n",
      "Average Reward for Agent 8 this episode : -0.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7776\n",
      "Average Reward for Agent 9 this episode : -0.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.0211\n",
      "Average Reward for Agent 10 this episode : -45.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.9570\n",
      "Average Reward for Agent 11 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.8388\n",
      "Average Reward for Agent 12 this episode : -20.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.3156\n",
      "Average Reward for Agent 13 this episode : -30.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.2917\n",
      "Reducing exploration for all agents to 0.0298\n",
      "Episode 204 is finished\n",
      "Average Reward for Agent 0 this episode : -9.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.4291\n",
      "Average Reward for Agent 1 this episode : -19.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 219.7881\n",
      "Average Reward for Agent 2 this episode : -23.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 124.3589\n",
      "Average Reward for Agent 3 this episode : -2.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.1701\n",
      "Average Reward for Agent 4 this episode : -21.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.7247\n",
      "Average Reward for Agent 5 this episode : -2.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3481\n",
      "Average Reward for Agent 6 this episode : -0.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.5592\n",
      "Average Reward for Agent 7 this episode : -15.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.6791\n",
      "Average Reward for Agent 8 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6915\n",
      "Average Reward for Agent 9 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.1864\n",
      "Average Reward for Agent 10 this episode : -17.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 376.8611\n",
      "Average Reward for Agent 11 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.6546\n",
      "Average Reward for Agent 12 this episode : -17.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.9122\n",
      "Average Reward for Agent 13 this episode : -14.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.4722\n",
      "Reducing exploration for all agents to 0.0293\n",
      "Episode 205 is finished\n",
      "Average Reward for Agent 0 this episode : -9.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.8634\n",
      "Average Reward for Agent 1 this episode : -20.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 276.3975\n",
      "Average Reward for Agent 2 this episode : -29.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.7956\n",
      "Average Reward for Agent 3 this episode : -1.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.1198\n",
      "Average Reward for Agent 4 this episode : -17.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 239.8712\n",
      "Average Reward for Agent 5 this episode : -0.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4281\n",
      "Average Reward for Agent 6 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.0710\n",
      "Average Reward for Agent 7 this episode : -15.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.4073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 8 this episode : -1.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7848\n",
      "Average Reward for Agent 9 this episode : -1.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.0328\n",
      "Average Reward for Agent 10 this episode : -18.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 328.4584\n",
      "Average Reward for Agent 11 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.4008\n",
      "Average Reward for Agent 12 this episode : -16.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.4900\n",
      "Average Reward for Agent 13 this episode : -12.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.7040\n",
      "Reducing exploration for all agents to 0.0288\n",
      "Episode 206 is finished\n",
      "Average Reward for Agent 0 this episode : -9.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.6796\n",
      "Average Reward for Agent 1 this episode : -17.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 231.6143\n",
      "Average Reward for Agent 2 this episode : -27.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 156.0871\n",
      "Average Reward for Agent 3 this episode : -2.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.8716\n",
      "Average Reward for Agent 4 this episode : -20.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 317.0437\n",
      "Average Reward for Agent 5 this episode : -5.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.8524\n",
      "Average Reward for Agent 6 this episode : -1.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.7328\n",
      "Average Reward for Agent 7 this episode : -13.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.4099\n",
      "Average Reward for Agent 8 this episode : -0.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9870\n",
      "Average Reward for Agent 9 this episode : -5.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.3168\n",
      "Average Reward for Agent 10 this episode : -60.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 180.4465\n",
      "Average Reward for Agent 11 this episode : -15.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.0482\n",
      "Average Reward for Agent 12 this episode : -29.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.2763\n",
      "Average Reward for Agent 13 this episode : -10.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.0079\n",
      "Reducing exploration for all agents to 0.0283\n",
      "Episode 207 is finished\n",
      "Average Reward for Agent 0 this episode : -8.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.1160\n",
      "Average Reward for Agent 1 this episode : -18.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 267.8620\n",
      "Average Reward for Agent 2 this episode : -33.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.5467\n",
      "Average Reward for Agent 3 this episode : -1.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.2501\n",
      "Average Reward for Agent 4 this episode : -18.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 343.6183\n",
      "Average Reward for Agent 5 this episode : -0.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1494\n",
      "Average Reward for Agent 6 this episode : -1.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.1000\n",
      "Average Reward for Agent 7 this episode : -15.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 126.4214\n",
      "Average Reward for Agent 8 this episode : -0.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4874\n",
      "Average Reward for Agent 9 this episode : -13.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.5724\n",
      "Average Reward for Agent 10 this episode : -45.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 148.0117\n",
      "Average Reward for Agent 11 this episode : -43.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 144.4942\n",
      "Average Reward for Agent 12 this episode : -12.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.2221\n",
      "Average Reward for Agent 13 this episode : -19.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.2152\n",
      "Reducing exploration for all agents to 0.0278\n",
      "Episode 208 is finished\n",
      "Average Reward for Agent 0 this episode : -8.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.9800\n",
      "Average Reward for Agent 1 this episode : -19.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 177.5907\n",
      "Average Reward for Agent 2 this episode : -53.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 192.3902\n",
      "Average Reward for Agent 3 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.8203\n",
      "Average Reward for Agent 4 this episode : -17.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.3634\n",
      "Average Reward for Agent 5 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9319\n",
      "Average Reward for Agent 6 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.5484\n",
      "Average Reward for Agent 7 this episode : -14.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 122.9543\n",
      "Average Reward for Agent 8 this episode : -0.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.4865\n",
      "Average Reward for Agent 9 this episode : -29.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8119\n",
      "Average Reward for Agent 10 this episode : -42.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 390.7974\n",
      "Average Reward for Agent 11 this episode : -26.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.0507\n",
      "Average Reward for Agent 12 this episode : -8.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.9102\n",
      "Average Reward for Agent 13 this episode : -2.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.9931\n",
      "Reducing exploration for all agents to 0.0273\n",
      "Episode 209 is finished\n",
      "Average Reward for Agent 0 this episode : -8.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.0315\n",
      "Average Reward for Agent 1 this episode : -16.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.1788\n",
      "Average Reward for Agent 2 this episode : -40.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 234.4191\n",
      "Average Reward for Agent 3 this episode : -0.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.2860\n",
      "Average Reward for Agent 4 this episode : -22.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 438.7581\n",
      "Average Reward for Agent 5 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8046\n",
      "Average Reward for Agent 6 this episode : -0.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.3046\n",
      "Average Reward for Agent 7 this episode : -14.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.3452\n",
      "Average Reward for Agent 8 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5986\n",
      "Average Reward for Agent 9 this episode : -22.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.1398\n",
      "Average Reward for Agent 10 this episode : -51.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 349.6345\n",
      "Average Reward for Agent 11 this episode : -40.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 238.3976\n",
      "Average Reward for Agent 12 this episode : -18.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.0728\n",
      "Average Reward for Agent 13 this episode : -3.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.9063\n",
      "Reducing exploration for all agents to 0.0268\n",
      "Episode 210 is finished\n",
      "Average Reward for Agent 0 this episode : -8.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.4830\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -15.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.4792\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -44.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 296.6857\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -6.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.2729\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -27.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.2996\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3970\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.6397\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -13.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.1472\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5753\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -19.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.2719\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -52.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.8253\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -46.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.9057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -12.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.4574\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -7.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.6389\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0264\n",
      "Episode 211 is finished\n",
      "Average Reward for Agent 0 this episode : -12.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.4805\n",
      "Average Reward for Agent 1 this episode : -13.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.5249\n",
      "Average Reward for Agent 2 this episode : -54.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 360.2963\n",
      "Average Reward for Agent 3 this episode : -11.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.5575\n",
      "Average Reward for Agent 4 this episode : -18.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 140.2484\n",
      "Average Reward for Agent 5 this episode : -1.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6361\n",
      "Average Reward for Agent 6 this episode : -0.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.0094\n",
      "Average Reward for Agent 7 this episode : -13.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.0870\n",
      "Average Reward for Agent 8 this episode : -5.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4561\n",
      "Average Reward for Agent 9 this episode : -20.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 210.6801\n",
      "Average Reward for Agent 10 this episode : -37.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 567.5945\n",
      "Average Reward for Agent 11 this episode : -35.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 578.3784\n",
      "Average Reward for Agent 12 this episode : -11.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.9668\n",
      "Average Reward for Agent 13 this episode : -14.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.8337\n",
      "Reducing exploration for all agents to 0.0259\n",
      "Episode 212 is finished\n",
      "Average Reward for Agent 0 this episode : -8.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.1324\n",
      "Average Reward for Agent 1 this episode : -22.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.2485\n",
      "Average Reward for Agent 2 this episode : -45.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.6408\n",
      "Average Reward for Agent 3 this episode : -6.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.3756\n",
      "Average Reward for Agent 4 this episode : -19.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 265.2144\n",
      "Average Reward for Agent 5 this episode : -2.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3772\n",
      "Average Reward for Agent 6 this episode : -0.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.7560\n",
      "Average Reward for Agent 7 this episode : -13.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.3211\n",
      "Average Reward for Agent 8 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3541\n",
      "Average Reward for Agent 9 this episode : -1.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.2844\n",
      "Average Reward for Agent 10 this episode : -7.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 148.1936\n",
      "Average Reward for Agent 11 this episode : -30.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 309.2182\n",
      "Average Reward for Agent 12 this episode : -8.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.2020\n",
      "Average Reward for Agent 13 this episode : -8.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.4894\n",
      "Reducing exploration for all agents to 0.0255\n",
      "Episode 213 is finished\n",
      "Average Reward for Agent 0 this episode : -9.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.2307\n",
      "Average Reward for Agent 1 this episode : -15.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 250.2594\n",
      "Average Reward for Agent 2 this episode : -54.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 311.6386\n",
      "Average Reward for Agent 3 this episode : -1.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.1216\n",
      "Average Reward for Agent 4 this episode : -23.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 287.5095\n",
      "Average Reward for Agent 5 this episode : -1.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3277\n",
      "Average Reward for Agent 6 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.1126\n",
      "Average Reward for Agent 7 this episode : -8.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.2936\n",
      "Average Reward for Agent 8 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3063\n",
      "Average Reward for Agent 9 this episode : -0.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 246.3578\n",
      "Average Reward for Agent 10 this episode : -1.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 731.6037\n",
      "Average Reward for Agent 11 this episode : -7.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 606.0239\n",
      "Average Reward for Agent 12 this episode : -7.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.4570\n",
      "Average Reward for Agent 13 this episode : -9.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.0640\n",
      "Reducing exploration for all agents to 0.025\n",
      "Episode 214 is finished\n",
      "Average Reward for Agent 0 this episode : -9.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.8590\n",
      "Average Reward for Agent 1 this episode : -16.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 282.7177\n",
      "Average Reward for Agent 2 this episode : -42.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.0513\n",
      "Average Reward for Agent 3 this episode : -0.46\n",
      "Saving architecture, weights, optimizer state for best agent-3\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent3_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.1878\n",
      "Average Reward for Agent 4 this episode : -17.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.7453\n",
      "Average Reward for Agent 5 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.2440\n",
      "Average Reward for Agent 6 this episode : -1.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1506\n",
      "Average Reward for Agent 7 this episode : -12.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3945\n",
      "Average Reward for Agent 8 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.0969\n",
      "Average Reward for Agent 9 this episode : -1.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.2867\n",
      "Average Reward for Agent 10 this episode : -1.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 452.5707\n",
      "Average Reward for Agent 11 this episode : -50.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.9104\n",
      "Average Reward for Agent 12 this episode : -6.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.3812\n",
      "Average Reward for Agent 13 this episode : -11.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.8837\n",
      "Reducing exploration for all agents to 0.0246\n",
      "Episode 215 is finished\n",
      "Average Reward for Agent 0 this episode : -9.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.9286\n",
      "Average Reward for Agent 1 this episode : -18.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.0690\n",
      "Average Reward for Agent 2 this episode : -43.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 237.6936\n",
      "Average Reward for Agent 3 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.0098\n",
      "Average Reward for Agent 4 this episode : -19.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.3274\n",
      "Average Reward for Agent 5 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1212\n",
      "Average Reward for Agent 6 this episode : -0.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.5721\n",
      "Average Reward for Agent 7 this episode : -12.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.9413\n",
      "Average Reward for Agent 8 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6365\n",
      "Average Reward for Agent 9 this episode : -43.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.8349\n",
      "Average Reward for Agent 10 this episode : -2.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 316.0307\n",
      "Average Reward for Agent 11 this episode : -22.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 263.5587\n",
      "Average Reward for Agent 12 this episode : -12.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.8011\n",
      "Average Reward for Agent 13 this episode : -18.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.6140\n",
      "Reducing exploration for all agents to 0.0242\n",
      "Episode 216 is finished\n",
      "Average Reward for Agent 0 this episode : -12.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.7496\n",
      "Average Reward for Agent 1 this episode : -16.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.3491\n",
      "Average Reward for Agent 2 this episode : -34.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 191.7955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 3 this episode : -5.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0268\n",
      "Average Reward for Agent 4 this episode : -17.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.9781\n",
      "Average Reward for Agent 5 this episode : -1.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0464\n",
      "Average Reward for Agent 6 this episode : -0.31\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2898\n",
      "Average Reward for Agent 7 this episode : -12.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.9782\n",
      "Average Reward for Agent 8 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6114\n",
      "Average Reward for Agent 9 this episode : -25.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 150.4204\n",
      "Average Reward for Agent 10 this episode : -6.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 180.7321\n",
      "Average Reward for Agent 11 this episode : -33.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 366.1126\n",
      "Average Reward for Agent 12 this episode : -10.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.7096\n",
      "Average Reward for Agent 13 this episode : -6.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.2319\n",
      "Reducing exploration for all agents to 0.0238\n",
      "Episode 217 is finished\n",
      "Average Reward for Agent 0 this episode : -8.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.9965\n",
      "Average Reward for Agent 1 this episode : -16.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 386.7484\n",
      "Average Reward for Agent 2 this episode : -42.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 139.1961\n",
      "Average Reward for Agent 3 this episode : -5.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.7370\n",
      "Average Reward for Agent 4 this episode : -20.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 243.1753\n",
      "Average Reward for Agent 5 this episode : -1.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1831\n",
      "Average Reward for Agent 6 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.8814\n",
      "Average Reward for Agent 7 this episode : -12.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.3519\n",
      "Average Reward for Agent 8 this episode : -0.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.9209\n",
      "Average Reward for Agent 9 this episode : -41.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.6334\n",
      "Average Reward for Agent 10 this episode : -38.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 163.7073\n",
      "Average Reward for Agent 11 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 136.7324\n",
      "Average Reward for Agent 12 this episode : -12.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.7112\n",
      "Average Reward for Agent 13 this episode : -19.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.8023\n",
      "Reducing exploration for all agents to 0.0234\n",
      "Episode 218 is finished\n",
      "Average Reward for Agent 0 this episode : -9.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.8277\n",
      "Average Reward for Agent 1 this episode : -16.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 169.2017\n",
      "Average Reward for Agent 2 this episode : -45.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.2338\n",
      "Average Reward for Agent 3 this episode : -7.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.1145\n",
      "Average Reward for Agent 4 this episode : -21.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.9596\n",
      "Average Reward for Agent 5 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1018\n",
      "Average Reward for Agent 6 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3274\n",
      "Average Reward for Agent 7 this episode : -12.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.9892\n",
      "Average Reward for Agent 8 this episode : -1.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0860\n",
      "Average Reward for Agent 9 this episode : -10.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 224.4409\n",
      "Average Reward for Agent 10 this episode : -39.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.2445\n",
      "Average Reward for Agent 11 this episode : -42.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.0090\n",
      "Average Reward for Agent 12 this episode : -6.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.3180\n",
      "Average Reward for Agent 13 this episode : -3.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.4501\n",
      "Reducing exploration for all agents to 0.023\n",
      "Episode 219 is finished\n",
      "Average Reward for Agent 0 this episode : -10.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.6308\n",
      "Average Reward for Agent 1 this episode : -14.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 168.7032\n",
      "Average Reward for Agent 2 this episode : -47.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.6621\n",
      "Average Reward for Agent 3 this episode : -2.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.0930\n",
      "Average Reward for Agent 4 this episode : -19.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.6625\n",
      "Average Reward for Agent 5 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6572\n",
      "Average Reward for Agent 6 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4155\n",
      "Average Reward for Agent 7 this episode : -13.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.3048\n",
      "Average Reward for Agent 8 this episode : -1.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0944\n",
      "Average Reward for Agent 9 this episode : -4.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 123.5535\n",
      "Average Reward for Agent 10 this episode : -42.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 273.2279\n",
      "Average Reward for Agent 11 this episode : -15.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 199.0097\n",
      "Average Reward for Agent 12 this episode : -10.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.6132\n",
      "Average Reward for Agent 13 this episode : -1.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.5629\n",
      "Reducing exploration for all agents to 0.0226\n",
      "Episode 220 is finished\n",
      "Average Reward for Agent 0 this episode : -8.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.3520\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -15.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 173.6096\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -48.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 375.0239\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.1429\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -23.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.9559\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -2.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6491\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7207\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -14.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.0614\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.3107\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -47.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.1671\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -57.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 324.6512\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.7221\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -10.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.9982\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -11.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.2555\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0222\n",
      "Episode 221 is finished\n",
      "Average Reward for Agent 0 this episode : -9.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.9696\n",
      "Average Reward for Agent 1 this episode : -15.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.6333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 2 this episode : -47.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 341.5912\n",
      "Average Reward for Agent 3 this episode : -5.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.3930\n",
      "Average Reward for Agent 4 this episode : -19.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 250.0712\n",
      "Average Reward for Agent 5 this episode : -0.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7918\n",
      "Average Reward for Agent 6 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3452\n",
      "Average Reward for Agent 7 this episode : -12.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.1502\n",
      "Average Reward for Agent 8 this episode : -1.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3327\n",
      "Average Reward for Agent 9 this episode : -42.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 413.9709\n",
      "Average Reward for Agent 10 this episode : -48.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 508.8001\n",
      "Average Reward for Agent 11 this episode : -0.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 236.2827\n",
      "Average Reward for Agent 12 this episode : -11.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.2113\n",
      "Average Reward for Agent 13 this episode : -15.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.8890\n",
      "Reducing exploration for all agents to 0.0218\n",
      "Episode 222 is finished\n",
      "Average Reward for Agent 0 this episode : -10.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.9360\n",
      "Average Reward for Agent 1 this episode : -16.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 184.8279\n",
      "Average Reward for Agent 2 this episode : -53.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 148.0067\n",
      "Average Reward for Agent 3 this episode : -3.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.3300\n",
      "Average Reward for Agent 4 this episode : -20.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 174.7631\n",
      "Average Reward for Agent 5 this episode : -0.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3830\n",
      "Average Reward for Agent 6 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3852\n",
      "Average Reward for Agent 7 this episode : -13.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.4374\n",
      "Average Reward for Agent 8 this episode : -2.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8540\n",
      "Average Reward for Agent 9 this episode : -13.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 100.3306\n",
      "Average Reward for Agent 10 this episode : -1.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 158.6933\n",
      "Average Reward for Agent 11 this episode : -1.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 156.7959\n",
      "Average Reward for Agent 12 this episode : -8.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.5024\n",
      "Average Reward for Agent 13 this episode : -10.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.5245\n",
      "Reducing exploration for all agents to 0.0214\n",
      "Episode 223 is finished\n",
      "Average Reward for Agent 0 this episode : -9.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.9261\n",
      "Average Reward for Agent 1 this episode : -17.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.9800\n",
      "Average Reward for Agent 2 this episode : -39.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 204.0556\n",
      "Average Reward for Agent 3 this episode : -1.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.4907\n",
      "Average Reward for Agent 4 this episode : -26.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.0822\n",
      "Average Reward for Agent 5 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8585\n",
      "Average Reward for Agent 6 this episode : -1.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3525\n",
      "Average Reward for Agent 7 this episode : -14.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.0769\n",
      "Average Reward for Agent 8 this episode : -1.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7422\n",
      "Average Reward for Agent 9 this episode : -0.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 271.1955\n",
      "Average Reward for Agent 10 this episode : -0.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 709.0652\n",
      "Average Reward for Agent 11 this episode : -4.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.3164\n",
      "Average Reward for Agent 12 this episode : -10.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.7207\n",
      "Average Reward for Agent 13 this episode : -6.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.6749\n",
      "Reducing exploration for all agents to 0.0211\n",
      "Episode 224 is finished\n",
      "Average Reward for Agent 0 this episode : -8.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.3645\n",
      "Average Reward for Agent 1 this episode : -17.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.2592\n",
      "Average Reward for Agent 2 this episode : -50.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 257.9243\n",
      "Average Reward for Agent 3 this episode : -5.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.3853\n",
      "Average Reward for Agent 4 this episode : -21.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 154.5783\n",
      "Average Reward for Agent 5 this episode : -1.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7390\n",
      "Average Reward for Agent 6 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5352\n",
      "Average Reward for Agent 7 this episode : -12.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.1352\n",
      "Average Reward for Agent 8 this episode : -3.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.2174\n",
      "Average Reward for Agent 9 this episode : -0.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 180.8435\n",
      "Average Reward for Agent 10 this episode : -4.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 341.3053\n",
      "Average Reward for Agent 11 this episode : -38.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 137.4032\n",
      "Average Reward for Agent 12 this episode : -5.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.6903\n",
      "Average Reward for Agent 13 this episode : -7.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 153.5197\n",
      "Reducing exploration for all agents to 0.0207\n",
      "Episode 225 is finished\n",
      "Average Reward for Agent 0 this episode : -9.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.8483\n",
      "Average Reward for Agent 1 this episode : -16.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 133.3269\n",
      "Average Reward for Agent 2 this episode : -51.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 148.3819\n",
      "Average Reward for Agent 3 this episode : -3.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.5124\n",
      "Average Reward for Agent 4 this episode : -19.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 181.6111\n",
      "Average Reward for Agent 5 this episode : -0.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0802\n",
      "Average Reward for Agent 6 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3447\n",
      "Average Reward for Agent 7 this episode : -13.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.3357\n",
      "Average Reward for Agent 8 this episode : -3.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3345\n",
      "Average Reward for Agent 9 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 89.2295\n",
      "Average Reward for Agent 10 this episode : -1.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.6605\n",
      "Average Reward for Agent 11 this episode : -10.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 216.8302\n",
      "Average Reward for Agent 12 this episode : -10.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.8857\n",
      "Average Reward for Agent 13 this episode : -10.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.6763\n",
      "Reducing exploration for all agents to 0.0203\n",
      "Episode 226 is finished\n",
      "Average Reward for Agent 0 this episode : -9.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.9080\n",
      "Average Reward for Agent 1 this episode : -13.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 139.9587\n",
      "Average Reward for Agent 2 this episode : -35.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.2992\n",
      "Average Reward for Agent 3 this episode : -9.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.7952\n",
      "Average Reward for Agent 4 this episode : -19.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 121.8338\n",
      "Average Reward for Agent 5 this episode : -0.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8610\n",
      "Average Reward for Agent 6 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.8020\n",
      "Average Reward for Agent 7 this episode : -13.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.0447\n",
      "Average Reward for Agent 8 this episode : -3.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4045\n",
      "Average Reward for Agent 9 this episode : -43.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.7776\n",
      "Average Reward for Agent 10 this episode : -24.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.1628\n",
      "Average Reward for Agent 11 this episode : -19.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.7305\n",
      "Average Reward for Agent 12 this episode : -8.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.3936\n",
      "Average Reward for Agent 13 this episode : -18.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.1558\n",
      "Reducing exploration for all agents to 0.02\n",
      "Episode 227 is finished\n",
      "Average Reward for Agent 0 this episode : -8.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.0198\n",
      "Average Reward for Agent 1 this episode : -14.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.7747\n",
      "Average Reward for Agent 2 this episode : -53.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 217.7529\n",
      "Average Reward for Agent 3 this episode : -2.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7461\n",
      "Average Reward for Agent 4 this episode : -17.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 129.3967\n",
      "Average Reward for Agent 5 this episode : -1.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8694\n",
      "Average Reward for Agent 6 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3520\n",
      "Average Reward for Agent 7 this episode : -13.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.2383\n",
      "Average Reward for Agent 8 this episode : -2.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6148\n",
      "Average Reward for Agent 9 this episode : -45.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 220.6613\n",
      "Average Reward for Agent 10 this episode : -50.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 174.4401\n",
      "Average Reward for Agent 11 this episode : -22.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.7281\n",
      "Average Reward for Agent 12 this episode : -9.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.1792\n",
      "Average Reward for Agent 13 this episode : -8.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.0577\n",
      "Reducing exploration for all agents to 0.0196\n",
      "Episode 228 is finished\n",
      "Average Reward for Agent 0 this episode : -8.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.9248\n",
      "Average Reward for Agent 1 this episode : -14.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.0163\n",
      "Average Reward for Agent 2 this episode : -55.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 238.5117\n",
      "Average Reward for Agent 3 this episode : -1.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.8996\n",
      "Average Reward for Agent 4 this episode : -21.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 182.6252\n",
      "Average Reward for Agent 5 this episode : -1.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3484\n",
      "Average Reward for Agent 6 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.4471\n",
      "Average Reward for Agent 7 this episode : -9.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.7846\n",
      "Average Reward for Agent 8 this episode : -1.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.6478\n",
      "Average Reward for Agent 9 this episode : -3.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.0130\n",
      "Average Reward for Agent 10 this episode : -49.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 536.6758\n",
      "Average Reward for Agent 11 this episode : -23.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.8239\n",
      "Average Reward for Agent 12 this episode : -6.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.1103\n",
      "Average Reward for Agent 13 this episode : -15.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 161.9998\n",
      "Reducing exploration for all agents to 0.0193\n",
      "Episode 229 is finished\n",
      "Average Reward for Agent 0 this episode : -8.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.8231\n",
      "Average Reward for Agent 1 this episode : -15.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.8931\n",
      "Average Reward for Agent 2 this episode : -47.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 164.1267\n",
      "Average Reward for Agent 3 this episode : -1.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.0633\n",
      "Average Reward for Agent 4 this episode : -17.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 174.3510\n",
      "Average Reward for Agent 5 this episode : -1.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6840\n",
      "Average Reward for Agent 6 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.0070\n",
      "Average Reward for Agent 7 this episode : -15.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1958\n",
      "Average Reward for Agent 8 this episode : -1.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5012\n",
      "Average Reward for Agent 9 this episode : -0.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.5633\n",
      "Average Reward for Agent 10 this episode : -49.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 371.9687\n",
      "Average Reward for Agent 11 this episode : -7.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.4236\n",
      "Average Reward for Agent 12 this episode : -8.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.0309\n",
      "Average Reward for Agent 13 this episode : -55.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 132.1363\n",
      "Reducing exploration for all agents to 0.019\n",
      "Episode 230 is finished\n",
      "Average Reward for Agent 0 this episode : -9.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.6494\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -10.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 172.7215\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -48.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 229.1872\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -6.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.2266\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -20.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 286.7814\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8935\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4360\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -10.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.0950\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1804\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 190.7786\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -2.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.3312\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -29.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.4270\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -3.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8925\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -46.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.5031\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0186\n",
      "Episode 231 is finished\n",
      "Average Reward for Agent 0 this episode : -9.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.8962\n",
      "Average Reward for Agent 1 this episode : -12.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 336.1835\n",
      "Average Reward for Agent 2 this episode : -41.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 162.4468\n",
      "Average Reward for Agent 3 this episode : -3.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.8808\n",
      "Average Reward for Agent 4 this episode : -19.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.7811\n",
      "Average Reward for Agent 5 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6293\n",
      "Average Reward for Agent 6 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6321\n",
      "Average Reward for Agent 7 this episode : -13.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.8320\n",
      "Average Reward for Agent 8 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7599\n",
      "Average Reward for Agent 9 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.5053\n",
      "Average Reward for Agent 10 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 307.0591\n",
      "Average Reward for Agent 11 this episode : -24.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 121.3792\n",
      "Average Reward for Agent 12 this episode : -5.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.4321\n",
      "Average Reward for Agent 13 this episode : -54.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 144.1963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.0183\n",
      "Episode 232 is finished\n",
      "Average Reward for Agent 0 this episode : -9.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.3504\n",
      "Average Reward for Agent 1 this episode : -12.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 358.1900\n",
      "Average Reward for Agent 2 this episode : -41.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.8258\n",
      "Average Reward for Agent 3 this episode : -4.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.5377\n",
      "Average Reward for Agent 4 this episode : -21.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.3190\n",
      "Average Reward for Agent 5 this episode : -1.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8108\n",
      "Average Reward for Agent 6 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.2184\n",
      "Average Reward for Agent 7 this episode : -14.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.0237\n",
      "Average Reward for Agent 8 this episode : -0.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.4572\n",
      "Average Reward for Agent 9 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.0567\n",
      "Average Reward for Agent 10 this episode : -21.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 170.9859\n",
      "Average Reward for Agent 11 this episode : -18.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.1103\n",
      "Average Reward for Agent 12 this episode : -5.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.7900\n",
      "Average Reward for Agent 13 this episode : -52.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 160.2095\n",
      "Reducing exploration for all agents to 0.018\n",
      "Episode 233 is finished\n",
      "Average Reward for Agent 0 this episode : -9.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.2491\n",
      "Average Reward for Agent 1 this episode : -14.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 215.0080\n",
      "Average Reward for Agent 2 this episode : -55.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 219.4415\n",
      "Average Reward for Agent 3 this episode : -5.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.9840\n",
      "Average Reward for Agent 4 this episode : -21.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.5046\n",
      "Average Reward for Agent 5 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9163\n",
      "Average Reward for Agent 6 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3816\n",
      "Average Reward for Agent 7 this episode : -14.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.5990\n",
      "Average Reward for Agent 8 this episode : -1.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.3133\n",
      "Average Reward for Agent 9 this episode : -1.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.4650\n",
      "Average Reward for Agent 10 this episode : -51.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 306.3175\n",
      "Average Reward for Agent 11 this episode : -5.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.9455\n",
      "Average Reward for Agent 12 this episode : -7.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3579\n",
      "Average Reward for Agent 13 this episode : -6.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 190.0999\n",
      "Reducing exploration for all agents to 0.0177\n",
      "Episode 234 is finished\n",
      "Average Reward for Agent 0 this episode : -8.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.7462\n",
      "Average Reward for Agent 1 this episode : -8.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.5176\n",
      "Average Reward for Agent 2 this episode : -40.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.8708\n",
      "Average Reward for Agent 3 this episode : -3.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.7206\n",
      "Average Reward for Agent 4 this episode : -16.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.5917\n",
      "Average Reward for Agent 5 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0961\n",
      "Average Reward for Agent 6 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8198\n",
      "Average Reward for Agent 7 this episode : -15.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0214\n",
      "Average Reward for Agent 8 this episode : -1.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0256\n",
      "Average Reward for Agent 9 this episode : -1.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.0025\n",
      "Average Reward for Agent 10 this episode : -38.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 178.6053\n",
      "Average Reward for Agent 11 this episode : -14.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.3660\n",
      "Average Reward for Agent 12 this episode : -11.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.8827\n",
      "Average Reward for Agent 13 this episode : -54.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 170.0919\n",
      "Reducing exploration for all agents to 0.0174\n",
      "Episode 235 is finished\n",
      "Average Reward for Agent 0 this episode : -8.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.8288\n",
      "Average Reward for Agent 1 this episode : -12.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.5294\n",
      "Average Reward for Agent 2 this episode : -47.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 306.4557\n",
      "Average Reward for Agent 3 this episode : -5.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.8516\n",
      "Average Reward for Agent 4 this episode : -18.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 203.6836\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4843\n",
      "Average Reward for Agent 6 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6768\n",
      "Average Reward for Agent 7 this episode : -11.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.1892\n",
      "Average Reward for Agent 8 this episode : -1.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3654\n",
      "Average Reward for Agent 9 this episode : -1.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.5500\n",
      "Average Reward for Agent 10 this episode : -41.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 213.4855\n",
      "Average Reward for Agent 11 this episode : -7.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1295\n",
      "Average Reward for Agent 12 this episode : -9.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.3555\n",
      "Average Reward for Agent 13 this episode : -44.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 152.5380\n",
      "Reducing exploration for all agents to 0.0171\n",
      "Episode 236 is finished\n",
      "Average Reward for Agent 0 this episode : -9.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.3318\n",
      "Average Reward for Agent 1 this episode : -14.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.2355\n",
      "Average Reward for Agent 2 this episode : -46.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.3905\n",
      "Average Reward for Agent 3 this episode : -1.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.2891\n",
      "Average Reward for Agent 4 this episode : -22.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 254.7280\n",
      "Average Reward for Agent 5 this episode : -1.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1486\n",
      "Average Reward for Agent 6 this episode : -1.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.7275\n",
      "Average Reward for Agent 7 this episode : -13.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.8566\n",
      "Average Reward for Agent 8 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4518\n",
      "Average Reward for Agent 9 this episode : -1.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.8784\n",
      "Average Reward for Agent 10 this episode : -52.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 229.1165\n",
      "Average Reward for Agent 11 this episode : -11.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.6031\n",
      "Average Reward for Agent 12 this episode : -10.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.4253\n",
      "Average Reward for Agent 13 this episode : -37.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 219.6023\n",
      "Reducing exploration for all agents to 0.0168\n",
      "Episode 237 is finished\n",
      "Average Reward for Agent 0 this episode : -11.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.8003\n",
      "Average Reward for Agent 1 this episode : -14.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.8807\n",
      "Average Reward for Agent 2 this episode : -49.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 241.6241\n",
      "Average Reward for Agent 3 this episode : -1.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.1162\n",
      "Average Reward for Agent 4 this episode : -21.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 155.3474\n",
      "Average Reward for Agent 5 this episode : -0.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 6 this episode : -0.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.7708\n",
      "Average Reward for Agent 7 this episode : -9.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.5520\n",
      "Average Reward for Agent 8 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3993\n",
      "Average Reward for Agent 9 this episode : -2.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.6957\n",
      "Average Reward for Agent 10 this episode : -50.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 195.8532\n",
      "Average Reward for Agent 11 this episode : -8.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.4922\n",
      "Average Reward for Agent 12 this episode : -9.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3206\n",
      "Average Reward for Agent 13 this episode : -12.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.4745\n",
      "Reducing exploration for all agents to 0.0165\n",
      "Episode 238 is finished\n",
      "Average Reward for Agent 0 this episode : -9.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.7184\n",
      "Average Reward for Agent 1 this episode : -14.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.4387\n",
      "Average Reward for Agent 2 this episode : -55.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 307.4872\n",
      "Average Reward for Agent 3 this episode : -5.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.2482\n",
      "Average Reward for Agent 4 this episode : -20.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 173.9767\n",
      "Average Reward for Agent 5 this episode : -2.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6356\n",
      "Average Reward for Agent 6 this episode : -0.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.2255\n",
      "Average Reward for Agent 7 this episode : -11.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.1769\n",
      "Average Reward for Agent 8 this episode : -1.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3199\n",
      "Average Reward for Agent 9 this episode : -1.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.7351\n",
      "Average Reward for Agent 10 this episode : -49.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 250.3168\n",
      "Average Reward for Agent 11 this episode : -7.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.2847\n",
      "Average Reward for Agent 12 this episode : -9.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.6205\n",
      "Average Reward for Agent 13 this episode : -9.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 253.5626\n",
      "Reducing exploration for all agents to 0.0162\n",
      "Episode 239 is finished\n",
      "Average Reward for Agent 0 this episode : -10.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.8039\n",
      "Average Reward for Agent 1 this episode : -15.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.8451\n",
      "Average Reward for Agent 2 this episode : -47.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.4266\n",
      "Average Reward for Agent 3 this episode : -2.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.9011\n",
      "Average Reward for Agent 4 this episode : -19.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 156.3401\n",
      "Average Reward for Agent 5 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1837\n",
      "Average Reward for Agent 6 this episode : -0.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2973\n",
      "Average Reward for Agent 7 this episode : -11.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.2890\n",
      "Average Reward for Agent 8 this episode : -1.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4891\n",
      "Average Reward for Agent 9 this episode : -2.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.0829\n",
      "Average Reward for Agent 10 this episode : -47.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 454.8944\n",
      "Average Reward for Agent 11 this episode : -13.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.5809\n",
      "Average Reward for Agent 12 this episode : -12.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.8800\n",
      "Average Reward for Agent 13 this episode : -2.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.3201\n",
      "Reducing exploration for all agents to 0.016\n",
      "Episode 240 is finished\n",
      "Average Reward for Agent 0 this episode : -8.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.4825\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -14.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 151.2416\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -51.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 201.9908\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -3.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.6936\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -18.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 276.2632\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1040\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9946\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -15.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.6507\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.5816\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -3.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.4742\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -48.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.2827\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -9.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.7196\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -8.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.2311\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -5.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.5878\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0157\n",
      "Episode 241 is finished\n",
      "Average Reward for Agent 0 this episode : -13.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.8316\n",
      "Average Reward for Agent 1 this episode : -14.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 178.2332\n",
      "Average Reward for Agent 2 this episode : -39.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 376.9191\n",
      "Average Reward for Agent 3 this episode : -1.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.5911\n",
      "Average Reward for Agent 4 this episode : -18.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 216.0793\n",
      "Average Reward for Agent 5 this episode : -0.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6311\n",
      "Average Reward for Agent 6 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.2998\n",
      "Average Reward for Agent 7 this episode : -10.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.9445\n",
      "Average Reward for Agent 8 this episode : -2.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.8885\n",
      "Average Reward for Agent 9 this episode : -3.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.1191\n",
      "Average Reward for Agent 10 this episode : -42.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 249.1583\n",
      "Average Reward for Agent 11 this episode : -8.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.5803\n",
      "Average Reward for Agent 12 this episode : -8.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.6321\n",
      "Average Reward for Agent 13 this episode : -4.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.5836\n",
      "Reducing exploration for all agents to 0.0154\n",
      "Episode 242 is finished\n",
      "Average Reward for Agent 0 this episode : -13.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.5748\n",
      "Average Reward for Agent 1 this episode : -13.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.2939\n",
      "Average Reward for Agent 2 this episode : -25.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.2666\n",
      "Average Reward for Agent 3 this episode : -1.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.0011\n",
      "Average Reward for Agent 4 this episode : -19.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 161.1880\n",
      "Average Reward for Agent 5 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5067\n",
      "Average Reward for Agent 6 this episode : -0.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6000\n",
      "Average Reward for Agent 7 this episode : -14.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 8 this episode : -2.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2421\n",
      "Average Reward for Agent 9 this episode : -3.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.2902\n",
      "Average Reward for Agent 10 this episode : -29.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1071.1663\n",
      "Average Reward for Agent 11 this episode : -11.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.3002\n",
      "Average Reward for Agent 12 this episode : -8.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.6385\n",
      "Average Reward for Agent 13 this episode : -18.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.6833\n",
      "Reducing exploration for all agents to 0.0152\n",
      "Episode 243 is finished\n",
      "Average Reward for Agent 0 this episode : -60.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.1904\n",
      "Average Reward for Agent 1 this episode : -14.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.3122\n",
      "Average Reward for Agent 2 this episode : -37.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 308.2233\n",
      "Average Reward for Agent 3 this episode : -1.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.2113\n",
      "Average Reward for Agent 4 this episode : -18.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 288.6175\n",
      "Average Reward for Agent 5 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1509\n",
      "Average Reward for Agent 6 this episode : -1.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8023\n",
      "Average Reward for Agent 7 this episode : -12.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5328\n",
      "Average Reward for Agent 8 this episode : -1.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.7136\n",
      "Average Reward for Agent 9 this episode : -3.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.6349\n",
      "Average Reward for Agent 10 this episode : -50.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 451.4456\n",
      "Average Reward for Agent 11 this episode : -7.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.4246\n",
      "Average Reward for Agent 12 this episode : -9.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5650\n",
      "Average Reward for Agent 13 this episode : -15.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.4121\n",
      "Reducing exploration for all agents to 0.0149\n",
      "Episode 244 is finished\n",
      "Average Reward for Agent 0 this episode : -35.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 561.8657\n",
      "Average Reward for Agent 1 this episode : -10.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 133.3141\n",
      "Average Reward for Agent 2 this episode : -55.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.1829\n",
      "Average Reward for Agent 3 this episode : -12.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.0955\n",
      "Average Reward for Agent 4 this episode : -16.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.7302\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2392\n",
      "Average Reward for Agent 6 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2184\n",
      "Average Reward for Agent 7 this episode : -10.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.4355\n",
      "Average Reward for Agent 8 this episode : -1.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.8260\n",
      "Average Reward for Agent 9 this episode : -3.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.2395\n",
      "Average Reward for Agent 10 this episode : -30.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 333.7205\n",
      "Average Reward for Agent 11 this episode : -4.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.3137\n",
      "Average Reward for Agent 12 this episode : -8.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.4277\n",
      "Average Reward for Agent 13 this episode : -15.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.5859\n",
      "Reducing exploration for all agents to 0.0146\n",
      "Episode 245 is finished\n",
      "Average Reward for Agent 0 this episode : -12.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 228.1839\n",
      "Average Reward for Agent 1 this episode : -12.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.6369\n",
      "Average Reward for Agent 2 this episode : -53.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.2693\n",
      "Average Reward for Agent 3 this episode : -5.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2783\n",
      "Average Reward for Agent 4 this episode : -19.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.4614\n",
      "Average Reward for Agent 5 this episode : -1.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0421\n",
      "Average Reward for Agent 6 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5473\n",
      "Average Reward for Agent 7 this episode : -11.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.0435\n",
      "Average Reward for Agent 8 this episode : -2.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.1740\n",
      "Average Reward for Agent 9 this episode : -2.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.2736\n",
      "Average Reward for Agent 10 this episode : -56.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 968.0775\n",
      "Average Reward for Agent 11 this episode : -5.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.6239\n",
      "Average Reward for Agent 12 this episode : -7.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.5012\n",
      "Average Reward for Agent 13 this episode : -17.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.2475\n",
      "Reducing exploration for all agents to 0.0144\n",
      "Episode 246 is finished\n",
      "Average Reward for Agent 0 this episode : -14.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 388.3299\n",
      "Average Reward for Agent 1 this episode : -14.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.2983\n",
      "Average Reward for Agent 2 this episode : -30.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 129.6987\n",
      "Average Reward for Agent 3 this episode : -10.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.0443\n",
      "Average Reward for Agent 4 this episode : -19.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.6033\n",
      "Average Reward for Agent 5 this episode : -1.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2561\n",
      "Average Reward for Agent 6 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7969\n",
      "Average Reward for Agent 7 this episode : -13.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.3123\n",
      "Average Reward for Agent 8 this episode : -1.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.1363\n",
      "Average Reward for Agent 9 this episode : -1.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.9990\n",
      "Average Reward for Agent 10 this episode : -52.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 248.1852\n",
      "Average Reward for Agent 11 this episode : -7.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.9198\n",
      "Average Reward for Agent 12 this episode : -8.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0313\n",
      "Average Reward for Agent 13 this episode : -16.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 140.9546\n",
      "Reducing exploration for all agents to 0.0141\n",
      "Episode 247 is finished\n",
      "Average Reward for Agent 0 this episode : -12.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.6044\n",
      "Average Reward for Agent 1 this episode : -9.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.5185\n",
      "Average Reward for Agent 2 this episode : -46.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 218.0683\n",
      "Average Reward for Agent 3 this episode : -3.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.2846\n",
      "Average Reward for Agent 4 this episode : -23.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.1526\n",
      "Average Reward for Agent 5 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5456\n",
      "Average Reward for Agent 6 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1955\n",
      "Average Reward for Agent 7 this episode : -11.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.2178\n",
      "Average Reward for Agent 8 this episode : -1.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.0213\n",
      "Average Reward for Agent 9 this episode : -1.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.2060\n",
      "Average Reward for Agent 10 this episode : -45.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 810.0087\n",
      "Average Reward for Agent 11 this episode : -7.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 123.2166\n",
      "Average Reward for Agent 12 this episode : -5.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.3321\n",
      "Average Reward for Agent 13 this episode : -42.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.3813\n",
      "Reducing exploration for all agents to 0.0139\n",
      "Episode 248 is finished\n",
      "Average Reward for Agent 0 this episode : -38.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 168.2249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 1 this episode : -10.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.6469\n",
      "Average Reward for Agent 2 this episode : -32.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.9407\n",
      "Average Reward for Agent 3 this episode : -1.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.6847\n",
      "Average Reward for Agent 4 this episode : -21.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.2477\n",
      "Average Reward for Agent 5 this episode : -1.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8400\n",
      "Average Reward for Agent 6 this episode : -0.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.8635\n",
      "Average Reward for Agent 7 this episode : -14.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.6467\n",
      "Average Reward for Agent 8 this episode : -1.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.8162\n",
      "Average Reward for Agent 9 this episode : -1.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.9427\n",
      "Average Reward for Agent 10 this episode : -53.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 500.8222\n",
      "Average Reward for Agent 11 this episode : -11.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.1606\n",
      "Average Reward for Agent 12 this episode : -11.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2759\n",
      "Average Reward for Agent 13 this episode : -51.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.4107\n",
      "Reducing exploration for all agents to 0.0137\n",
      "Episode 249 is finished\n",
      "Average Reward for Agent 0 this episode : -11.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.1015\n",
      "Average Reward for Agent 1 this episode : -9.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 175.6219\n",
      "Average Reward for Agent 2 this episode : -47.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 138.9208\n",
      "Average Reward for Agent 3 this episode : -1.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.0202\n",
      "Average Reward for Agent 4 this episode : -18.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.2247\n",
      "Average Reward for Agent 5 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0137\n",
      "Average Reward for Agent 6 this episode : -0.24\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9638\n",
      "Average Reward for Agent 7 this episode : -12.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 136.7354\n",
      "Average Reward for Agent 8 this episode : -1.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.9253\n",
      "Average Reward for Agent 9 this episode : -1.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.2651\n",
      "Average Reward for Agent 10 this episode : -48.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 284.6881\n",
      "Average Reward for Agent 11 this episode : -8.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.2638\n",
      "Average Reward for Agent 12 this episode : -5.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.4856\n",
      "Average Reward for Agent 13 this episode : -43.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.9302\n",
      "Reducing exploration for all agents to 0.0134\n",
      "Episode 250 is finished\n",
      "Average Reward for Agent 0 this episode : -36.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.2470\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -15.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 258.1107\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -28.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.7119\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -4.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.2261\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -19.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 172.1970\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0541\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6835\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -10.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.4749\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.3226\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -1.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.7809\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -38.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 663.4043\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -11.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.9302\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -13.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5183\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -70.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.6771\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0132\n",
      "Episode 251 is finished\n",
      "Average Reward for Agent 0 this episode : -35.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 139.2364\n",
      "Average Reward for Agent 1 this episode : -6.2\n",
      "Saving architecture, weights, optimizer state for best agent-1\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent1_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.8023\n",
      "Average Reward for Agent 2 this episode : -39.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 148.9049\n",
      "Average Reward for Agent 3 this episode : -6.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9488\n",
      "Average Reward for Agent 4 this episode : -20.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 261.5122\n",
      "Average Reward for Agent 5 this episode : -1.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8259\n",
      "Average Reward for Agent 6 this episode : -1.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.4622\n",
      "Average Reward for Agent 7 this episode : -15.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.4666\n",
      "Average Reward for Agent 8 this episode : -1.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.9698\n",
      "Average Reward for Agent 9 this episode : -1.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.9841\n",
      "Average Reward for Agent 10 this episode : -46.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 286.7099\n",
      "Average Reward for Agent 11 this episode : -3.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.2652\n",
      "Average Reward for Agent 12 this episode : -5.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.4334\n",
      "Average Reward for Agent 13 this episode : -53.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 178.4230\n",
      "Reducing exploration for all agents to 0.013\n",
      "Episode 252 is finished\n",
      "Average Reward for Agent 0 this episode : -42.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.9918\n",
      "Average Reward for Agent 1 this episode : -10.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.1439\n",
      "Average Reward for Agent 2 this episode : -14.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.8770\n",
      "Average Reward for Agent 3 this episode : -5.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6800\n",
      "Average Reward for Agent 4 this episode : -17.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.4574\n",
      "Average Reward for Agent 5 this episode : -0.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6212\n",
      "Average Reward for Agent 6 this episode : -1.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4871\n",
      "Average Reward for Agent 7 this episode : -16.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.1332\n",
      "Average Reward for Agent 8 this episode : -1.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.9667\n",
      "Average Reward for Agent 9 this episode : -1.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.4492\n",
      "Average Reward for Agent 10 this episode : -61.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 440.1700\n",
      "Average Reward for Agent 11 this episode : -3.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.5205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 12 this episode : -9.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.4221\n",
      "Average Reward for Agent 13 this episode : -46.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 272.1072\n",
      "Reducing exploration for all agents to 0.0127\n",
      "Episode 253 is finished\n",
      "Average Reward for Agent 0 this episode : -14.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 117.1384\n",
      "Average Reward for Agent 1 this episode : -11.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 167.6347\n",
      "Average Reward for Agent 2 this episode : -41.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.9723\n",
      "Average Reward for Agent 3 this episode : -2.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.7332\n",
      "Average Reward for Agent 4 this episode : -16.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 154.9458\n",
      "Average Reward for Agent 5 this episode : -2.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2403\n",
      "Average Reward for Agent 6 this episode : -1.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.4922\n",
      "Average Reward for Agent 7 this episode : -14.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1879\n",
      "Average Reward for Agent 8 this episode : -3.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.3517\n",
      "Average Reward for Agent 9 this episode : -1.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.0553\n",
      "Average Reward for Agent 10 this episode : -52.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 560.5237\n",
      "Average Reward for Agent 11 this episode : -4.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.5854\n",
      "Average Reward for Agent 12 this episode : -5.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.0060\n",
      "Average Reward for Agent 13 this episode : -54.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.3508\n",
      "Reducing exploration for all agents to 0.0125\n",
      "Episode 254 is finished\n",
      "Average Reward for Agent 0 this episode : -16.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.2107\n",
      "Average Reward for Agent 1 this episode : -12.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.3964\n",
      "Average Reward for Agent 2 this episode : -10.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 615.1497\n",
      "Average Reward for Agent 3 this episode : -7.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.6412\n",
      "Average Reward for Agent 4 this episode : -18.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.6954\n",
      "Average Reward for Agent 5 this episode : -1.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0631\n",
      "Average Reward for Agent 6 this episode : -1.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.4020\n",
      "Average Reward for Agent 7 this episode : -15.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.4686\n",
      "Average Reward for Agent 8 this episode : -1.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.4567\n",
      "Average Reward for Agent 9 this episode : -1.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.8517\n",
      "Average Reward for Agent 10 this episode : -51.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.9057\n",
      "Average Reward for Agent 11 this episode : -7.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8402\n",
      "Average Reward for Agent 12 this episode : -11.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.7996\n",
      "Average Reward for Agent 13 this episode : -47.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 191.7764\n",
      "Reducing exploration for all agents to 0.0123\n",
      "Episode 255 is finished\n",
      "Average Reward for Agent 0 this episode : -16.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.5912\n",
      "Average Reward for Agent 1 this episode : -12.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 142.8686\n",
      "Average Reward for Agent 2 this episode : -14.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 346.5079\n",
      "Average Reward for Agent 3 this episode : -4.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2869\n",
      "Average Reward for Agent 4 this episode : -19.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.4226\n",
      "Average Reward for Agent 5 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7211\n",
      "Average Reward for Agent 6 this episode : -1.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.1357\n",
      "Average Reward for Agent 7 this episode : -14.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.1396\n",
      "Average Reward for Agent 8 this episode : -2.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.5213\n",
      "Average Reward for Agent 9 this episode : -1.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.1060\n",
      "Average Reward for Agent 10 this episode : -51.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 557.8795\n",
      "Average Reward for Agent 11 this episode : -5.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.3427\n",
      "Average Reward for Agent 12 this episode : -11.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.6021\n",
      "Average Reward for Agent 13 this episode : -60.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 133.9605\n",
      "Reducing exploration for all agents to 0.0121\n",
      "Episode 256 is finished\n",
      "Average Reward for Agent 0 this episode : -41.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.7310\n",
      "Average Reward for Agent 1 this episode : -13.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.7920\n",
      "Average Reward for Agent 2 this episode : -33.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 468.2490\n",
      "Average Reward for Agent 3 this episode : -2.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.6025\n",
      "Average Reward for Agent 4 this episode : -18.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.8793\n",
      "Average Reward for Agent 5 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0914\n",
      "Average Reward for Agent 6 this episode : -1.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.9529\n",
      "Average Reward for Agent 7 this episode : -13.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.5698\n",
      "Average Reward for Agent 8 this episode : -2.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.1210\n",
      "Average Reward for Agent 9 this episode : -1.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.0952\n",
      "Average Reward for Agent 10 this episode : -62.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 168.9900\n",
      "Average Reward for Agent 11 this episode : -3.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.5807\n",
      "Average Reward for Agent 12 this episode : -4.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.7275\n",
      "Average Reward for Agent 13 this episode : -38.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 184.5748\n",
      "Reducing exploration for all agents to 0.0119\n",
      "Episode 257 is finished\n",
      "Average Reward for Agent 0 this episode : -40.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.2295\n",
      "Average Reward for Agent 1 this episode : -9.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 152.7505\n",
      "Average Reward for Agent 2 this episode : -42.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 329.1619\n",
      "Average Reward for Agent 3 this episode : -3.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.9823\n",
      "Average Reward for Agent 4 this episode : -16.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.5276\n",
      "Average Reward for Agent 5 this episode : -1.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9805\n",
      "Average Reward for Agent 6 this episode : -1.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.5722\n",
      "Average Reward for Agent 7 this episode : -16.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 125.3822\n",
      "Average Reward for Agent 8 this episode : -2.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.8373\n",
      "Average Reward for Agent 9 this episode : -1.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.8874\n",
      "Average Reward for Agent 10 this episode : -46.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 373.4002\n",
      "Average Reward for Agent 11 this episode : -3.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.2352\n",
      "Average Reward for Agent 12 this episode : -12.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.0303\n",
      "Average Reward for Agent 13 this episode : -44.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.6820\n",
      "Reducing exploration for all agents to 0.0117\n",
      "Episode 258 is finished\n",
      "Average Reward for Agent 0 this episode : -20.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.5360\n",
      "Average Reward for Agent 1 this episode : -12.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 141.9079\n",
      "Average Reward for Agent 2 this episode : -35.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 283.3174\n",
      "Average Reward for Agent 3 this episode : -3.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0300\n",
      "Average Reward for Agent 4 this episode : -24.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 169.9229\n",
      "Average Reward for Agent 5 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1936\n",
      "Average Reward for Agent 6 this episode : -2.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.9344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 7 this episode : -14.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.1640\n",
      "Average Reward for Agent 8 this episode : -2.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.1185\n",
      "Average Reward for Agent 9 this episode : -2.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.7159\n",
      "Average Reward for Agent 10 this episode : -40.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 558.4656\n",
      "Average Reward for Agent 11 this episode : -2.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.3137\n",
      "Average Reward for Agent 12 this episode : -7.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.8486\n",
      "Average Reward for Agent 13 this episode : -46.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.1461\n",
      "Reducing exploration for all agents to 0.0115\n",
      "Episode 259 is finished\n",
      "Average Reward for Agent 0 this episode : -43.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.2942\n",
      "Average Reward for Agent 1 this episode : -6.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 219.2371\n",
      "Average Reward for Agent 2 this episode : -39.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 540.8256\n",
      "Average Reward for Agent 3 this episode : -14.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.9140\n",
      "Average Reward for Agent 4 this episode : -18.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.4831\n",
      "Average Reward for Agent 5 this episode : -0.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7301\n",
      "Average Reward for Agent 6 this episode : -1.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.7288\n",
      "Average Reward for Agent 7 this episode : -15.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.1745\n",
      "Average Reward for Agent 8 this episode : -1.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.2412\n",
      "Average Reward for Agent 9 this episode : -2.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.3491\n",
      "Average Reward for Agent 10 this episode : -51.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 160.6829\n",
      "Average Reward for Agent 11 this episode : -2.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.2249\n",
      "Average Reward for Agent 12 this episode : -2.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.9009\n",
      "Average Reward for Agent 13 this episode : -43.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.7518\n",
      "Reducing exploration for all agents to 0.0113\n",
      "Episode 260 is finished\n",
      "Average Reward for Agent 0 this episode : -16.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.0306\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -8.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 206.8813\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -21.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 258.9526\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -4.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.5339\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -18.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.8924\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3112\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.0842\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -13.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.8044\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6373\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -4.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.8465\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -44.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 314.2818\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -1.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.7567\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -10.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.8544\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -54.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.5363\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0111\n",
      "Episode 261 is finished\n",
      "Average Reward for Agent 0 this episode : -14.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.0284\n",
      "Average Reward for Agent 1 this episode : -13.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 100.9678\n",
      "Average Reward for Agent 2 this episode : -29.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 161.5964\n",
      "Average Reward for Agent 3 this episode : -2.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1725\n",
      "Average Reward for Agent 4 this episode : -19.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.1442\n",
      "Average Reward for Agent 5 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5717\n",
      "Average Reward for Agent 6 this episode : -7.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.7348\n",
      "Average Reward for Agent 7 this episode : -14.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.1768\n",
      "Average Reward for Agent 8 this episode : -1.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.6954\n",
      "Average Reward for Agent 9 this episode : -3.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.1963\n",
      "Average Reward for Agent 10 this episode : -38.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 239.8520\n",
      "Average Reward for Agent 11 this episode : -3.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.0539\n",
      "Average Reward for Agent 12 this episode : -2.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.5435\n",
      "Average Reward for Agent 13 this episode : -37.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 173.2711\n",
      "Reducing exploration for all agents to 0.0109\n",
      "Episode 262 is finished\n",
      "Average Reward for Agent 0 this episode : -43.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.1652\n",
      "Average Reward for Agent 1 this episode : -10.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 100.8413\n",
      "Average Reward for Agent 2 this episode : -20.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 271.2299\n",
      "Average Reward for Agent 3 this episode : -11.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.9900\n",
      "Average Reward for Agent 4 this episode : -18.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.3225\n",
      "Average Reward for Agent 5 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4578\n",
      "Average Reward for Agent 6 this episode : -15.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.2356\n",
      "Average Reward for Agent 7 this episode : -14.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.7507\n",
      "Average Reward for Agent 8 this episode : -1.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1092\n",
      "Average Reward for Agent 9 this episode : -6.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.8893\n",
      "Average Reward for Agent 10 this episode : -52.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.1281\n",
      "Average Reward for Agent 11 this episode : -2.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.3724\n",
      "Average Reward for Agent 12 this episode : -10.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.9200\n",
      "Average Reward for Agent 13 this episode : -55.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 124.9902\n",
      "Reducing exploration for all agents to 0.0107\n",
      "Episode 263 is finished\n",
      "Average Reward for Agent 0 this episode : -12.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.5413\n",
      "Average Reward for Agent 1 this episode : -12.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.8866\n",
      "Average Reward for Agent 2 this episode : -28.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 156.5504\n",
      "Average Reward for Agent 3 this episode : -1.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2070\n",
      "Average Reward for Agent 4 this episode : -14.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 197.0028\n",
      "Average Reward for Agent 5 this episode : -1.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2639\n",
      "Average Reward for Agent 6 this episode : -5.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.1853\n",
      "Average Reward for Agent 7 this episode : -14.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.0277\n",
      "Average Reward for Agent 8 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.2412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 9 this episode : -3.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.7796\n",
      "Average Reward for Agent 10 this episode : -58.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 287.8488\n",
      "Average Reward for Agent 11 this episode : -3.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.0077\n",
      "Average Reward for Agent 12 this episode : -3.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.6454\n",
      "Average Reward for Agent 13 this episode : -42.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.8498\n",
      "Reducing exploration for all agents to 0.0105\n",
      "Episode 264 is finished\n",
      "Average Reward for Agent 0 this episode : -37.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.9815\n",
      "Average Reward for Agent 1 this episode : -12.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.8025\n",
      "Average Reward for Agent 2 this episode : -12.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 307.4982\n",
      "Average Reward for Agent 3 this episode : -1.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.0405\n",
      "Average Reward for Agent 4 this episode : -20.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3768\n",
      "Average Reward for Agent 5 this episode : -0.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8893\n",
      "Average Reward for Agent 6 this episode : -10.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.4634\n",
      "Average Reward for Agent 7 this episode : -15.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.3177\n",
      "Average Reward for Agent 8 this episode : -1.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.6677\n",
      "Average Reward for Agent 9 this episode : -3.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.7628\n",
      "Average Reward for Agent 10 this episode : -61.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.6435\n",
      "Average Reward for Agent 11 this episode : -3.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.9000\n",
      "Average Reward for Agent 12 this episode : -11.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7854\n",
      "Average Reward for Agent 13 this episode : -53.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.5686\n",
      "Reducing exploration for all agents to 0.0104\n",
      "Episode 265 is finished\n",
      "Average Reward for Agent 0 this episode : -38.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.3866\n",
      "Average Reward for Agent 1 this episode : -10.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.1873\n",
      "Average Reward for Agent 2 this episode : -41.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 268.1026\n",
      "Average Reward for Agent 3 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.3812\n",
      "Average Reward for Agent 4 this episode : -18.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.6394\n",
      "Average Reward for Agent 5 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2131\n",
      "Average Reward for Agent 6 this episode : -7.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.1897\n",
      "Average Reward for Agent 7 this episode : -12.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.2130\n",
      "Average Reward for Agent 8 this episode : -1.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3344\n",
      "Average Reward for Agent 9 this episode : -2.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.2853\n",
      "Average Reward for Agent 10 this episode : -47.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 232.6381\n",
      "Average Reward for Agent 11 this episode : -2.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.3873\n",
      "Average Reward for Agent 12 this episode : -5.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7085\n",
      "Average Reward for Agent 13 this episode : -53.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 89.8872\n",
      "Reducing exploration for all agents to 0.0102\n",
      "Episode 266 is finished\n",
      "Average Reward for Agent 0 this episode : -37.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.3958\n",
      "Average Reward for Agent 1 this episode : -8.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.6913\n",
      "Average Reward for Agent 2 this episode : -40.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 167.1890\n",
      "Average Reward for Agent 3 this episode : -1.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0301\n",
      "Average Reward for Agent 4 this episode : -15.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.2054\n",
      "Average Reward for Agent 5 this episode : -1.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7094\n",
      "Average Reward for Agent 6 this episode : -15.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.6310\n",
      "Average Reward for Agent 7 this episode : -13.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.2479\n",
      "Average Reward for Agent 8 this episode : -2.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.9084\n",
      "Average Reward for Agent 9 this episode : -1.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.8907\n",
      "Average Reward for Agent 10 this episode : -45.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 221.0627\n",
      "Average Reward for Agent 11 this episode : -2.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.0414\n",
      "Average Reward for Agent 12 this episode : -8.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.1774\n",
      "Average Reward for Agent 13 this episode : -50.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 136.6132\n",
      "Reducing exploration for all agents to 0.01\n",
      "Episode 267 is finished\n",
      "Average Reward for Agent 0 this episode : -39.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.3390\n",
      "Average Reward for Agent 1 this episode : -11.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.7646\n",
      "Average Reward for Agent 2 this episode : -40.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 182.4724\n",
      "Average Reward for Agent 3 this episode : -3.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3999\n",
      "Average Reward for Agent 4 this episode : -14.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.6567\n",
      "Average Reward for Agent 5 this episode : -1.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6284\n",
      "Average Reward for Agent 6 this episode : -14.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.0904\n",
      "Average Reward for Agent 7 this episode : -13.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.1597\n",
      "Average Reward for Agent 8 this episode : -3.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.2525\n",
      "Average Reward for Agent 9 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.6337\n",
      "Average Reward for Agent 10 this episode : -53.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.6167\n",
      "Average Reward for Agent 11 this episode : -2.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.8788\n",
      "Average Reward for Agent 12 this episode : -6.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.4035\n",
      "Average Reward for Agent 13 this episode : -43.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.2318\n",
      "Reducing exploration for all agents to 0.0098\n",
      "Episode 268 is finished\n",
      "Average Reward for Agent 0 this episode : -36.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.3908\n",
      "Average Reward for Agent 1 this episode : -13.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6884\n",
      "Average Reward for Agent 2 this episode : -33.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 263.1738\n",
      "Average Reward for Agent 3 this episode : -4.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3134\n",
      "Average Reward for Agent 4 this episode : -20.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 162.5731\n",
      "Average Reward for Agent 5 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8034\n",
      "Average Reward for Agent 6 this episode : -44.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.6959\n",
      "Average Reward for Agent 7 this episode : -15.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.1243\n",
      "Average Reward for Agent 8 this episode : -4.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.3163\n",
      "Average Reward for Agent 9 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.8378\n",
      "Average Reward for Agent 10 this episode : -46.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 222.4634\n",
      "Average Reward for Agent 11 this episode : -3.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.4753\n",
      "Average Reward for Agent 12 this episode : -4.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6170\n",
      "Average Reward for Agent 13 this episode : -43.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.5531\n",
      "Reducing exploration for all agents to 0.0097\n",
      "Episode 269 is finished\n",
      "Average Reward for Agent 0 this episode : -21.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.0566\n",
      "Average Reward for Agent 1 this episode : -11.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.6854\n",
      "Average Reward for Agent 2 this episode : -38.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 321.1048\n",
      "Average Reward for Agent 3 this episode : -4.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.1501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 4 this episode : -18.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.6140\n",
      "Average Reward for Agent 5 this episode : -0.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0638\n",
      "Average Reward for Agent 6 this episode : -15.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.3005\n",
      "Average Reward for Agent 7 this episode : -13.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.1124\n",
      "Average Reward for Agent 8 this episode : -2.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.8188\n",
      "Average Reward for Agent 9 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.2265\n",
      "Average Reward for Agent 10 this episode : -49.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 229.4629\n",
      "Average Reward for Agent 11 this episode : -3.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.3968\n",
      "Average Reward for Agent 12 this episode : -4.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.8041\n",
      "Average Reward for Agent 13 this episode : -49.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.0861\n",
      "Reducing exploration for all agents to 0.0095\n",
      "Episode 270 is finished\n",
      "Average Reward for Agent 0 this episode : -32.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.4906\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -12.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.5043\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -41.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 231.4845\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -4.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6865\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -15.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.5241\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3269\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -4.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.1093\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -12.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.2726\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.2407\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.6345\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -52.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 206.6386\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -2.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.9530\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -11.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7223\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -43.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.2152\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0093\n",
      "Episode 271 is finished\n",
      "Average Reward for Agent 0 this episode : -19.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.5859\n",
      "Average Reward for Agent 1 this episode : -5.25\n",
      "Saving architecture, weights, optimizer state for best agent-1\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent1_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7082\n",
      "Average Reward for Agent 2 this episode : -39.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 209.3694\n",
      "Average Reward for Agent 3 this episode : -2.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6919\n",
      "Average Reward for Agent 4 this episode : -18.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.7188\n",
      "Average Reward for Agent 5 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2601\n",
      "Average Reward for Agent 6 this episode : -0.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.5365\n",
      "Average Reward for Agent 7 this episode : -11.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.1533\n",
      "Average Reward for Agent 8 this episode : -0.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.3591\n",
      "Average Reward for Agent 9 this episode : -0.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.7586\n",
      "Average Reward for Agent 10 this episode : -44.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 261.1198\n",
      "Average Reward for Agent 11 this episode : -2.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8898\n",
      "Average Reward for Agent 12 this episode : -3.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.6039\n",
      "Average Reward for Agent 13 this episode : -48.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 129.6765\n",
      "Reducing exploration for all agents to 0.0092\n",
      "Episode 272 is finished\n",
      "Average Reward for Agent 0 this episode : -41.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.3811\n",
      "Average Reward for Agent 1 this episode : -8.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5332\n",
      "Average Reward for Agent 2 this episode : -20.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 201.1481\n",
      "Average Reward for Agent 3 this episode : -2.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7135\n",
      "Average Reward for Agent 4 this episode : -18.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.8256\n",
      "Average Reward for Agent 5 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3019\n",
      "Average Reward for Agent 6 this episode : -3.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.5266\n",
      "Average Reward for Agent 7 this episode : -13.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.5241\n",
      "Average Reward for Agent 8 this episode : -0.11\n",
      "Saving architecture, weights, optimizer state for best agent-8\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6082\n",
      "Average Reward for Agent 9 this episode : -0.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.6349\n",
      "Average Reward for Agent 10 this episode : -45.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.7140\n",
      "Average Reward for Agent 11 this episode : -3.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.3209\n",
      "Average Reward for Agent 12 this episode : -9.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0793\n",
      "Average Reward for Agent 13 this episode : -43.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 158.3965\n",
      "Reducing exploration for all agents to 0.009\n",
      "Episode 273 is finished\n",
      "Average Reward for Agent 0 this episode : -12.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.6954\n",
      "Average Reward for Agent 1 this episode : -14.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7295\n",
      "Average Reward for Agent 2 this episode : -38.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.9856\n",
      "Average Reward for Agent 3 this episode : -2.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0931\n",
      "Average Reward for Agent 4 this episode : -19.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.5420\n",
      "Average Reward for Agent 5 this episode : -0.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8724\n",
      "Average Reward for Agent 6 this episode : -5.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.7302\n",
      "Average Reward for Agent 7 this episode : -13.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.1941\n",
      "Average Reward for Agent 8 this episode : -0.07\n",
      "Saving architecture, weights, optimizer state for best agent-8\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent8_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3875\n",
      "Average Reward for Agent 9 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.6489\n",
      "Average Reward for Agent 10 this episode : -48.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 199.3137\n",
      "Average Reward for Agent 11 this episode : -2.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.1185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 12 this episode : -6.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5884\n",
      "Average Reward for Agent 13 this episode : -42.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.7175\n",
      "Reducing exploration for all agents to 0.0089\n",
      "Episode 274 is finished\n",
      "Average Reward for Agent 0 this episode : -41.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.7459\n",
      "Average Reward for Agent 1 this episode : -10.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4512\n",
      "Average Reward for Agent 2 this episode : -45.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.3257\n",
      "Average Reward for Agent 3 this episode : -8.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5765\n",
      "Average Reward for Agent 4 this episode : -16.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.0493\n",
      "Average Reward for Agent 5 this episode : -1.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5710\n",
      "Average Reward for Agent 6 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.1755\n",
      "Average Reward for Agent 7 this episode : -10.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9219\n",
      "Average Reward for Agent 8 this episode : -0.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1159\n",
      "Average Reward for Agent 9 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.2309\n",
      "Average Reward for Agent 10 this episode : -50.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 144.1733\n",
      "Average Reward for Agent 11 this episode : -1.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.3595\n",
      "Average Reward for Agent 12 this episode : -4.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7947\n",
      "Average Reward for Agent 13 this episode : -52.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.3681\n",
      "Reducing exploration for all agents to 0.0087\n",
      "Episode 275 is finished\n",
      "Average Reward for Agent 0 this episode : -41.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.1227\n",
      "Average Reward for Agent 1 this episode : -9.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.5051\n",
      "Average Reward for Agent 2 this episode : -48.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.0125\n",
      "Average Reward for Agent 3 this episode : -2.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3113\n",
      "Average Reward for Agent 4 this episode : -23.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 75.1885\n",
      "Average Reward for Agent 5 this episode : -0.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7761\n",
      "Average Reward for Agent 6 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.6585\n",
      "Average Reward for Agent 7 this episode : -8.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.6407\n",
      "Average Reward for Agent 8 this episode : -0.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2031\n",
      "Average Reward for Agent 9 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.0309\n",
      "Average Reward for Agent 10 this episode : -44.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.9145\n",
      "Average Reward for Agent 11 this episode : -3.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.5172\n",
      "Average Reward for Agent 12 this episode : -6.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2634\n",
      "Average Reward for Agent 13 this episode : -45.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 185.7181\n",
      "Reducing exploration for all agents to 0.0086\n",
      "Episode 276 is finished\n",
      "Average Reward for Agent 0 this episode : -43.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.6453\n",
      "Average Reward for Agent 1 this episode : -3.49\n",
      "Saving architecture, weights, optimizer state for best agent-1\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent1_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.7414\n",
      "Average Reward for Agent 2 this episode : -49.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.3472\n",
      "Average Reward for Agent 3 this episode : -5.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.8033\n",
      "Average Reward for Agent 4 this episode : -22.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.6833\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7321\n",
      "Average Reward for Agent 6 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.3990\n",
      "Average Reward for Agent 7 this episode : -11.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.9945\n",
      "Average Reward for Agent 8 this episode : -0.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3495\n",
      "Average Reward for Agent 9 this episode : -0.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.4816\n",
      "Average Reward for Agent 10 this episode : -53.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.4693\n",
      "Average Reward for Agent 11 this episode : -2.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.2060\n",
      "Average Reward for Agent 12 this episode : -5.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1213\n",
      "Average Reward for Agent 13 this episode : -50.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.8620\n",
      "Reducing exploration for all agents to 0.0084\n",
      "Episode 277 is finished\n",
      "Average Reward for Agent 0 this episode : -40.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.8240\n",
      "Average Reward for Agent 1 this episode : -5.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.4533\n",
      "Average Reward for Agent 2 this episode : -40.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 133.4223\n",
      "Average Reward for Agent 3 this episode : -4.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1719\n",
      "Average Reward for Agent 4 this episode : -17.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.2588\n",
      "Average Reward for Agent 5 this episode : -0.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3964\n",
      "Average Reward for Agent 6 this episode : -1.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9123\n",
      "Average Reward for Agent 7 this episode : -10.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.2570\n",
      "Average Reward for Agent 8 this episode : -0.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6451\n",
      "Average Reward for Agent 9 this episode : -1.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.2941\n",
      "Average Reward for Agent 10 this episode : -44.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 144.1036\n",
      "Average Reward for Agent 11 this episode : -2.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.8483\n",
      "Average Reward for Agent 12 this episode : -4.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3348\n",
      "Average Reward for Agent 13 this episode : -46.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 232.5774\n",
      "Reducing exploration for all agents to 0.0083\n",
      "Episode 278 is finished\n",
      "Average Reward for Agent 0 this episode : -17.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.7351\n",
      "Average Reward for Agent 1 this episode : -10.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.5261\n",
      "Average Reward for Agent 2 this episode : -43.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.5444\n",
      "Average Reward for Agent 3 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1258\n",
      "Average Reward for Agent 4 this episode : -19.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.9064\n",
      "Average Reward for Agent 5 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5284\n",
      "Average Reward for Agent 6 this episode : -0.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.4612\n",
      "Average Reward for Agent 7 this episode : -11.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.3657\n",
      "Average Reward for Agent 8 this episode : -0.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2951\n",
      "Average Reward for Agent 9 this episode : -1.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.6184\n",
      "Average Reward for Agent 10 this episode : -48.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.2492\n",
      "Average Reward for Agent 11 this episode : -2.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.0983\n",
      "Average Reward for Agent 12 this episode : -8.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2110\n",
      "Average Reward for Agent 13 this episode : -42.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.7144\n",
      "Reducing exploration for all agents to 0.0081\n",
      "Episode 279 is finished\n",
      "Average Reward for Agent 0 this episode : -20.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.2309\n",
      "Average Reward for Agent 1 this episode : -10.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.3817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 2 this episode : -45.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 156.7545\n",
      "Average Reward for Agent 3 this episode : -3.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.9086\n",
      "Average Reward for Agent 4 this episode : -20.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 152.6767\n",
      "Average Reward for Agent 5 this episode : -0.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8953\n",
      "Average Reward for Agent 6 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.5079\n",
      "Average Reward for Agent 7 this episode : -13.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.0343\n",
      "Average Reward for Agent 8 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7665\n",
      "Average Reward for Agent 9 this episode : -9.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.4961\n",
      "Average Reward for Agent 10 this episode : -52.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 156.4500\n",
      "Average Reward for Agent 11 this episode : -2.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.2399\n",
      "Average Reward for Agent 12 this episode : -4.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.7985\n",
      "Average Reward for Agent 13 this episode : -39.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 185.8966\n",
      "Reducing exploration for all agents to 0.008\n",
      "Episode 280 is finished\n",
      "Average Reward for Agent 0 this episode : -40.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.2081\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -10.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.3045\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -19.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 244.1553\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -3.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4915\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -17.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.1067\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0653\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4747\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -13.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.0017\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.8827\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -3.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.3928\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -55.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.3193\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -2.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.8148\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -9.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.5967\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -49.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.3641\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0078\n",
      "Episode 281 is finished\n",
      "Average Reward for Agent 0 this episode : -10.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 124.0472\n",
      "Average Reward for Agent 1 this episode : -13.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 195.3755\n",
      "Average Reward for Agent 2 this episode : -47.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 269.3832\n",
      "Average Reward for Agent 3 this episode : -4.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.3463\n",
      "Average Reward for Agent 4 this episode : -21.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 139.1559\n",
      "Average Reward for Agent 5 this episode : -0.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6995\n",
      "Average Reward for Agent 6 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9264\n",
      "Average Reward for Agent 7 this episode : -12.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7651\n",
      "Average Reward for Agent 8 this episode : -0.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2616\n",
      "Average Reward for Agent 9 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.4121\n",
      "Average Reward for Agent 10 this episode : -45.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 259.2610\n",
      "Average Reward for Agent 11 this episode : -4.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.7357\n",
      "Average Reward for Agent 12 this episode : -5.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.1876\n",
      "Average Reward for Agent 13 this episode : -46.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.9128\n",
      "Reducing exploration for all agents to 0.0077\n",
      "Episode 282 is finished\n",
      "Average Reward for Agent 0 this episode : -40.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.1893\n",
      "Average Reward for Agent 1 this episode : -11.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.1763\n",
      "Average Reward for Agent 2 this episode : -49.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.9262\n",
      "Average Reward for Agent 3 this episode : -2.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7876\n",
      "Average Reward for Agent 4 this episode : -18.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.8189\n",
      "Average Reward for Agent 5 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5673\n",
      "Average Reward for Agent 6 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0875\n",
      "Average Reward for Agent 7 this episode : -9.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.0792\n",
      "Average Reward for Agent 8 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.5935\n",
      "Average Reward for Agent 9 this episode : -0.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.9234\n",
      "Average Reward for Agent 10 this episode : -34.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.0023\n",
      "Average Reward for Agent 11 this episode : -3.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.7399\n",
      "Average Reward for Agent 12 this episode : -6.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.1779\n",
      "Average Reward for Agent 13 this episode : -13.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 209.4149\n",
      "Reducing exploration for all agents to 0.0076\n",
      "Episode 283 is finished\n",
      "Average Reward for Agent 0 this episode : -42.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.5159\n",
      "Average Reward for Agent 1 this episode : -12.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9921\n",
      "Average Reward for Agent 2 this episode : -44.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.1109\n",
      "Average Reward for Agent 3 this episode : -7.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5196\n",
      "Average Reward for Agent 4 this episode : -16.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.2697\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0129\n",
      "Average Reward for Agent 6 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.6512\n",
      "Average Reward for Agent 7 this episode : -11.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.4729\n",
      "Average Reward for Agent 8 this episode : -0.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.8834\n",
      "Average Reward for Agent 9 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.5401\n",
      "Average Reward for Agent 10 this episode : -48.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.5015\n",
      "Average Reward for Agent 11 this episode : -4.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.5336\n",
      "Average Reward for Agent 12 this episode : -3.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.0747\n",
      "Average Reward for Agent 13 this episode : -5.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 290.7378\n",
      "Reducing exploration for all agents to 0.0075\n",
      "Episode 284 is finished\n",
      "Average Reward for Agent 0 this episode : -40.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.3215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 1 this episode : -7.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 175.8509\n",
      "Average Reward for Agent 2 this episode : -50.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.0714\n",
      "Average Reward for Agent 3 this episode : -4.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1730\n",
      "Average Reward for Agent 4 this episode : -21.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.4465\n",
      "Average Reward for Agent 5 this episode : -0.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.9947\n",
      "Average Reward for Agent 6 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.5826\n",
      "Average Reward for Agent 7 this episode : -13.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.9675\n",
      "Average Reward for Agent 8 this episode : -0.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.1500\n",
      "Average Reward for Agent 9 this episode : -0.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.5930\n",
      "Average Reward for Agent 10 this episode : -61.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 234.2793\n",
      "Average Reward for Agent 11 this episode : -6.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.0352\n",
      "Average Reward for Agent 12 this episode : -5.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9971\n",
      "Average Reward for Agent 13 this episode : -2.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.2901\n",
      "Reducing exploration for all agents to 0.0073\n",
      "Episode 285 is finished\n",
      "Average Reward for Agent 0 this episode : -44.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.8557\n",
      "Average Reward for Agent 1 this episode : -10.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.7169\n",
      "Average Reward for Agent 2 this episode : -43.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.2816\n",
      "Average Reward for Agent 3 this episode : -6.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7003\n",
      "Average Reward for Agent 4 this episode : -16.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.4241\n",
      "Average Reward for Agent 5 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1644\n",
      "Average Reward for Agent 6 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.2054\n",
      "Average Reward for Agent 7 this episode : -14.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.4286\n",
      "Average Reward for Agent 8 this episode : -1.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1455\n",
      "Average Reward for Agent 9 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.3784\n",
      "Average Reward for Agent 10 this episode : -50.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.3949\n",
      "Average Reward for Agent 11 this episode : -4.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.4688\n",
      "Average Reward for Agent 12 this episode : -17.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.0250\n",
      "Average Reward for Agent 13 this episode : -48.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 181.3405\n",
      "Reducing exploration for all agents to 0.0072\n",
      "Episode 286 is finished\n",
      "Average Reward for Agent 0 this episode : -20.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.8835\n",
      "Average Reward for Agent 1 this episode : -9.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8603\n",
      "Average Reward for Agent 2 this episode : -45.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.7511\n",
      "Average Reward for Agent 3 this episode : -0.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6031\n",
      "Average Reward for Agent 4 this episode : -19.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.4632\n",
      "Average Reward for Agent 5 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9167\n",
      "Average Reward for Agent 6 this episode : -0.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0846\n",
      "Average Reward for Agent 7 this episode : -14.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3056\n",
      "Average Reward for Agent 8 this episode : -1.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2652\n",
      "Average Reward for Agent 9 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.7661\n",
      "Average Reward for Agent 10 this episode : -54.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.8334\n",
      "Average Reward for Agent 11 this episode : -4.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.2924\n",
      "Average Reward for Agent 12 this episode : -5.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6439\n",
      "Average Reward for Agent 13 this episode : -41.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 262.7529\n",
      "Reducing exploration for all agents to 0.0071\n",
      "Episode 287 is finished\n",
      "Average Reward for Agent 0 this episode : -44.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.0521\n",
      "Average Reward for Agent 1 this episode : -14.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.4728\n",
      "Average Reward for Agent 2 this episode : -46.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 127.8099\n",
      "Average Reward for Agent 3 this episode : -4.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.3357\n",
      "Average Reward for Agent 4 this episode : -19.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 115.6014\n",
      "Average Reward for Agent 5 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8733\n",
      "Average Reward for Agent 6 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1189\n",
      "Average Reward for Agent 7 this episode : -12.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.7002\n",
      "Average Reward for Agent 8 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7261\n",
      "Average Reward for Agent 9 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.6292\n",
      "Average Reward for Agent 10 this episode : -64.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 211.6123\n",
      "Average Reward for Agent 11 this episode : -3.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.5276\n",
      "Average Reward for Agent 12 this episode : -7.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.1970\n",
      "Average Reward for Agent 13 this episode : -38.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.0575\n",
      "Reducing exploration for all agents to 0.007\n",
      "Episode 288 is finished\n",
      "Average Reward for Agent 0 this episode : -37.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.4263\n",
      "Average Reward for Agent 1 this episode : -9.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4006\n",
      "Average Reward for Agent 2 this episode : -39.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.6650\n",
      "Average Reward for Agent 3 this episode : -5.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.2371\n",
      "Average Reward for Agent 4 this episode : -16.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 172.8620\n",
      "Average Reward for Agent 5 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7785\n",
      "Average Reward for Agent 6 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.9654\n",
      "Average Reward for Agent 7 this episode : -13.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.1132\n",
      "Average Reward for Agent 8 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0606\n",
      "Average Reward for Agent 9 this episode : -1.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.3895\n",
      "Average Reward for Agent 10 this episode : -58.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.0520\n",
      "Average Reward for Agent 11 this episode : -2.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.2323\n",
      "Average Reward for Agent 12 this episode : -10.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.8360\n",
      "Average Reward for Agent 13 this episode : -7.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.4870\n",
      "Reducing exploration for all agents to 0.0068\n",
      "Episode 289 is finished\n",
      "Average Reward for Agent 0 this episode : -40.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.0372\n",
      "Average Reward for Agent 1 this episode : -9.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.6829\n",
      "Average Reward for Agent 2 this episode : -35.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 141.8223\n",
      "Average Reward for Agent 3 this episode : -4.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.8555\n",
      "Average Reward for Agent 4 this episode : -16.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.7623\n",
      "Average Reward for Agent 5 this episode : -1.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9433\n",
      "Average Reward for Agent 6 this episode : -0.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9754\n",
      "Average Reward for Agent 7 this episode : -12.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.5133\n",
      "Average Reward for Agent 8 this episode : -0.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0018\n",
      "Average Reward for Agent 9 this episode : -1.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.0379\n",
      "Average Reward for Agent 10 this episode : -49.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 181.4820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 11 this episode : -3.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.2110\n",
      "Average Reward for Agent 12 this episode : -7.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.1326\n",
      "Average Reward for Agent 13 this episode : -7.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 188.5316\n",
      "Reducing exploration for all agents to 0.0067\n",
      "Episode 290 is finished\n",
      "Average Reward for Agent 0 this episode : -13.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.2746\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -14.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.8239\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -54.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.4288\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -4.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8674\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -18.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8029\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.9067\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -12.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.1896\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -15.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0820\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.9876\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -2.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.2698\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -36.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 160.5226\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -3.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.4879\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -7.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.9285\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -22.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.1586\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0066\n",
      "Episode 291 is finished\n",
      "Average Reward for Agent 0 this episode : -42.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 114.2551\n",
      "Average Reward for Agent 1 this episode : -7.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3925\n",
      "Average Reward for Agent 2 this episode : -49.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.6993\n",
      "Average Reward for Agent 3 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0282\n",
      "Average Reward for Agent 4 this episode : -16.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.7335\n",
      "Average Reward for Agent 5 this episode : -1.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3695\n",
      "Average Reward for Agent 6 this episode : -0.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.9260\n",
      "Average Reward for Agent 7 this episode : -12.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.9335\n",
      "Average Reward for Agent 8 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2869\n",
      "Average Reward for Agent 9 this episode : -4.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.6488\n",
      "Average Reward for Agent 10 this episode : -49.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 289.0737\n",
      "Average Reward for Agent 11 this episode : -2.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.5151\n",
      "Average Reward for Agent 12 this episode : -6.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.4406\n",
      "Average Reward for Agent 13 this episode : -49.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.6643\n",
      "Reducing exploration for all agents to 0.0065\n",
      "Episode 292 is finished\n",
      "Average Reward for Agent 0 this episode : -43.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.3495\n",
      "Average Reward for Agent 1 this episode : -10.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 179.0519\n",
      "Average Reward for Agent 2 this episode : -50.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.0737\n",
      "Average Reward for Agent 3 this episode : -5.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4046\n",
      "Average Reward for Agent 4 this episode : -16.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.2765\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0968\n",
      "Average Reward for Agent 6 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.8467\n",
      "Average Reward for Agent 7 this episode : -13.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.5347\n",
      "Average Reward for Agent 8 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4617\n",
      "Average Reward for Agent 9 this episode : -7.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 153.5677\n",
      "Average Reward for Agent 10 this episode : -52.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 108.0750\n",
      "Average Reward for Agent 11 this episode : -2.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.6791\n",
      "Average Reward for Agent 12 this episode : -4.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.3167\n",
      "Average Reward for Agent 13 this episode : -7.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 162.3475\n",
      "Reducing exploration for all agents to 0.0064\n",
      "Episode 293 is finished\n",
      "Average Reward for Agent 0 this episode : -13.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 127.2262\n",
      "Average Reward for Agent 1 this episode : -12.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.1084\n",
      "Average Reward for Agent 2 this episode : -47.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 125.6092\n",
      "Average Reward for Agent 3 this episode : -11.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6579\n",
      "Average Reward for Agent 4 this episode : -18.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4625\n",
      "Average Reward for Agent 5 this episode : -0.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4576\n",
      "Average Reward for Agent 6 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.8144\n",
      "Average Reward for Agent 7 this episode : -12.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.6398\n",
      "Average Reward for Agent 8 this episode : -0.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3425\n",
      "Average Reward for Agent 9 this episode : -16.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 176.5130\n",
      "Average Reward for Agent 10 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 158.5798\n",
      "Average Reward for Agent 11 this episode : -2.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.0984\n",
      "Average Reward for Agent 12 this episode : -7.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.1256\n",
      "Average Reward for Agent 13 this episode : -8.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 117.2377\n",
      "Reducing exploration for all agents to 0.0063\n",
      "Episode 294 is finished\n",
      "Average Reward for Agent 0 this episode : -34.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.3145\n",
      "Average Reward for Agent 1 this episode : -13.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 120.4534\n",
      "Average Reward for Agent 2 this episode : -51.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.8021\n",
      "Average Reward for Agent 3 this episode : -3.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5166\n",
      "Average Reward for Agent 4 this episode : -20.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.0845\n",
      "Average Reward for Agent 5 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9836\n",
      "Average Reward for Agent 6 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2938\n",
      "Average Reward for Agent 7 this episode : -12.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1425\n",
      "Average Reward for Agent 8 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.3183\n",
      "Average Reward for Agent 9 this episode : -38.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 306.3453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 10 this episode : -34.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 245.2687\n",
      "Average Reward for Agent 11 this episode : -2.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.5684\n",
      "Average Reward for Agent 12 this episode : -3.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.5878\n",
      "Average Reward for Agent 13 this episode : -16.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.5106\n",
      "Reducing exploration for all agents to 0.0062\n",
      "Episode 295 is finished\n",
      "Average Reward for Agent 0 this episode : -39.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.9156\n",
      "Average Reward for Agent 1 this episode : -11.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.4404\n",
      "Average Reward for Agent 2 this episode : -46.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.7757\n",
      "Average Reward for Agent 3 this episode : -4.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.5755\n",
      "Average Reward for Agent 4 this episode : -14.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6701\n",
      "Average Reward for Agent 5 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3974\n",
      "Average Reward for Agent 6 this episode : -0.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.4791\n",
      "Average Reward for Agent 7 this episode : -12.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8770\n",
      "Average Reward for Agent 8 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3372\n",
      "Average Reward for Agent 9 this episode : -0.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 245.2676\n",
      "Average Reward for Agent 10 this episode : -35.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.8001\n",
      "Average Reward for Agent 11 this episode : -2.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.4170\n",
      "Average Reward for Agent 12 this episode : -4.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.4576\n",
      "Average Reward for Agent 13 this episode : -7.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.2908\n",
      "Reducing exploration for all agents to 0.0061\n",
      "Episode 296 is finished\n",
      "Average Reward for Agent 0 this episode : -30.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.7039\n",
      "Average Reward for Agent 1 this episode : -12.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.8746\n",
      "Average Reward for Agent 2 this episode : -52.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.1270\n",
      "Average Reward for Agent 3 this episode : -1.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.8314\n",
      "Average Reward for Agent 4 this episode : -17.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.8637\n",
      "Average Reward for Agent 5 this episode : -0.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3626\n",
      "Average Reward for Agent 6 this episode : -0.11\n",
      "Saving architecture, weights, optimizer state for best agent-6\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent6_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4823\n",
      "Average Reward for Agent 7 this episode : -14.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.9720\n",
      "Average Reward for Agent 8 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.3116\n",
      "Average Reward for Agent 9 this episode : -0.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 153.9822\n",
      "Average Reward for Agent 10 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.1281\n",
      "Average Reward for Agent 11 this episode : -2.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.7607\n",
      "Average Reward for Agent 12 this episode : -4.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.1751\n",
      "Average Reward for Agent 13 this episode : -16.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.6222\n",
      "Reducing exploration for all agents to 0.0059\n",
      "Episode 297 is finished\n",
      "Average Reward for Agent 0 this episode : -41.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.4533\n",
      "Average Reward for Agent 1 this episode : -7.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.0531\n",
      "Average Reward for Agent 2 this episode : -38.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.1060\n",
      "Average Reward for Agent 3 this episode : -3.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8338\n",
      "Average Reward for Agent 4 this episode : -18.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.7088\n",
      "Average Reward for Agent 5 this episode : -0.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8598\n",
      "Average Reward for Agent 6 this episode : -11.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.8965\n",
      "Average Reward for Agent 7 this episode : -14.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4911\n",
      "Average Reward for Agent 8 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.7996\n",
      "Average Reward for Agent 9 this episode : -15.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 207.8106\n",
      "Average Reward for Agent 10 this episode : -60.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 256.3701\n",
      "Average Reward for Agent 11 this episode : -3.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.5481\n",
      "Average Reward for Agent 12 this episode : -4.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1885\n",
      "Average Reward for Agent 13 this episode : -1.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.6268\n",
      "Reducing exploration for all agents to 0.0058\n",
      "Episode 298 is finished\n",
      "Average Reward for Agent 0 this episode : -42.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.9333\n",
      "Average Reward for Agent 1 this episode : -12.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.9347\n",
      "Average Reward for Agent 2 this episode : -48.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.2869\n",
      "Average Reward for Agent 3 this episode : -3.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0953\n",
      "Average Reward for Agent 4 this episode : -18.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1586\n",
      "Average Reward for Agent 5 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6860\n",
      "Average Reward for Agent 6 this episode : -14.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.2489\n",
      "Average Reward for Agent 7 this episode : -12.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.8108\n",
      "Average Reward for Agent 8 this episode : -1.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.3589\n",
      "Average Reward for Agent 9 this episode : -16.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.1945\n",
      "Average Reward for Agent 10 this episode : -2.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 174.4465\n",
      "Average Reward for Agent 11 this episode : -3.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.5247\n",
      "Average Reward for Agent 12 this episode : -6.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4633\n",
      "Average Reward for Agent 13 this episode : -6.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.6419\n",
      "Reducing exploration for all agents to 0.0057\n",
      "Episode 299 is finished\n",
      "Average Reward for Agent 0 this episode : -33.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.3256\n",
      "Average Reward for Agent 1 this episode : -13.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 142.4260\n",
      "Average Reward for Agent 2 this episode : -49.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.3520\n",
      "Average Reward for Agent 3 this episode : -2.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2313\n",
      "Average Reward for Agent 4 this episode : -16.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7150\n",
      "Average Reward for Agent 5 this episode : -0.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1118\n",
      "Average Reward for Agent 6 this episode : -1.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 184.8023\n",
      "Average Reward for Agent 7 this episode : -14.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.5618\n",
      "Average Reward for Agent 8 this episode : -1.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.3624\n",
      "Average Reward for Agent 9 this episode : -20.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.9312\n",
      "Average Reward for Agent 10 this episode : -1.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 123.4282\n",
      "Average Reward for Agent 11 this episode : -3.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 194.2527\n",
      "Average Reward for Agent 12 this episode : -6.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9669\n",
      "Average Reward for Agent 13 this episode : -10.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.4627\n",
      "Reducing exploration for all agents to 0.0056\n",
      "Episode 300 is finished\n",
      "Average Reward for Agent 0 this episode : -40.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 80.9223\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -10.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -57.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.1066\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -2.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.8286\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -17.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.1445\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7115\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.7083\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -10.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.0154\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1617\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -22.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.7131\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -15.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 144.2854\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -4.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.9307\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -8.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.7057\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -14.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.2005\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0056\n",
      "Episode 301 is finished\n",
      "Average Reward for Agent 0 this episode : -32.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 126.0463\n",
      "Average Reward for Agent 1 this episode : -12.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6563\n",
      "Average Reward for Agent 2 this episode : -55.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.8301\n",
      "Average Reward for Agent 3 this episode : -5.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3208\n",
      "Average Reward for Agent 4 this episode : -16.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.3813\n",
      "Average Reward for Agent 5 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7985\n",
      "Average Reward for Agent 6 this episode : -0.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.6088\n",
      "Average Reward for Agent 7 this episode : -11.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6371\n",
      "Average Reward for Agent 8 this episode : -1.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2229\n",
      "Average Reward for Agent 9 this episode : -21.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 105.8897\n",
      "Average Reward for Agent 10 this episode : -2.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 240.3017\n",
      "Average Reward for Agent 11 this episode : -8.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 127.3064\n",
      "Average Reward for Agent 12 this episode : -8.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.5879\n",
      "Average Reward for Agent 13 this episode : -9.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.8308\n",
      "Reducing exploration for all agents to 0.0055\n",
      "Episode 302 is finished\n",
      "Average Reward for Agent 0 this episode : -41.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 125.3830\n",
      "Average Reward for Agent 1 this episode : -12.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.3291\n",
      "Average Reward for Agent 2 this episode : -48.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 222.9471\n",
      "Average Reward for Agent 3 this episode : -2.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1237\n",
      "Average Reward for Agent 4 this episode : -17.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5945\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3966\n",
      "Average Reward for Agent 6 this episode : -15.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.0063\n",
      "Average Reward for Agent 7 this episode : -13.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.4428\n",
      "Average Reward for Agent 8 this episode : -1.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.0657\n",
      "Average Reward for Agent 9 this episode : -20.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 150.4942\n",
      "Average Reward for Agent 10 this episode : -6.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 212.5482\n",
      "Average Reward for Agent 11 this episode : -6.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 127.1925\n",
      "Average Reward for Agent 12 this episode : -3.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.2224\n",
      "Average Reward for Agent 13 this episode : -14.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.6758\n",
      "Reducing exploration for all agents to 0.0054\n",
      "Episode 303 is finished\n",
      "Average Reward for Agent 0 this episode : -11.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 138.7246\n",
      "Average Reward for Agent 1 this episode : -15.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.8289\n",
      "Average Reward for Agent 2 this episode : -49.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.9274\n",
      "Average Reward for Agent 3 this episode : -6.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.5426\n",
      "Average Reward for Agent 4 this episode : -15.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.4027\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5971\n",
      "Average Reward for Agent 6 this episode : -0.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.4932\n",
      "Average Reward for Agent 7 this episode : -12.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.1025\n",
      "Average Reward for Agent 8 this episode : -1.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2073\n",
      "Average Reward for Agent 9 this episode : -10.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.9494\n",
      "Average Reward for Agent 10 this episode : -6.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 453.3342\n",
      "Average Reward for Agent 11 this episode : -7.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 225.5532\n",
      "Average Reward for Agent 12 this episode : -3.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.3995\n",
      "Average Reward for Agent 13 this episode : -6.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.9869\n",
      "Reducing exploration for all agents to 0.0053\n",
      "Episode 304 is finished\n",
      "Average Reward for Agent 0 this episode : -11.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.5323\n",
      "Average Reward for Agent 1 this episode : -11.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 164.1879\n",
      "Average Reward for Agent 2 this episode : -50.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.1098\n",
      "Average Reward for Agent 3 this episode : -5.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6973\n",
      "Average Reward for Agent 4 this episode : -18.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.1542\n",
      "Average Reward for Agent 5 this episode : -0.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0626\n",
      "Average Reward for Agent 6 this episode : -1.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.7109\n",
      "Average Reward for Agent 7 this episode : -9.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6524\n",
      "Average Reward for Agent 8 this episode : -2.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2508\n",
      "Average Reward for Agent 9 this episode : -21.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.5651\n",
      "Average Reward for Agent 10 this episode : -55.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 369.2569\n",
      "Average Reward for Agent 11 this episode : -6.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 278.9003\n",
      "Average Reward for Agent 12 this episode : -5.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2744\n",
      "Average Reward for Agent 13 this episode : -8.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.0072\n",
      "Reducing exploration for all agents to 0.0052\n",
      "Episode 305 is finished\n",
      "Average Reward for Agent 0 this episode : -11.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.1279\n",
      "Average Reward for Agent 1 this episode : -7.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.7831\n",
      "Average Reward for Agent 2 this episode : -51.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.3032\n",
      "Average Reward for Agent 3 this episode : -2.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0755\n",
      "Average Reward for Agent 4 this episode : -16.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.2362\n",
      "Average Reward for Agent 5 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8703\n",
      "Average Reward for Agent 6 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.8639\n",
      "Average Reward for Agent 7 this episode : -10.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6683\n",
      "Average Reward for Agent 8 this episode : -2.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.0387\n",
      "Average Reward for Agent 9 this episode : -17.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.3944\n",
      "Average Reward for Agent 10 this episode : -46.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 244.6613\n",
      "Average Reward for Agent 11 this episode : -7.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 223.7248\n",
      "Average Reward for Agent 12 this episode : -7.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7638\n",
      "Average Reward for Agent 13 this episode : -42.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.8698\n",
      "Reducing exploration for all agents to 0.0051\n",
      "Episode 306 is finished\n",
      "Average Reward for Agent 0 this episode : -14.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.1355\n",
      "Average Reward for Agent 1 this episode : -12.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.7079\n",
      "Average Reward for Agent 2 this episode : -49.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.4704\n",
      "Average Reward for Agent 3 this episode : -8.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.4784\n",
      "Average Reward for Agent 4 this episode : -15.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.4985\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5868\n",
      "Average Reward for Agent 6 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0124\n",
      "Average Reward for Agent 7 this episode : -12.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.3670\n",
      "Average Reward for Agent 8 this episode : -3.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.6867\n",
      "Average Reward for Agent 9 this episode : -13.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.8952\n",
      "Average Reward for Agent 10 this episode : -59.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 268.3329\n",
      "Average Reward for Agent 11 this episode : -6.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 234.8077\n",
      "Average Reward for Agent 12 this episode : -4.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2878\n",
      "Average Reward for Agent 13 this episode : -51.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.1956\n",
      "Reducing exploration for all agents to 0.005\n",
      "Episode 307 is finished\n",
      "Average Reward for Agent 0 this episode : -11.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.6466\n",
      "Average Reward for Agent 1 this episode : -11.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.5491\n",
      "Average Reward for Agent 2 this episode : -44.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 162.2886\n",
      "Average Reward for Agent 3 this episode : -0.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1074\n",
      "Average Reward for Agent 4 this episode : -17.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.5403\n",
      "Average Reward for Agent 5 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.2271\n",
      "Average Reward for Agent 6 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7545\n",
      "Average Reward for Agent 7 this episode : -10.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0541\n",
      "Average Reward for Agent 8 this episode : -1.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.5930\n",
      "Average Reward for Agent 9 this episode : -17.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.5740\n",
      "Average Reward for Agent 10 this episode : -60.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.8698\n",
      "Average Reward for Agent 11 this episode : -4.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 210.2153\n",
      "Average Reward for Agent 12 this episode : -3.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.6083\n",
      "Average Reward for Agent 13 this episode : -9.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.2189\n",
      "Reducing exploration for all agents to 0.0049\n",
      "Episode 308 is finished\n",
      "Average Reward for Agent 0 this episode : -11.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.1201\n",
      "Average Reward for Agent 1 this episode : -12.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6073\n",
      "Average Reward for Agent 2 this episode : -44.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.6192\n",
      "Average Reward for Agent 3 this episode : -3.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0500\n",
      "Average Reward for Agent 4 this episode : -18.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.7734\n",
      "Average Reward for Agent 5 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8641\n",
      "Average Reward for Agent 6 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2916\n",
      "Average Reward for Agent 7 this episode : -16.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.8618\n",
      "Average Reward for Agent 8 this episode : -1.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.8538\n",
      "Average Reward for Agent 9 this episode : -17.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.6306\n",
      "Average Reward for Agent 10 this episode : -64.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 227.9055\n",
      "Average Reward for Agent 11 this episode : -3.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.0137\n",
      "Average Reward for Agent 12 this episode : -5.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4359\n",
      "Average Reward for Agent 13 this episode : -27.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.8088\n",
      "Reducing exploration for all agents to 0.0048\n",
      "Episode 309 is finished\n",
      "Average Reward for Agent 0 this episode : -10.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.7698\n",
      "Average Reward for Agent 1 this episode : -13.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.8874\n",
      "Average Reward for Agent 2 this episode : -43.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 122.0096\n",
      "Average Reward for Agent 3 this episode : -2.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7230\n",
      "Average Reward for Agent 4 this episode : -18.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0300\n",
      "Average Reward for Agent 5 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7842\n",
      "Average Reward for Agent 6 this episode : -1.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1730\n",
      "Average Reward for Agent 7 this episode : -14.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.4480\n",
      "Average Reward for Agent 8 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.9072\n",
      "Average Reward for Agent 9 this episode : -19.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.9168\n",
      "Average Reward for Agent 10 this episode : -56.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 126.7975\n",
      "Average Reward for Agent 11 this episode : -4.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 123.2609\n",
      "Average Reward for Agent 12 this episode : -4.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.2184\n",
      "Average Reward for Agent 13 this episode : -48.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.1508\n",
      "Reducing exploration for all agents to 0.0047\n",
      "Episode 310 is finished\n",
      "Average Reward for Agent 0 this episode : -11.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.6179\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -4.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.8001\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -39.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.0516\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -8.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -16.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9620\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3521\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4743\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -14.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8656\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.6580\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -20.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.2396\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -50.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 111.5336\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -3.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 131.3076\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -3.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0261\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -54.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.9520\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0047\n",
      "Episode 311 is finished\n",
      "Average Reward for Agent 0 this episode : -12.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.2159\n",
      "Average Reward for Agent 1 this episode : -12.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.8644\n",
      "Average Reward for Agent 2 this episode : -36.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 300.3206\n",
      "Average Reward for Agent 3 this episode : -4.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7504\n",
      "Average Reward for Agent 4 this episode : -18.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.7391\n",
      "Average Reward for Agent 5 this episode : -0.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5010\n",
      "Average Reward for Agent 6 this episode : -2.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.5213\n",
      "Average Reward for Agent 7 this episode : -12.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.3026\n",
      "Average Reward for Agent 8 this episode : -0.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.2221\n",
      "Average Reward for Agent 9 this episode : -16.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.0694\n",
      "Average Reward for Agent 10 this episode : -48.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 311.1097\n",
      "Average Reward for Agent 11 this episode : -3.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.6927\n",
      "Average Reward for Agent 12 this episode : -7.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.4149\n",
      "Average Reward for Agent 13 this episode : -49.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.9414\n",
      "Reducing exploration for all agents to 0.0046\n",
      "Episode 312 is finished\n",
      "Average Reward for Agent 0 this episode : -24.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.5051\n",
      "Average Reward for Agent 1 this episode : -11.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9724\n",
      "Average Reward for Agent 2 this episode : -37.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 122.6003\n",
      "Average Reward for Agent 3 this episode : -1.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8956\n",
      "Average Reward for Agent 4 this episode : -18.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3692\n",
      "Average Reward for Agent 5 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0452\n",
      "Average Reward for Agent 6 this episode : -3.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.1950\n",
      "Average Reward for Agent 7 this episode : -15.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6481\n",
      "Average Reward for Agent 8 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.0636\n",
      "Average Reward for Agent 9 this episode : -19.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.0671\n",
      "Average Reward for Agent 10 this episode : -29.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 673.4749\n",
      "Average Reward for Agent 11 this episode : -4.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.3149\n",
      "Average Reward for Agent 12 this episode : -4.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2048\n",
      "Average Reward for Agent 13 this episode : -56.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 125.9960\n",
      "Reducing exploration for all agents to 0.0045\n",
      "Episode 313 is finished\n",
      "Average Reward for Agent 0 this episode : -15.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.2768\n",
      "Average Reward for Agent 1 this episode : -13.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.4199\n",
      "Average Reward for Agent 2 this episode : -39.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 358.5567\n",
      "Average Reward for Agent 3 this episode : -0.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6694\n",
      "Average Reward for Agent 4 this episode : -19.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3120\n",
      "Average Reward for Agent 5 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7544\n",
      "Average Reward for Agent 6 this episode : -1.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.9970\n",
      "Average Reward for Agent 7 this episode : -18.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6466\n",
      "Average Reward for Agent 8 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.8006\n",
      "Average Reward for Agent 9 this episode : -10.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.0995\n",
      "Average Reward for Agent 10 this episode : -56.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 339.2770\n",
      "Average Reward for Agent 11 this episode : -3.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.1675\n",
      "Average Reward for Agent 12 this episode : -5.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7389\n",
      "Average Reward for Agent 13 this episode : -55.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.3039\n",
      "Reducing exploration for all agents to 0.0044\n",
      "Episode 314 is finished\n",
      "Average Reward for Agent 0 this episode : -13.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.9919\n",
      "Average Reward for Agent 1 this episode : -12.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.1441\n",
      "Average Reward for Agent 2 this episode : -43.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.7263\n",
      "Average Reward for Agent 3 this episode : -5.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5680\n",
      "Average Reward for Agent 4 this episode : -18.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.1176\n",
      "Average Reward for Agent 5 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9030\n",
      "Average Reward for Agent 6 this episode : -1.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.8605\n",
      "Average Reward for Agent 7 this episode : -13.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2532\n",
      "Average Reward for Agent 8 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.8120\n",
      "Average Reward for Agent 9 this episode : -6.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.9764\n",
      "Average Reward for Agent 10 this episode : -61.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 410.7097\n",
      "Average Reward for Agent 11 this episode : -4.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.0845\n",
      "Average Reward for Agent 12 this episode : -4.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.8110\n",
      "Average Reward for Agent 13 this episode : -47.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 172.9228\n",
      "Reducing exploration for all agents to 0.0044\n",
      "Episode 315 is finished\n",
      "Average Reward for Agent 0 this episode : -13.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.8810\n",
      "Average Reward for Agent 1 this episode : -7.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9345\n",
      "Average Reward for Agent 2 this episode : -43.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.3791\n",
      "Average Reward for Agent 3 this episode : -4.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9215\n",
      "Average Reward for Agent 4 this episode : -18.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.7825\n",
      "Average Reward for Agent 5 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5238\n",
      "Average Reward for Agent 6 this episode : -1.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.4435\n",
      "Average Reward for Agent 7 this episode : -10.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.9769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 8 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1288\n",
      "Average Reward for Agent 9 this episode : -17.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.7192\n",
      "Average Reward for Agent 10 this episode : -65.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 723.8567\n",
      "Average Reward for Agent 11 this episode : -4.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.2397\n",
      "Average Reward for Agent 12 this episode : -7.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6661\n",
      "Average Reward for Agent 13 this episode : -44.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 52.4700\n",
      "Reducing exploration for all agents to 0.0043\n",
      "Episode 316 is finished\n",
      "Average Reward for Agent 0 this episode : -13.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.7002\n",
      "Average Reward for Agent 1 this episode : -7.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6099\n",
      "Average Reward for Agent 2 this episode : -44.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.2907\n",
      "Average Reward for Agent 3 this episode : -5.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.0213\n",
      "Average Reward for Agent 4 this episode : -17.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3981\n",
      "Average Reward for Agent 5 this episode : -0.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1917\n",
      "Average Reward for Agent 6 this episode : -1.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6535\n",
      "Average Reward for Agent 7 this episode : -13.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2290\n",
      "Average Reward for Agent 8 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9663\n",
      "Average Reward for Agent 9 this episode : -17.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 133.9369\n",
      "Average Reward for Agent 10 this episode : -56.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 330.3774\n",
      "Average Reward for Agent 11 this episode : -5.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.2120\n",
      "Average Reward for Agent 12 this episode : -6.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.1364\n",
      "Average Reward for Agent 13 this episode : -46.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 133.9089\n",
      "Reducing exploration for all agents to 0.0042\n",
      "Episode 317 is finished\n",
      "Average Reward for Agent 0 this episode : -13.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.2928\n",
      "Average Reward for Agent 1 this episode : -13.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6849\n",
      "Average Reward for Agent 2 this episode : -43.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.7112\n",
      "Average Reward for Agent 3 this episode : -1.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5667\n",
      "Average Reward for Agent 4 this episode : -21.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.4668\n",
      "Average Reward for Agent 5 this episode : -0.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1535\n",
      "Average Reward for Agent 6 this episode : -0.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.6291\n",
      "Average Reward for Agent 7 this episode : -11.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2949\n",
      "Average Reward for Agent 8 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3746\n",
      "Average Reward for Agent 9 this episode : -21.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.1595\n",
      "Average Reward for Agent 10 this episode : -48.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 426.9586\n",
      "Average Reward for Agent 11 this episode : -7.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 130.8644\n",
      "Average Reward for Agent 12 this episode : -6.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.7655\n",
      "Average Reward for Agent 13 this episode : -40.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.0003\n",
      "Reducing exploration for all agents to 0.0041\n",
      "Episode 318 is finished\n",
      "Average Reward for Agent 0 this episode : -14.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.1291\n",
      "Average Reward for Agent 1 this episode : -7.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.9264\n",
      "Average Reward for Agent 2 this episode : -40.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.5451\n",
      "Average Reward for Agent 3 this episode : -1.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4184\n",
      "Average Reward for Agent 4 this episode : -17.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0235\n",
      "Average Reward for Agent 5 this episode : -0.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5328\n",
      "Average Reward for Agent 6 this episode : -0.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1324\n",
      "Average Reward for Agent 7 this episode : -8.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.8796\n",
      "Average Reward for Agent 8 this episode : -1.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8127\n",
      "Average Reward for Agent 9 this episode : -13.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.0400\n",
      "Average Reward for Agent 10 this episode : -56.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 243.3979\n",
      "Average Reward for Agent 11 this episode : -10.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 174.7101\n",
      "Average Reward for Agent 12 this episode : -8.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.3990\n",
      "Average Reward for Agent 13 this episode : -45.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.9826\n",
      "Reducing exploration for all agents to 0.0041\n",
      "Episode 319 is finished\n",
      "Average Reward for Agent 0 this episode : -13.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.1379\n",
      "Average Reward for Agent 1 this episode : -7.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 141.2152\n",
      "Average Reward for Agent 2 this episode : -46.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.8465\n",
      "Average Reward for Agent 3 this episode : -2.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9504\n",
      "Average Reward for Agent 4 this episode : -16.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0575\n",
      "Average Reward for Agent 5 this episode : -0.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4684\n",
      "Average Reward for Agent 6 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3602\n",
      "Average Reward for Agent 7 this episode : -11.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9191\n",
      "Average Reward for Agent 8 this episode : -0.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1609\n",
      "Average Reward for Agent 9 this episode : -19.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6576\n",
      "Average Reward for Agent 10 this episode : -53.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 340.1475\n",
      "Average Reward for Agent 11 this episode : -11.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 187.8068\n",
      "Average Reward for Agent 12 this episode : -5.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.2512\n",
      "Average Reward for Agent 13 this episode : -48.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.9665\n",
      "Reducing exploration for all agents to 0.004\n",
      "Episode 320 is finished\n",
      "Average Reward for Agent 0 this episode : -15.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.8574\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -4.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5825\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -38.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.6579\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -2.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.7660\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -17.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4057\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1188\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7335\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -9.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.9710\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8825\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -14.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.1913\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -43.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 115.0964\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -6.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 118.8837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -6.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7631\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -41.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.9395\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0039\n",
      "Episode 321 is finished\n",
      "Average Reward for Agent 0 this episode : -11.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.9506\n",
      "Average Reward for Agent 1 this episode : -10.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7442\n",
      "Average Reward for Agent 2 this episode : -40.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.3392\n",
      "Average Reward for Agent 3 this episode : -4.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8531\n",
      "Average Reward for Agent 4 this episode : -19.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.0411\n",
      "Average Reward for Agent 5 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6704\n",
      "Average Reward for Agent 6 this episode : -1.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2533\n",
      "Average Reward for Agent 7 this episode : -9.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.2594\n",
      "Average Reward for Agent 8 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3918\n",
      "Average Reward for Agent 9 this episode : -17.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.4697\n",
      "Average Reward for Agent 10 this episode : -45.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 234.1531\n",
      "Average Reward for Agent 11 this episode : -1.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 172.0941\n",
      "Average Reward for Agent 12 this episode : -5.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.7124\n",
      "Average Reward for Agent 13 this episode : -48.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.2291\n",
      "Reducing exploration for all agents to 0.0039\n",
      "Episode 322 is finished\n",
      "Average Reward for Agent 0 this episode : -17.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.4813\n",
      "Average Reward for Agent 1 this episode : -4.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.0673\n",
      "Average Reward for Agent 2 this episode : -44.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.2849\n",
      "Average Reward for Agent 3 this episode : -5.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5356\n",
      "Average Reward for Agent 4 this episode : -14.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3445\n",
      "Average Reward for Agent 5 this episode : -0.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.2230\n",
      "Average Reward for Agent 6 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7028\n",
      "Average Reward for Agent 7 this episode : -11.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6728\n",
      "Average Reward for Agent 8 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5980\n",
      "Average Reward for Agent 9 this episode : -13.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.9457\n",
      "Average Reward for Agent 10 this episode : -34.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 633.5563\n",
      "Average Reward for Agent 11 this episode : -1.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.4438\n",
      "Average Reward for Agent 12 this episode : -4.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.8841\n",
      "Average Reward for Agent 13 this episode : -39.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.2902\n",
      "Reducing exploration for all agents to 0.0038\n",
      "Episode 323 is finished\n",
      "Average Reward for Agent 0 this episode : -22.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.1942\n",
      "Average Reward for Agent 1 this episode : -11.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7749\n",
      "Average Reward for Agent 2 this episode : -40.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.9249\n",
      "Average Reward for Agent 3 this episode : -3.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.2107\n",
      "Average Reward for Agent 4 this episode : -13.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4643\n",
      "Average Reward for Agent 5 this episode : -0.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9562\n",
      "Average Reward for Agent 6 this episode : -1.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5473\n",
      "Average Reward for Agent 7 this episode : -13.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.9488\n",
      "Average Reward for Agent 8 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.1189\n",
      "Average Reward for Agent 9 this episode : -3.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.4613\n",
      "Average Reward for Agent 10 this episode : -35.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 413.8347\n",
      "Average Reward for Agent 11 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 125.8063\n",
      "Average Reward for Agent 12 this episode : -6.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.9943\n",
      "Average Reward for Agent 13 this episode : -44.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.3489\n",
      "Reducing exploration for all agents to 0.0037\n",
      "Episode 324 is finished\n",
      "Average Reward for Agent 0 this episode : -18.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.9257\n",
      "Average Reward for Agent 1 this episode : -10.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6437\n",
      "Average Reward for Agent 2 this episode : -41.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.2708\n",
      "Average Reward for Agent 3 this episode : -2.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1771\n",
      "Average Reward for Agent 4 this episode : -18.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.3343\n",
      "Average Reward for Agent 5 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9256\n",
      "Average Reward for Agent 6 this episode : -4.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9249\n",
      "Average Reward for Agent 7 this episode : -11.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4101\n",
      "Average Reward for Agent 8 this episode : -0.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4682\n",
      "Average Reward for Agent 9 this episode : -1.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.9630\n",
      "Average Reward for Agent 10 this episode : -38.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 227.3518\n",
      "Average Reward for Agent 11 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 89.9447\n",
      "Average Reward for Agent 12 this episode : -8.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.9292\n",
      "Average Reward for Agent 13 this episode : -42.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.7579\n",
      "Reducing exploration for all agents to 0.0037\n",
      "Episode 325 is finished\n",
      "Average Reward for Agent 0 this episode : -15.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.7985\n",
      "Average Reward for Agent 1 this episode : -5.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.2968\n",
      "Average Reward for Agent 2 this episode : -53.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.6891\n",
      "Average Reward for Agent 3 this episode : -5.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.1065\n",
      "Average Reward for Agent 4 this episode : -23.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.3020\n",
      "Average Reward for Agent 5 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9221\n",
      "Average Reward for Agent 6 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.0670\n",
      "Average Reward for Agent 7 this episode : -12.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2175\n",
      "Average Reward for Agent 8 this episode : -0.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5177\n",
      "Average Reward for Agent 9 this episode : -1.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.8501\n",
      "Average Reward for Agent 10 this episode : -45.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 398.2901\n",
      "Average Reward for Agent 11 this episode : -0.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.6207\n",
      "Average Reward for Agent 12 this episode : -5.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7988\n",
      "Average Reward for Agent 13 this episode : -44.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.3125\n",
      "Reducing exploration for all agents to 0.0036\n",
      "Episode 326 is finished\n",
      "Average Reward for Agent 0 this episode : -13.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.4284\n",
      "Average Reward for Agent 1 this episode : -6.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8600\n",
      "Average Reward for Agent 2 this episode : -44.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 44.5228\n",
      "Average Reward for Agent 3 this episode : -1.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.8696\n",
      "Average Reward for Agent 4 this episode : -21.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.2258\n",
      "Average Reward for Agent 5 this episode : -0.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 6 this episode : -1.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5271\n",
      "Average Reward for Agent 7 this episode : -10.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8187\n",
      "Average Reward for Agent 8 this episode : -0.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7257\n",
      "Average Reward for Agent 9 this episode : -2.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.0019\n",
      "Average Reward for Agent 10 this episode : -51.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 523.4622\n",
      "Average Reward for Agent 11 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.3468\n",
      "Average Reward for Agent 12 this episode : -5.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.5680\n",
      "Average Reward for Agent 13 this episode : -45.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.9653\n",
      "Reducing exploration for all agents to 0.0035\n",
      "Episode 327 is finished\n",
      "Average Reward for Agent 0 this episode : -12.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.7771\n",
      "Average Reward for Agent 1 this episode : -13.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.1408\n",
      "Average Reward for Agent 2 this episode : -46.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.6689\n",
      "Average Reward for Agent 3 this episode : -3.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.8458\n",
      "Average Reward for Agent 4 this episode : -19.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.7410\n",
      "Average Reward for Agent 5 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4822\n",
      "Average Reward for Agent 6 this episode : -2.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.0828\n",
      "Average Reward for Agent 7 this episode : -14.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7132\n",
      "Average Reward for Agent 8 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8951\n",
      "Average Reward for Agent 9 this episode : -3.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.9915\n",
      "Average Reward for Agent 10 this episode : -56.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.3598\n",
      "Average Reward for Agent 11 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.2121\n",
      "Average Reward for Agent 12 this episode : -7.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.7976\n",
      "Average Reward for Agent 13 this episode : -44.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.4113\n",
      "Reducing exploration for all agents to 0.0035\n",
      "Episode 328 is finished\n",
      "Average Reward for Agent 0 this episode : -14.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.7576\n",
      "Average Reward for Agent 1 this episode : -8.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.9708\n",
      "Average Reward for Agent 2 this episode : -39.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.9596\n",
      "Average Reward for Agent 3 this episode : -9.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1691\n",
      "Average Reward for Agent 4 this episode : -20.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.5649\n",
      "Average Reward for Agent 5 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2743\n",
      "Average Reward for Agent 6 this episode : -1.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8316\n",
      "Average Reward for Agent 7 this episode : -12.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.3717\n",
      "Average Reward for Agent 8 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8803\n",
      "Average Reward for Agent 9 this episode : -9.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 89.9178\n",
      "Average Reward for Agent 10 this episode : -53.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 583.2089\n",
      "Average Reward for Agent 11 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.5122\n",
      "Average Reward for Agent 12 this episode : -4.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4792\n",
      "Average Reward for Agent 13 this episode : -53.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.3783\n",
      "Reducing exploration for all agents to 0.0034\n",
      "Episode 329 is finished\n",
      "Average Reward for Agent 0 this episode : -13.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.2196\n",
      "Average Reward for Agent 1 this episode : -9.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.5981\n",
      "Average Reward for Agent 2 this episode : -42.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.3924\n",
      "Average Reward for Agent 3 this episode : -3.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5522\n",
      "Average Reward for Agent 4 this episode : -17.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2242\n",
      "Average Reward for Agent 5 this episode : -0.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6243\n",
      "Average Reward for Agent 6 this episode : -2.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6315\n",
      "Average Reward for Agent 7 this episode : -12.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.1177\n",
      "Average Reward for Agent 8 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6327\n",
      "Average Reward for Agent 9 this episode : -17.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.6787\n",
      "Average Reward for Agent 10 this episode : -55.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 331.2067\n",
      "Average Reward for Agent 11 this episode : -63.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 243.4646\n",
      "Average Reward for Agent 12 this episode : -6.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.8012\n",
      "Average Reward for Agent 13 this episode : -46.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.2258\n",
      "Reducing exploration for all agents to 0.0034\n",
      "Episode 330 is finished\n",
      "Average Reward for Agent 0 this episode : -13.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.1826\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -9.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.0323\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -40.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.9949\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -4.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.4230\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -19.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.9202\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4470\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7746\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -13.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0798\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9276\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -23.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.6517\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -52.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 472.8186\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -2.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.4824\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -5.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.1559\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -58.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.4941\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0033\n",
      "Episode 331 is finished\n",
      "Average Reward for Agent 0 this episode : -15.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.7188\n",
      "Average Reward for Agent 1 this episode : -10.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.1609\n",
      "Average Reward for Agent 2 this episode : -49.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.7600\n",
      "Average Reward for Agent 3 this episode : -2.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6346\n",
      "Average Reward for Agent 4 this episode : -17.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 250.6045\n",
      "Average Reward for Agent 5 this episode : -0.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8191\n",
      "Average Reward for Agent 6 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3344\n",
      "Average Reward for Agent 7 this episode : -10.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.5796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 8 this episode : -1.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4871\n",
      "Average Reward for Agent 9 this episode : -20.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.1022\n",
      "Average Reward for Agent 10 this episode : -37.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 325.9182\n",
      "Average Reward for Agent 11 this episode : -17.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 330.0278\n",
      "Average Reward for Agent 12 this episode : -11.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.0952\n",
      "Average Reward for Agent 13 this episode : -48.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.9236\n",
      "Reducing exploration for all agents to 0.0032\n",
      "Episode 332 is finished\n",
      "Average Reward for Agent 0 this episode : -17.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.3880\n",
      "Average Reward for Agent 1 this episode : -8.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2519\n",
      "Average Reward for Agent 2 this episode : -49.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.4855\n",
      "Average Reward for Agent 3 this episode : -3.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.4884\n",
      "Average Reward for Agent 4 this episode : -20.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.2954\n",
      "Average Reward for Agent 5 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2936\n",
      "Average Reward for Agent 6 this episode : -1.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4216\n",
      "Average Reward for Agent 7 this episode : -11.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8128\n",
      "Average Reward for Agent 8 this episode : -1.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.2693\n",
      "Average Reward for Agent 9 this episode : -12.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.5810\n",
      "Average Reward for Agent 10 this episode : -48.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 237.9923\n",
      "Average Reward for Agent 11 this episode : -34.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 403.5765\n",
      "Average Reward for Agent 12 this episode : -5.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.2110\n",
      "Average Reward for Agent 13 this episode : -15.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 151.5057\n",
      "Reducing exploration for all agents to 0.0032\n",
      "Episode 333 is finished\n",
      "Average Reward for Agent 0 this episode : -12.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.6150\n",
      "Average Reward for Agent 1 this episode : -13.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.1645\n",
      "Average Reward for Agent 2 this episode : -58.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.5477\n",
      "Average Reward for Agent 3 this episode : -2.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1351\n",
      "Average Reward for Agent 4 this episode : -14.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.5084\n",
      "Average Reward for Agent 5 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9131\n",
      "Average Reward for Agent 6 this episode : -1.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8713\n",
      "Average Reward for Agent 7 this episode : -11.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5694\n",
      "Average Reward for Agent 8 this episode : -1.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.7224\n",
      "Average Reward for Agent 9 this episode : -2.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.3312\n",
      "Average Reward for Agent 10 this episode : -41.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 154.4376\n",
      "Average Reward for Agent 11 this episode : -34.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.3830\n",
      "Average Reward for Agent 12 this episode : -8.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.8866\n",
      "Average Reward for Agent 13 this episode : -11.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.9487\n",
      "Reducing exploration for all agents to 0.0031\n",
      "Episode 334 is finished\n",
      "Average Reward for Agent 0 this episode : -19.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.0419\n",
      "Average Reward for Agent 1 this episode : -10.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0225\n",
      "Average Reward for Agent 2 this episode : -48.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.1037\n",
      "Average Reward for Agent 3 this episode : -2.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6353\n",
      "Average Reward for Agent 4 this episode : -18.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.3970\n",
      "Average Reward for Agent 5 this episode : -0.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4217\n",
      "Average Reward for Agent 6 this episode : -1.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5548\n",
      "Average Reward for Agent 7 this episode : -11.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.7274\n",
      "Average Reward for Agent 8 this episode : -1.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.3034\n",
      "Average Reward for Agent 9 this episode : -2.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.8197\n",
      "Average Reward for Agent 10 this episode : -36.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 255.3861\n",
      "Average Reward for Agent 11 this episode : -32.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.7810\n",
      "Average Reward for Agent 12 this episode : -3.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9929\n",
      "Average Reward for Agent 13 this episode : -34.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 89.8830\n",
      "Reducing exploration for all agents to 0.0031\n",
      "Episode 335 is finished\n",
      "Average Reward for Agent 0 this episode : -19.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.5330\n",
      "Average Reward for Agent 1 this episode : -5.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9823\n",
      "Average Reward for Agent 2 this episode : -42.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.2589\n",
      "Average Reward for Agent 3 this episode : -2.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7528\n",
      "Average Reward for Agent 4 this episode : -14.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4908\n",
      "Average Reward for Agent 5 this episode : -0.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4836\n",
      "Average Reward for Agent 6 this episode : -0.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0253\n",
      "Average Reward for Agent 7 this episode : -12.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2147\n",
      "Average Reward for Agent 8 this episode : -1.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.8811\n",
      "Average Reward for Agent 9 this episode : -7.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 155.4386\n",
      "Average Reward for Agent 10 this episode : -48.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 235.7023\n",
      "Average Reward for Agent 11 this episode : -26.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.9962\n",
      "Average Reward for Agent 12 this episode : -3.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.8120\n",
      "Average Reward for Agent 13 this episode : -44.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.5870\n",
      "Reducing exploration for all agents to 0.003\n",
      "Episode 336 is finished\n",
      "Average Reward for Agent 0 this episode : -15.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.2107\n",
      "Average Reward for Agent 1 this episode : -12.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.4050\n",
      "Average Reward for Agent 2 this episode : -48.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.7326\n",
      "Average Reward for Agent 3 this episode : -3.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3233\n",
      "Average Reward for Agent 4 this episode : -17.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0684\n",
      "Average Reward for Agent 5 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0623\n",
      "Average Reward for Agent 6 this episode : -12.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.5019\n",
      "Average Reward for Agent 7 this episode : -13.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4099\n",
      "Average Reward for Agent 8 this episode : -1.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.6574\n",
      "Average Reward for Agent 9 this episode : -13.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.5490\n",
      "Average Reward for Agent 10 this episode : -45.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 363.1840\n",
      "Average Reward for Agent 11 this episode : -26.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 238.2942\n",
      "Average Reward for Agent 12 this episode : -4.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.8672\n",
      "Average Reward for Agent 13 this episode : -31.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.1014\n",
      "Reducing exploration for all agents to 0.003\n",
      "Episode 337 is finished\n",
      "Average Reward for Agent 0 this episode : -13.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.3876\n",
      "Average Reward for Agent 1 this episode : -11.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4693\n",
      "Average Reward for Agent 2 this episode : -48.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 135.4769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 3 this episode : -2.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2123\n",
      "Average Reward for Agent 4 this episode : -16.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.2919\n",
      "Average Reward for Agent 5 this episode : -0.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8828\n",
      "Average Reward for Agent 6 this episode : -1.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.9427\n",
      "Average Reward for Agent 7 this episode : -11.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6767\n",
      "Average Reward for Agent 8 this episode : -1.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9053\n",
      "Average Reward for Agent 9 this episode : -18.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.0566\n",
      "Average Reward for Agent 10 this episode : -38.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 299.6945\n",
      "Average Reward for Agent 11 this episode : -27.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 133.7649\n",
      "Average Reward for Agent 12 this episode : -4.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9986\n",
      "Average Reward for Agent 13 this episode : -10.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.7655\n",
      "Reducing exploration for all agents to 0.0029\n",
      "Episode 338 is finished\n",
      "Average Reward for Agent 0 this episode : -18.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.2311\n",
      "Average Reward for Agent 1 this episode : -13.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.5689\n",
      "Average Reward for Agent 2 this episode : -46.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 152.2754\n",
      "Average Reward for Agent 3 this episode : -2.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5648\n",
      "Average Reward for Agent 4 this episode : -16.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.8830\n",
      "Average Reward for Agent 5 this episode : -0.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7420\n",
      "Average Reward for Agent 6 this episode : -1.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.5395\n",
      "Average Reward for Agent 7 this episode : -9.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7614\n",
      "Average Reward for Agent 8 this episode : -1.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.7521\n",
      "Average Reward for Agent 9 this episode : -21.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 141.2212\n",
      "Average Reward for Agent 10 this episode : -39.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 149.2630\n",
      "Average Reward for Agent 11 this episode : -30.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.4637\n",
      "Average Reward for Agent 12 this episode : -4.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7488\n",
      "Average Reward for Agent 13 this episode : -5.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.9646\n",
      "Reducing exploration for all agents to 0.0029\n",
      "Episode 339 is finished\n",
      "Average Reward for Agent 0 this episode : -27.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.5065\n",
      "Average Reward for Agent 1 this episode : -10.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7095\n",
      "Average Reward for Agent 2 this episode : -51.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.1306\n",
      "Average Reward for Agent 3 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2240\n",
      "Average Reward for Agent 4 this episode : -13.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.3595\n",
      "Average Reward for Agent 5 this episode : -0.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6438\n",
      "Average Reward for Agent 6 this episode : -1.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.9424\n",
      "Average Reward for Agent 7 this episode : -11.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.9014\n",
      "Average Reward for Agent 8 this episode : -1.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.1872\n",
      "Average Reward for Agent 9 this episode : -20.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.7854\n",
      "Average Reward for Agent 10 this episode : -27.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 354.8549\n",
      "Average Reward for Agent 11 this episode : -28.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.5489\n",
      "Average Reward for Agent 12 this episode : -3.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2541\n",
      "Average Reward for Agent 13 this episode : -8.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.8435\n",
      "Reducing exploration for all agents to 0.0028\n",
      "Episode 340 is finished\n",
      "Average Reward for Agent 0 this episode : -22.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 31.6620\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -7.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.4515\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -47.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.8919\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -3.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7391\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -20.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.5917\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4275\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.9312\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -13.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0260\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2585\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -13.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.5678\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -36.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 319.7881\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -27.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.8925\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -6.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1838\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -10.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.4698\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0028\n",
      "Episode 341 is finished\n",
      "Average Reward for Agent 0 this episode : -24.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.1677\n",
      "Average Reward for Agent 1 this episode : -10.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.6933\n",
      "Average Reward for Agent 2 this episode : -46.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.3611\n",
      "Average Reward for Agent 3 this episode : -4.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4099\n",
      "Average Reward for Agent 4 this episode : -20.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.1787\n",
      "Average Reward for Agent 5 this episode : -0.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1009\n",
      "Average Reward for Agent 6 this episode : -1.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5666\n",
      "Average Reward for Agent 7 this episode : -12.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.1117\n",
      "Average Reward for Agent 8 this episode : -1.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1317\n",
      "Average Reward for Agent 9 this episode : -5.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 67.3004\n",
      "Average Reward for Agent 10 this episode : -23.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 390.8648\n",
      "Average Reward for Agent 11 this episode : -22.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.8942\n",
      "Average Reward for Agent 12 this episode : -6.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3703\n",
      "Average Reward for Agent 13 this episode : -33.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.1199\n",
      "Reducing exploration for all agents to 0.0027\n",
      "Episode 342 is finished\n",
      "Average Reward for Agent 0 this episode : -26.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.3745\n",
      "Average Reward for Agent 1 this episode : -9.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6324\n",
      "Average Reward for Agent 2 this episode : -59.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.7747\n",
      "Average Reward for Agent 3 this episode : -3.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4893\n",
      "Average Reward for Agent 4 this episode : -19.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 28.6549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 5 this episode : -0.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6744\n",
      "Average Reward for Agent 6 this episode : -0.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2126\n",
      "Average Reward for Agent 7 this episode : -10.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.2357\n",
      "Average Reward for Agent 8 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4894\n",
      "Average Reward for Agent 9 this episode : -1.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.3794\n",
      "Average Reward for Agent 10 this episode : -21.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 173.5049\n",
      "Average Reward for Agent 11 this episode : -23.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 460.1569\n",
      "Average Reward for Agent 12 this episode : -5.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.5318\n",
      "Average Reward for Agent 13 this episode : -35.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.2573\n",
      "Reducing exploration for all agents to 0.0027\n",
      "Episode 343 is finished\n",
      "Average Reward for Agent 0 this episode : -31.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.5477\n",
      "Average Reward for Agent 1 this episode : -10.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4711\n",
      "Average Reward for Agent 2 this episode : -46.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.0444\n",
      "Average Reward for Agent 3 this episode : -3.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.3256\n",
      "Average Reward for Agent 4 this episode : -17.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.9263\n",
      "Average Reward for Agent 5 this episode : -0.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9110\n",
      "Average Reward for Agent 6 this episode : -1.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2803\n",
      "Average Reward for Agent 7 this episode : -12.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7651\n",
      "Average Reward for Agent 8 this episode : -0.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.4349\n",
      "Average Reward for Agent 9 this episode : -1.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.8598\n",
      "Average Reward for Agent 10 this episode : -15.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 321.7771\n",
      "Average Reward for Agent 11 this episode : -28.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.9460\n",
      "Average Reward for Agent 12 this episode : -7.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6710\n",
      "Average Reward for Agent 13 this episode : -46.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.3405\n",
      "Reducing exploration for all agents to 0.0026\n",
      "Episode 344 is finished\n",
      "Average Reward for Agent 0 this episode : -38.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.4542\n",
      "Average Reward for Agent 1 this episode : -10.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.4901\n",
      "Average Reward for Agent 2 this episode : -49.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 30.1073\n",
      "Average Reward for Agent 3 this episode : -1.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 22.1194\n",
      "Average Reward for Agent 4 this episode : -15.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.7320\n",
      "Average Reward for Agent 5 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8238\n",
      "Average Reward for Agent 6 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0534\n",
      "Average Reward for Agent 7 this episode : -14.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.1618\n",
      "Average Reward for Agent 8 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6834\n",
      "Average Reward for Agent 9 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 168.6930\n",
      "Average Reward for Agent 10 this episode : -14.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 205.8251\n",
      "Average Reward for Agent 11 this episode : -37.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 374.2917\n",
      "Average Reward for Agent 12 this episode : -7.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3949\n",
      "Average Reward for Agent 13 this episode : -8.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.5915\n",
      "Reducing exploration for all agents to 0.0026\n",
      "Episode 345 is finished\n",
      "Average Reward for Agent 0 this episode : -39.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.7972\n",
      "Average Reward for Agent 1 this episode : -10.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4600\n",
      "Average Reward for Agent 2 this episode : -50.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.1807\n",
      "Average Reward for Agent 3 this episode : -2.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6201\n",
      "Average Reward for Agent 4 this episode : -17.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.7352\n",
      "Average Reward for Agent 5 this episode : -1.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9546\n",
      "Average Reward for Agent 6 this episode : -0.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.4383\n",
      "Average Reward for Agent 7 this episode : -13.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 16.9347\n",
      "Average Reward for Agent 8 this episode : -0.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.4135\n",
      "Average Reward for Agent 9 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.5258\n",
      "Average Reward for Agent 10 this episode : -35.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 222.3351\n",
      "Average Reward for Agent 11 this episode : -13.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 69.4160\n",
      "Average Reward for Agent 12 this episode : -7.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0855\n",
      "Average Reward for Agent 13 this episode : -17.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 124.5698\n",
      "Reducing exploration for all agents to 0.0025\n",
      "Episode 346 is finished\n",
      "Average Reward for Agent 0 this episode : -39.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.2647\n",
      "Average Reward for Agent 1 this episode : -11.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.9006\n",
      "Average Reward for Agent 2 this episode : -52.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.6444\n",
      "Average Reward for Agent 3 this episode : -2.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0043\n",
      "Average Reward for Agent 4 this episode : -15.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.7962\n",
      "Average Reward for Agent 5 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1054\n",
      "Average Reward for Agent 6 this episode : -0.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1287\n",
      "Average Reward for Agent 7 this episode : -12.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3112\n",
      "Average Reward for Agent 8 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.5887\n",
      "Average Reward for Agent 9 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.7243\n",
      "Average Reward for Agent 10 this episode : -41.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 148.1075\n",
      "Average Reward for Agent 11 this episode : -11.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 156.5894\n",
      "Average Reward for Agent 12 this episode : -8.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2773\n",
      "Average Reward for Agent 13 this episode : -10.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.5100\n",
      "Reducing exploration for all agents to 0.0025\n",
      "Episode 347 is finished\n",
      "Average Reward for Agent 0 this episode : -34.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.7517\n",
      "Average Reward for Agent 1 this episode : -12.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6009\n",
      "Average Reward for Agent 2 this episode : -46.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.7611\n",
      "Average Reward for Agent 3 this episode : -4.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.5946\n",
      "Average Reward for Agent 4 this episode : -15.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.1012\n",
      "Average Reward for Agent 5 this episode : -0.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4640\n",
      "Average Reward for Agent 6 this episode : -1.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.4968\n",
      "Average Reward for Agent 7 this episode : -14.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4900\n",
      "Average Reward for Agent 8 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0022\n",
      "Average Reward for Agent 9 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.7976\n",
      "Average Reward for Agent 10 this episode : -3.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 213.8887\n",
      "Average Reward for Agent 11 this episode : -32.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 175.3300\n",
      "Average Reward for Agent 12 this episode : -7.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0378\n",
      "Average Reward for Agent 13 this episode : -38.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 77.8022\n",
      "Reducing exploration for all agents to 0.0025\n",
      "Episode 348 is finished\n",
      "Average Reward for Agent 0 this episode : -38.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.3355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 1 this episode : -6.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.2063\n",
      "Average Reward for Agent 2 this episode : -45.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.2195\n",
      "Average Reward for Agent 3 this episode : -2.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.4995\n",
      "Average Reward for Agent 4 this episode : -17.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2809\n",
      "Average Reward for Agent 5 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.5360\n",
      "Average Reward for Agent 6 this episode : -0.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.7351\n",
      "Average Reward for Agent 7 this episode : -11.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 20.2113\n",
      "Average Reward for Agent 8 this episode : -1.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.4788\n",
      "Average Reward for Agent 9 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.3604\n",
      "Average Reward for Agent 10 this episode : -9.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 277.5350\n",
      "Average Reward for Agent 11 this episode : -35.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.1480\n",
      "Average Reward for Agent 12 this episode : -3.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7239\n",
      "Average Reward for Agent 13 this episode : -38.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.4522\n",
      "Reducing exploration for all agents to 0.0024\n",
      "Episode 349 is finished\n",
      "Average Reward for Agent 0 this episode : -33.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.5829\n",
      "Average Reward for Agent 1 this episode : -10.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.4243\n",
      "Average Reward for Agent 2 this episode : -50.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.3518\n",
      "Average Reward for Agent 3 this episode : -1.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.5507\n",
      "Average Reward for Agent 4 this episode : -18.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.4105\n",
      "Average Reward for Agent 5 this episode : -0.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0766\n",
      "Average Reward for Agent 6 this episode : -1.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1517\n",
      "Average Reward for Agent 7 this episode : -10.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3821\n",
      "Average Reward for Agent 8 this episode : -1.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0613\n",
      "Average Reward for Agent 9 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.7738\n",
      "Average Reward for Agent 10 this episode : -45.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 211.2412\n",
      "Average Reward for Agent 11 this episode : -6.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 194.0010\n",
      "Average Reward for Agent 12 this episode : -5.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.8899\n",
      "Average Reward for Agent 13 this episode : -6.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.8044\n",
      "Reducing exploration for all agents to 0.0024\n",
      "Episode 350 is finished\n",
      "Average Reward for Agent 0 this episode : -17.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.9521\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -13.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9011\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -46.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.0909\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -1.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0723\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -22.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.2577\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.4083\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.0169\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -14.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.3048\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -1.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.7967\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 38.8057\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -45.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 220.4250\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -9.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 102.3866\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -3.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.8209\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -14.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.8379\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0023\n",
      "Episode 351 is finished\n",
      "Average Reward for Agent 0 this episode : -58.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 202.9419\n",
      "Average Reward for Agent 1 this episode : -8.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.5515\n",
      "Average Reward for Agent 2 this episode : -53.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.9428\n",
      "Average Reward for Agent 3 this episode : -1.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6814\n",
      "Average Reward for Agent 4 this episode : -16.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2770\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9802\n",
      "Average Reward for Agent 6 this episode : -1.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4700\n",
      "Average Reward for Agent 7 this episode : -13.12\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 48.9716\n",
      "Average Reward for Agent 8 this episode : -1.41\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1291\n",
      "Average Reward for Agent 9 this episode : -0.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.4597\n",
      "Average Reward for Agent 10 this episode : -59.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 228.1279\n",
      "Average Reward for Agent 11 this episode : -9.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.2109\n",
      "Average Reward for Agent 12 this episode : -5.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.7695\n",
      "Average Reward for Agent 13 this episode : -3.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.2466\n",
      "Reducing exploration for all agents to 0.0023\n",
      "Episode 352 is finished\n",
      "Average Reward for Agent 0 this episode : -12.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.0276\n",
      "Average Reward for Agent 1 this episode : -13.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4965\n",
      "Average Reward for Agent 2 this episode : -45.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.9619\n",
      "Average Reward for Agent 3 this episode : -4.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2682\n",
      "Average Reward for Agent 4 this episode : -17.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 91.5848\n",
      "Average Reward for Agent 5 this episode : -0.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4814\n",
      "Average Reward for Agent 6 this episode : -1.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7873\n",
      "Average Reward for Agent 7 this episode : -13.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9588\n",
      "Average Reward for Agent 8 this episode : -1.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.4537\n",
      "Average Reward for Agent 9 this episode : -0.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 62.0760\n",
      "Average Reward for Agent 10 this episode : -48.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 73.4806\n",
      "Average Reward for Agent 11 this episode : -8.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.6453\n",
      "Average Reward for Agent 12 this episode : -2.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.6926\n",
      "Average Reward for Agent 13 this episode : -14.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 35.8350\n",
      "Reducing exploration for all agents to 0.0023\n",
      "Episode 353 is finished\n",
      "Average Reward for Agent 0 this episode : -43.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 195.9412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 1 this episode : -9.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.0857\n",
      "Average Reward for Agent 2 this episode : -52.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 82.0227\n",
      "Average Reward for Agent 3 this episode : -3.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9014\n",
      "Average Reward for Agent 4 this episode : -21.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6269\n",
      "Average Reward for Agent 5 this episode : -0.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0868\n",
      "Average Reward for Agent 6 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3977\n",
      "Average Reward for Agent 7 this episode : -12.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1747\n",
      "Average Reward for Agent 8 this episode : -1.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3078\n",
      "Average Reward for Agent 9 this episode : -0.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.1390\n",
      "Average Reward for Agent 10 this episode : -32.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 263.7808\n",
      "Average Reward for Agent 11 this episode : -6.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.8948\n",
      "Average Reward for Agent 12 this episode : -6.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.2207\n",
      "Average Reward for Agent 13 this episode : -14.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.9190\n",
      "Reducing exploration for all agents to 0.0022\n",
      "Episode 354 is finished\n",
      "Average Reward for Agent 0 this episode : -45.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.1952\n",
      "Average Reward for Agent 1 this episode : -9.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.4715\n",
      "Average Reward for Agent 2 this episode : -51.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.4396\n",
      "Average Reward for Agent 3 this episode : -3.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1844\n",
      "Average Reward for Agent 4 this episode : -15.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.0781\n",
      "Average Reward for Agent 5 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.6465\n",
      "Average Reward for Agent 6 this episode : -0.78\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1687\n",
      "Average Reward for Agent 7 this episode : -11.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.3699\n",
      "Average Reward for Agent 8 this episode : -1.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.4438\n",
      "Average Reward for Agent 9 this episode : -0.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.5455\n",
      "Average Reward for Agent 10 this episode : -11.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 145.2071\n",
      "Average Reward for Agent 11 this episode : -3.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.7514\n",
      "Average Reward for Agent 12 this episode : -5.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1138\n",
      "Average Reward for Agent 13 this episode : -10.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.8466\n",
      "Reducing exploration for all agents to 0.0022\n",
      "Episode 355 is finished\n",
      "Average Reward for Agent 0 this episode : -17.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.9248\n",
      "Average Reward for Agent 1 this episode : -13.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3700\n",
      "Average Reward for Agent 2 this episode : -50.76\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.1077\n",
      "Average Reward for Agent 3 this episode : -4.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.2168\n",
      "Average Reward for Agent 4 this episode : -20.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.0140\n",
      "Average Reward for Agent 5 this episode : -1.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9986\n",
      "Average Reward for Agent 6 this episode : -0.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4972\n",
      "Average Reward for Agent 7 this episode : -11.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7591\n",
      "Average Reward for Agent 8 this episode : -1.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.0389\n",
      "Average Reward for Agent 9 this episode : -0.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.1188\n",
      "Average Reward for Agent 10 this episode : -12.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 375.0826\n",
      "Average Reward for Agent 11 this episode : -3.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 332.0146\n",
      "Average Reward for Agent 12 this episode : -6.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.0750\n",
      "Average Reward for Agent 13 this episode : -4.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 58.4106\n",
      "Reducing exploration for all agents to 0.0021\n",
      "Episode 356 is finished\n",
      "Average Reward for Agent 0 this episode : -38.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 60.2430\n",
      "Average Reward for Agent 1 this episode : -13.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5202\n",
      "Average Reward for Agent 2 this episode : -47.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.3347\n",
      "Average Reward for Agent 3 this episode : -4.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.5733\n",
      "Average Reward for Agent 4 this episode : -18.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 110.4379\n",
      "Average Reward for Agent 5 this episode : -0.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1510\n",
      "Average Reward for Agent 6 this episode : -1.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8773\n",
      "Average Reward for Agent 7 this episode : -13.06\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2416\n",
      "Average Reward for Agent 8 this episode : -1.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5690\n",
      "Average Reward for Agent 9 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 57.3549\n",
      "Average Reward for Agent 10 this episode : -25.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 307.4846\n",
      "Average Reward for Agent 11 this episode : -3.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 216.9115\n",
      "Average Reward for Agent 12 this episode : -6.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.7628\n",
      "Average Reward for Agent 13 this episode : -7.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 76.2972\n",
      "Reducing exploration for all agents to 0.0021\n",
      "Episode 357 is finished\n",
      "Average Reward for Agent 0 this episode : -30.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 126.3237\n",
      "Average Reward for Agent 1 this episode : -12.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0800\n",
      "Average Reward for Agent 2 this episode : -46.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 93.2608\n",
      "Average Reward for Agent 3 this episode : -3.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.4121\n",
      "Average Reward for Agent 4 this episode : -19.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.3238\n",
      "Average Reward for Agent 5 this episode : -0.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2923\n",
      "Average Reward for Agent 6 this episode : -0.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3872\n",
      "Average Reward for Agent 7 this episode : -13.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.2789\n",
      "Average Reward for Agent 8 this episode : -1.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6664\n",
      "Average Reward for Agent 9 this episode : -0.65\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.8474\n",
      "Average Reward for Agent 10 this episode : -14.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 496.2447\n",
      "Average Reward for Agent 11 this episode : -5.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 236.5925\n",
      "Average Reward for Agent 12 this episode : -4.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.1958\n",
      "Average Reward for Agent 13 this episode : -16.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.4460\n",
      "Reducing exploration for all agents to 0.0021\n",
      "Episode 358 is finished\n",
      "Average Reward for Agent 0 this episode : -40.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.8398\n",
      "Average Reward for Agent 1 this episode : -12.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0641\n",
      "Average Reward for Agent 2 this episode : -50.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.2378\n",
      "Average Reward for Agent 3 this episode : -2.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.6899\n",
      "Average Reward for Agent 4 this episode : -16.05\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.0951\n",
      "Average Reward for Agent 5 this episode : -0.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4433\n",
      "Average Reward for Agent 6 this episode : -0.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1598\n",
      "Average Reward for Agent 7 this episode : -13.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1438\n",
      "Average Reward for Agent 8 this episode : -1.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.1318\n",
      "Average Reward for Agent 9 this episode : -0.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.0043\n",
      "Average Reward for Agent 10 this episode : -27.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 591.1052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 11 this episode : -8.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 354.6861\n",
      "Average Reward for Agent 12 this episode : -4.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.7497\n",
      "Average Reward for Agent 13 this episode : -7.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.4694\n",
      "Reducing exploration for all agents to 0.002\n",
      "Episode 359 is finished\n",
      "Average Reward for Agent 0 this episode : -38.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.7813\n",
      "Average Reward for Agent 1 this episode : -13.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.2314\n",
      "Average Reward for Agent 2 this episode : -54.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 116.0127\n",
      "Average Reward for Agent 3 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5630\n",
      "Average Reward for Agent 4 this episode : -18.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.0885\n",
      "Average Reward for Agent 5 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0756\n",
      "Average Reward for Agent 6 this episode : -1.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.1049\n",
      "Average Reward for Agent 7 this episode : -11.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.0669\n",
      "Average Reward for Agent 8 this episode : -1.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6633\n",
      "Average Reward for Agent 9 this episode : -0.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 54.0628\n",
      "Average Reward for Agent 10 this episode : -14.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 313.2361\n",
      "Average Reward for Agent 11 this episode : -29.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 250.8424\n",
      "Average Reward for Agent 12 this episode : -5.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.5689\n",
      "Average Reward for Agent 13 this episode : -13.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.1154\n",
      "Reducing exploration for all agents to 0.002\n",
      "Episode 360 is finished\n",
      "Average Reward for Agent 0 this episode : -40.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.3619\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -11.45\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.9936\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -54.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.7229\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -3.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.1219\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -18.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.2375\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3295\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3836\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -11.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.4055\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5251\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -0.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.1579\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -5.93\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 419.4131\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -32.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 219.1554\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -3.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7815\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -7.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.6904\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.002\n",
      "Episode 361 is finished\n",
      "Average Reward for Agent 0 this episode : -40.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.6920\n",
      "Average Reward for Agent 1 this episode : -10.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.7434\n",
      "Average Reward for Agent 2 this episode : -50.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.5685\n",
      "Average Reward for Agent 3 this episode : -1.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6348\n",
      "Average Reward for Agent 4 this episode : -20.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.4365\n",
      "Average Reward for Agent 5 this episode : -0.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4341\n",
      "Average Reward for Agent 6 this episode : -1.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5530\n",
      "Average Reward for Agent 7 this episode : -10.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.9554\n",
      "Average Reward for Agent 8 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.4090\n",
      "Average Reward for Agent 9 this episode : -1.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 34.9520\n",
      "Average Reward for Agent 10 this episode : -2.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 503.5298\n",
      "Average Reward for Agent 11 this episode : -32.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 119.7474\n",
      "Average Reward for Agent 12 this episode : -4.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.9344\n",
      "Average Reward for Agent 13 this episode : -10.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.0904\n",
      "Reducing exploration for all agents to 0.0019\n",
      "Episode 362 is finished\n",
      "Average Reward for Agent 0 this episode : -37.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 157.8000\n",
      "Average Reward for Agent 1 this episode : -11.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.1122\n",
      "Average Reward for Agent 2 this episode : -51.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 175.6673\n",
      "Average Reward for Agent 3 this episode : -3.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.2008\n",
      "Average Reward for Agent 4 this episode : -20.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 39.9051\n",
      "Average Reward for Agent 5 this episode : -0.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4069\n",
      "Average Reward for Agent 6 this episode : -0.86\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7031\n",
      "Average Reward for Agent 7 this episode : -8.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.5581\n",
      "Average Reward for Agent 8 this episode : -0.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.8889\n",
      "Average Reward for Agent 9 this episode : -0.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 36.9916\n",
      "Average Reward for Agent 10 this episode : -10.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 358.7114\n",
      "Average Reward for Agent 11 this episode : -38.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 247.4493\n",
      "Average Reward for Agent 12 this episode : -3.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3566\n",
      "Average Reward for Agent 13 this episode : -9.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 65.8736\n",
      "Reducing exploration for all agents to 0.0019\n",
      "Episode 363 is finished\n",
      "Average Reward for Agent 0 this episode : -57.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.8691\n",
      "Average Reward for Agent 1 this episode : -8.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.0743\n",
      "Average Reward for Agent 2 this episode : -47.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 70.9481\n",
      "Average Reward for Agent 3 this episode : -1.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6461\n",
      "Average Reward for Agent 4 this episode : -16.85\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.2141\n",
      "Average Reward for Agent 5 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0569\n",
      "Average Reward for Agent 6 this episode : -0.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.6474\n",
      "Average Reward for Agent 7 this episode : -11.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3284\n",
      "Average Reward for Agent 8 this episode : -0.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0405\n",
      "Average Reward for Agent 9 this episode : -0.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.1929\n",
      "Average Reward for Agent 10 this episode : -7.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 236.3666\n",
      "Average Reward for Agent 11 this episode : -35.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 175.1316\n",
      "Average Reward for Agent 12 this episode : -4.96\n",
      "Train on 128 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 - 0s - loss: 2.5395\n",
      "Average Reward for Agent 13 this episode : -13.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.0575\n",
      "Reducing exploration for all agents to 0.0019\n",
      "Episode 364 is finished\n",
      "Average Reward for Agent 0 this episode : -63.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 210.8124\n",
      "Average Reward for Agent 1 this episode : -8.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 12.8051\n",
      "Average Reward for Agent 2 this episode : -48.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.4533\n",
      "Average Reward for Agent 3 this episode : -1.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5701\n",
      "Average Reward for Agent 4 this episode : -16.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3543\n",
      "Average Reward for Agent 5 this episode : -0.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4038\n",
      "Average Reward for Agent 6 this episode : -1.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.0103\n",
      "Average Reward for Agent 7 this episode : -12.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.4869\n",
      "Average Reward for Agent 8 this episode : -0.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.8536\n",
      "Average Reward for Agent 9 this episode : -1.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 41.8542\n",
      "Average Reward for Agent 10 this episode : -6.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 162.1196\n",
      "Average Reward for Agent 11 this episode : -37.2\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 197.7617\n",
      "Average Reward for Agent 12 this episode : -4.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9932\n",
      "Average Reward for Agent 13 this episode : -11.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.0160\n",
      "Reducing exploration for all agents to 0.0018\n",
      "Episode 365 is finished\n",
      "Average Reward for Agent 0 this episode : -32.61\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 127.1588\n",
      "Average Reward for Agent 1 this episode : -12.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 106.8593\n",
      "Average Reward for Agent 2 this episode : -49.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 98.2536\n",
      "Average Reward for Agent 3 this episode : -2.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0284\n",
      "Average Reward for Agent 4 this episode : -16.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.3046\n",
      "Average Reward for Agent 5 this episode : -0.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4405\n",
      "Average Reward for Agent 6 this episode : -1.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.5129\n",
      "Average Reward for Agent 7 this episode : -14.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0681\n",
      "Average Reward for Agent 8 this episode : -0.66\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6068\n",
      "Average Reward for Agent 9 this episode : -1.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 56.1254\n",
      "Average Reward for Agent 10 this episode : -7.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 164.7295\n",
      "Average Reward for Agent 11 this episode : -35.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 208.3745\n",
      "Average Reward for Agent 12 this episode : -4.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9584\n",
      "Average Reward for Agent 13 this episode : -6.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 53.4256\n",
      "Reducing exploration for all agents to 0.0018\n",
      "Episode 366 is finished\n",
      "Average Reward for Agent 0 this episode : -17.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 109.8426\n",
      "Average Reward for Agent 1 this episode : -15.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 64.1681\n",
      "Average Reward for Agent 2 this episode : -44.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.7140\n",
      "Average Reward for Agent 3 this episode : -2.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0481\n",
      "Average Reward for Agent 4 this episode : -19.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.9244\n",
      "Average Reward for Agent 5 this episode : -0.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3666\n",
      "Average Reward for Agent 6 this episode : -1.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 17.6828\n",
      "Average Reward for Agent 7 this episode : -14.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6280\n",
      "Average Reward for Agent 8 this episode : -1.16\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.7039\n",
      "Average Reward for Agent 9 this episode : -0.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.3515\n",
      "Average Reward for Agent 10 this episode : -2.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 146.4622\n",
      "Average Reward for Agent 11 this episode : -30.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.4897\n",
      "Average Reward for Agent 12 this episode : -4.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6124\n",
      "Average Reward for Agent 13 this episode : -13.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 42.1230\n",
      "Reducing exploration for all agents to 0.0018\n",
      "Episode 367 is finished\n",
      "Average Reward for Agent 0 this episode : -34.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 95.9731\n",
      "Average Reward for Agent 1 this episode : -15.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.7819\n",
      "Average Reward for Agent 2 this episode : -49.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.8139\n",
      "Average Reward for Agent 3 this episode : -4.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8931\n",
      "Average Reward for Agent 4 this episode : -19.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.8519\n",
      "Average Reward for Agent 5 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.0358\n",
      "Average Reward for Agent 6 this episode : -0.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.0930\n",
      "Average Reward for Agent 7 this episode : -13.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8954\n",
      "Average Reward for Agent 8 this episode : -2.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0710\n",
      "Average Reward for Agent 9 this episode : -1.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.1857\n",
      "Average Reward for Agent 10 this episode : -3.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 167.1106\n",
      "Average Reward for Agent 11 this episode : -33.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 132.4917\n",
      "Average Reward for Agent 12 this episode : -4.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.9745\n",
      "Average Reward for Agent 13 this episode : -10.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.9156\n",
      "Reducing exploration for all agents to 0.0017\n",
      "Episode 368 is finished\n",
      "Average Reward for Agent 0 this episode : -46.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.4230\n",
      "Average Reward for Agent 1 this episode : -11.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2524\n",
      "Average Reward for Agent 2 this episode : -46.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.6055\n",
      "Average Reward for Agent 3 this episode : -2.03\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7980\n",
      "Average Reward for Agent 4 this episode : -15.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3007\n",
      "Average Reward for Agent 5 this episode : -0.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0485\n",
      "Average Reward for Agent 6 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.1598\n",
      "Average Reward for Agent 7 this episode : -9.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.6439\n",
      "Average Reward for Agent 8 this episode : -1.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2611\n",
      "Average Reward for Agent 9 this episode : -1.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 79.3734\n",
      "Average Reward for Agent 10 this episode : -12.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 138.6330\n",
      "Average Reward for Agent 11 this episode : -29.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 72.2864\n",
      "Average Reward for Agent 12 this episode : -4.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3514\n",
      "Average Reward for Agent 13 this episode : -48.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.0517\n",
      "Reducing exploration for all agents to 0.0017\n",
      "Episode 369 is finished\n",
      "Average Reward for Agent 0 this episode : -39.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 87.1832\n",
      "Average Reward for Agent 1 this episode : -6.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 59.2068\n",
      "Average Reward for Agent 2 this episode : -42.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.8761\n",
      "Average Reward for Agent 3 this episode : -6.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.2356\n",
      "Average Reward for Agent 4 this episode : -16.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4641\n",
      "Average Reward for Agent 5 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7517\n",
      "Average Reward for Agent 6 this episode : -0.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5301\n",
      "Average Reward for Agent 7 this episode : -11.64\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.4740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 8 this episode : -1.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.5082\n",
      "Average Reward for Agent 9 this episode : -1.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 46.0898\n",
      "Average Reward for Agent 10 this episode : -3.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 183.1566\n",
      "Average Reward for Agent 11 this episode : -32.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 51.6363\n",
      "Average Reward for Agent 12 this episode : -5.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3032\n",
      "Average Reward for Agent 13 this episode : -40.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 45.4130\n",
      "Reducing exploration for all agents to 0.0017\n",
      "Episode 370 is finished\n",
      "Average Reward for Agent 0 this episode : -31.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 94.2573\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -12.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.1808\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -41.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 37.2346\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -2.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4255\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -15.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.2916\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4974\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -1.32\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.7332\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -15.53\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.9904\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.0224\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -1.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.2230\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -6.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 230.1795\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -36.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 113.1361\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -4.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.5647\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -47.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.6861\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0017\n",
      "Episode 371 is finished\n",
      "Average Reward for Agent 0 this episode : -36.63\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.9824\n",
      "Average Reward for Agent 1 this episode : -8.24\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.1237\n",
      "Average Reward for Agent 2 this episode : -45.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.3912\n",
      "Average Reward for Agent 3 this episode : -7.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5024\n",
      "Average Reward for Agent 4 this episode : -15.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.7594\n",
      "Average Reward for Agent 5 this episode : 0.0\n",
      "Saving architecture, weights, optimizer state for best agent-5\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent5_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7445\n",
      "Average Reward for Agent 6 this episode : -0.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1478\n",
      "Average Reward for Agent 7 this episode : -11.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4450\n",
      "Average Reward for Agent 8 this episode : -0.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.2971\n",
      "Average Reward for Agent 9 this episode : -1.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.5475\n",
      "Average Reward for Agent 10 this episode : -4.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 244.1625\n",
      "Average Reward for Agent 11 this episode : -35.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.4022\n",
      "Average Reward for Agent 12 this episode : -3.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.7240\n",
      "Average Reward for Agent 13 this episode : -48.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.9091\n",
      "Reducing exploration for all agents to 0.0016\n",
      "Episode 372 is finished\n",
      "Average Reward for Agent 0 this episode : -36.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 19.5057\n",
      "Average Reward for Agent 1 this episode : -14.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.4538\n",
      "Average Reward for Agent 2 this episode : -47.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 90.3131\n",
      "Average Reward for Agent 3 this episode : -5.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.5393\n",
      "Average Reward for Agent 4 this episode : -15.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.6237\n",
      "Average Reward for Agent 5 this episode : -0.72\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4175\n",
      "Average Reward for Agent 6 this episode : -1.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8789\n",
      "Average Reward for Agent 7 this episode : -12.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.8529\n",
      "Average Reward for Agent 8 this episode : -0.07\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9306\n",
      "Average Reward for Agent 9 this episode : -1.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 68.4296\n",
      "Average Reward for Agent 10 this episode : -5.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 200.7860\n",
      "Average Reward for Agent 11 this episode : -35.67\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 393.2792\n",
      "Average Reward for Agent 12 this episode : -6.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6865\n",
      "Average Reward for Agent 13 this episode : -35.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 103.6055\n",
      "Reducing exploration for all agents to 0.0016\n",
      "Episode 373 is finished\n",
      "Average Reward for Agent 0 this episode : -35.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.5515\n",
      "Average Reward for Agent 1 this episode : -14.35\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2255\n",
      "Average Reward for Agent 2 this episode : -29.01\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 84.9182\n",
      "Average Reward for Agent 3 this episode : -7.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.9186\n",
      "Average Reward for Agent 4 this episode : -19.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.2949\n",
      "Average Reward for Agent 5 this episode : -0.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.1170\n",
      "Average Reward for Agent 6 this episode : -1.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.3981\n",
      "Average Reward for Agent 7 this episode : -17.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.1999\n",
      "Average Reward for Agent 8 this episode : -0.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5597\n",
      "Average Reward for Agent 9 this episode : -2.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.8112\n",
      "Average Reward for Agent 10 this episode : -4.59\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 189.6779\n",
      "Average Reward for Agent 11 this episode : -34.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.9095\n",
      "Average Reward for Agent 12 this episode : -7.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.0105\n",
      "Average Reward for Agent 13 this episode : -51.37\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 107.5562\n",
      "Reducing exploration for all agents to 0.0016\n",
      "Episode 374 is finished\n",
      "Average Reward for Agent 0 this episode : -37.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.2892\n",
      "Average Reward for Agent 1 this episode : -11.42\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 26.4167\n",
      "Average Reward for Agent 2 this episode : -40.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 122.7871\n",
      "Average Reward for Agent 3 this episode : -6.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.9283\n",
      "Average Reward for Agent 4 this episode : -18.97\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.6379\n",
      "Average Reward for Agent 5 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.8987\n",
      "Average Reward for Agent 6 this episode : -1.02\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.6704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for Agent 7 this episode : -10.98\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.6130\n",
      "Average Reward for Agent 8 this episode : -0.52\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5160\n",
      "Average Reward for Agent 9 this episode : -2.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 85.0164\n",
      "Average Reward for Agent 10 this episode : -2.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 162.1833\n",
      "Average Reward for Agent 11 this episode : -33.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 261.0465\n",
      "Average Reward for Agent 12 this episode : -5.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0881\n",
      "Average Reward for Agent 13 this episode : -53.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 71.9935\n",
      "Reducing exploration for all agents to 0.0015\n",
      "Episode 375 is finished\n",
      "Average Reward for Agent 0 this episode : -41.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 49.7902\n",
      "Average Reward for Agent 1 this episode : -10.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.6513\n",
      "Average Reward for Agent 2 this episode : -46.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 97.5100\n",
      "Average Reward for Agent 3 this episode : -4.88\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9086\n",
      "Average Reward for Agent 4 this episode : -17.25\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.3255\n",
      "Average Reward for Agent 5 this episode : -0.29\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6776\n",
      "Average Reward for Agent 6 this episode : -0.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.6327\n",
      "Average Reward for Agent 7 this episode : -13.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.9397\n",
      "Average Reward for Agent 8 this episode : -0.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.3547\n",
      "Average Reward for Agent 9 this episode : -2.23\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 125.0780\n",
      "Average Reward for Agent 10 this episode : -6.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 166.9789\n",
      "Average Reward for Agent 11 this episode : -42.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 112.7399\n",
      "Average Reward for Agent 12 this episode : -3.62\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2774\n",
      "Average Reward for Agent 13 this episode : -52.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 99.1061\n",
      "Reducing exploration for all agents to 0.0015\n",
      "Episode 376 is finished\n",
      "Average Reward for Agent 0 this episode : -43.92\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 74.0524\n",
      "Average Reward for Agent 1 this episode : -12.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.5908\n",
      "Average Reward for Agent 2 this episode : -43.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.5283\n",
      "Average Reward for Agent 3 this episode : -1.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.9301\n",
      "Average Reward for Agent 4 this episode : -18.17\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 9.7026\n",
      "Average Reward for Agent 5 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3184\n",
      "Average Reward for Agent 6 this episode : -0.77\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.9779\n",
      "Average Reward for Agent 7 this episode : -13.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.1925\n",
      "Average Reward for Agent 8 this episode : -0.48\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.5977\n",
      "Average Reward for Agent 9 this episode : -3.27\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.2208\n",
      "Average Reward for Agent 10 this episode : -11.26\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 140.8744\n",
      "Average Reward for Agent 11 this episode : -36.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 180.4889\n",
      "Average Reward for Agent 12 this episode : -6.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.4825\n",
      "Average Reward for Agent 13 this episode : -45.55\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 92.9765\n",
      "Reducing exploration for all agents to 0.0015\n",
      "Episode 377 is finished\n",
      "Average Reward for Agent 0 this episode : -44.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.3900\n",
      "Average Reward for Agent 1 this episode : -14.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 15.8256\n",
      "Average Reward for Agent 2 this episode : -33.83\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 249.1891\n",
      "Average Reward for Agent 3 this episode : -3.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.0666\n",
      "Average Reward for Agent 4 this episode : -20.87\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3397\n",
      "Average Reward for Agent 5 this episode : -0.3\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.4221\n",
      "Average Reward for Agent 6 this episode : -2.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 32.3228\n",
      "Average Reward for Agent 7 this episode : -15.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.2107\n",
      "Average Reward for Agent 8 this episode : -0.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.0337\n",
      "Average Reward for Agent 9 this episode : -2.11\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 173.7718\n",
      "Average Reward for Agent 10 this episode : -5.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 202.2209\n",
      "Average Reward for Agent 11 this episode : -35.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 116.4141\n",
      "Average Reward for Agent 12 this episode : -3.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3657\n",
      "Average Reward for Agent 13 this episode : -56.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 83.7003\n",
      "Reducing exploration for all agents to 0.0015\n",
      "Episode 378 is finished\n",
      "Average Reward for Agent 0 this episode : -43.5\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 33.1068\n",
      "Average Reward for Agent 1 this episode : -13.08\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 13.5458\n",
      "Average Reward for Agent 2 this episode : -42.4\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 232.5036\n",
      "Average Reward for Agent 3 this episode : -3.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.4923\n",
      "Average Reward for Agent 4 this episode : -18.89\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 101.6205\n",
      "Average Reward for Agent 5 this episode : -0.46\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.3932\n",
      "Average Reward for Agent 6 this episode : -1.28\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 40.8069\n",
      "Average Reward for Agent 7 this episode : -12.71\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 23.5805\n",
      "Average Reward for Agent 8 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8820\n",
      "Average Reward for Agent 9 this episode : -3.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 47.5158\n",
      "Average Reward for Agent 10 this episode : -6.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 181.6543\n",
      "Average Reward for Agent 11 this episode : -28.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 144.4960\n",
      "Average Reward for Agent 12 this episode : -4.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0615\n",
      "Average Reward for Agent 13 this episode : -47.99\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 78.8517\n",
      "Reducing exploration for all agents to 0.0014\n",
      "Episode 379 is finished\n",
      "Average Reward for Agent 0 this episode : -44.19\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 63.8536\n",
      "Average Reward for Agent 1 this episode : -2.5\n",
      "Saving architecture, weights, optimizer state for best agent-1\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Balance\\Agents_Results\\DQN\\BestAgent1_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.9666\n",
      "Average Reward for Agent 2 this episode : -44.84\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 173.4491\n",
      "Average Reward for Agent 3 this episode : -1.38\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.7634\n",
      "Average Reward for Agent 4 this episode : -16.33\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 27.1313\n",
      "Average Reward for Agent 5 this episode : -0.9\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0437\n",
      "Average Reward for Agent 6 this episode : -0.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 43.1280\n",
      "Average Reward for Agent 7 this episode : -12.51\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 5.4054\n",
      "Average Reward for Agent 8 this episode : -0.58\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8988\n",
      "Average Reward for Agent 9 this episode : -4.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 168.6121\n",
      "Average Reward for Agent 10 this episode : -8.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 145.3011\n",
      "Average Reward for Agent 11 this episode : -37.95\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.9173\n",
      "Average Reward for Agent 12 this episode : -4.47\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.3255\n",
      "Average Reward for Agent 13 this episode : -52.21\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 61.6533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing exploration for all agents to 0.0014\n",
      "Episode 380 is finished\n",
      "Average Reward for Agent 0 this episode : -41.43\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 24.9880\n",
      "Weights succesfully copied to Target model for Agent 0.\n",
      "Average Reward for Agent 1 this episode : -9.49\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 11.1378\n",
      "Weights succesfully copied to Target model for Agent 1.\n",
      "Average Reward for Agent 2 this episode : -39.09\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 213.5188\n",
      "Weights succesfully copied to Target model for Agent 2.\n",
      "Average Reward for Agent 3 this episode : -2.0\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.3454\n",
      "Weights succesfully copied to Target model for Agent 3.\n",
      "Average Reward for Agent 4 this episode : -17.74\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 6.5898\n",
      "Weights succesfully copied to Target model for Agent 4.\n",
      "Average Reward for Agent 5 this episode : -0.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.0535\n",
      "Weights succesfully copied to Target model for Agent 5.\n",
      "Average Reward for Agent 6 this episode : -0.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.3092\n",
      "Weights succesfully copied to Target model for Agent 6.\n",
      "Average Reward for Agent 7 this episode : -8.81\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 8.9523\n",
      "Weights succesfully copied to Target model for Agent 7.\n",
      "Average Reward for Agent 8 this episode : -0.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.6326\n",
      "Weights succesfully copied to Target model for Agent 8.\n",
      "Average Reward for Agent 9 this episode : -4.44\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 25.7436\n",
      "Weights succesfully copied to Target model for Agent 9.\n",
      "Average Reward for Agent 10 this episode : -5.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 189.0476\n",
      "Weights succesfully copied to Target model for Agent 10.\n",
      "Average Reward for Agent 11 this episode : -35.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 66.3051\n",
      "Weights succesfully copied to Target model for Agent 11.\n",
      "Average Reward for Agent 12 this episode : -5.13\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.8083\n",
      "Weights succesfully copied to Target model for Agent 12.\n",
      "Average Reward for Agent 13 this episode : -51.79\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 104.5795\n",
      "Weights succesfully copied to Target model for Agent 13.\n",
      "Reducing exploration for all agents to 0.0014\n",
      "Episode 381 is finished\n",
      "Average Reward for Agent 0 this episode : -37.22\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 96.5394\n",
      "Average Reward for Agent 1 this episode : -9.75\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 14.2833\n",
      "Average Reward for Agent 2 this episode : -41.7\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 55.6197\n",
      "Average Reward for Agent 3 this episode : -1.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7955\n",
      "Average Reward for Agent 4 this episode : -14.31\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.7046\n",
      "Average Reward for Agent 5 this episode : -0.91\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.3823\n",
      "Average Reward for Agent 6 this episode : -0.96\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 21.0156\n",
      "Average Reward for Agent 7 this episode : -11.1\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 4.3141\n",
      "Average Reward for Agent 8 this episode : -0.56\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.8253\n",
      "Average Reward for Agent 9 this episode : -4.68\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.7938\n",
      "Average Reward for Agent 10 this episode : -2.39\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 176.1217\n",
      "Average Reward for Agent 11 this episode : -33.36\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 88.6146\n",
      "Average Reward for Agent 12 this episode : -3.82\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 10.0372\n",
      "Average Reward for Agent 13 this episode : -51.6\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 86.1862\n",
      "Reducing exploration for all agents to 0.0014\n",
      "Episode 382 is finished\n",
      "Average Reward for Agent 0 this episode : -36.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 242.8042\n",
      "Average Reward for Agent 1 this episode : -12.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 7.5093\n",
      "Average Reward for Agent 2 this episode : -48.34\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 29.7407\n",
      "Average Reward for Agent 3 this episode : -4.18\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.0797\n",
      "Average Reward for Agent 4 this episode : -15.73\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 18.0307\n",
      "Average Reward for Agent 5 this episode : -0.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 0.7759\n",
      "Average Reward for Agent 6 this episode : -0.8\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.8250\n",
      "Average Reward for Agent 7 this episode : -13.57\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 1.7745\n",
      "Average Reward for Agent 8 this episode : -0.14\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 2.2011\n",
      "Average Reward for Agent 9 this episode : -3.94\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 50.3148\n",
      "Average Reward for Agent 10 this episode : -1.69\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 165.3525\n",
      "Average Reward for Agent 11 this episode : -12.04\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 139.2238\n",
      "Average Reward for Agent 12 this episode : -5.54\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 3.2778\n",
      "Average Reward for Agent 13 this episode : -46.15\n",
      "Train on 128 samples\n",
      "128/128 - 0s - loss: 81.5758\n",
      "Reducing exploration for all agents to 0.0013\n"
     ]
    }
   ],
   "source": [
    "Balance_MultiDQN_Agents.train(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Balance_MultiDQN_Agents.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of tensorflow.python.keras.layers.core failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rzhang\\Anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 244, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Rzhang\\Anaconda3\\envs\\tf2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 378, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Rzhang\\Anaconda3\\envs\\tf2\\lib\\imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"C:\\Users\\Rzhang\\Anaconda3\\envs\\tf2\\lib\\importlib\\__init__.py\", line 148, in reload\n",
      "    raise ImportError(msg.format(name), name=name)\n",
      "ImportError: module DQNAgents not in sys.modules\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pre-Trained Agent 0, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 1, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 2, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 3, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 4, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 5, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 6, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 7, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 8, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 9, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 10, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 11, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 12, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n",
      "Loading Pre-Trained Agent 13, Architecture, Optimizer and Memory.\n",
      "Items successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "Balance_MultiDQN_Agents.load(best = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Straight\n",
    "\n",
    "---> The lack of speed comes from the size of the model (particularly the change of color of the heads). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Straight'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Straight_dictionary =\\\n",
    "{'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'compatible_actions' : {     0 : [1, 0, 1, 0],\n",
    "                                     1 : [0, 1, 0, 1]\n",
    "        },\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '3-1', '5-1', '7-1'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [5],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' }\n",
    "        },\n",
    " 'demand' : { 'default' : [400, 400, 400, 400]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot\n"
     ]
    }
   ],
   "source": [
    "print('tot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Straight.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "0.08910298347473145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: <Vissim_SCU_class.Signal_Control_Unit at 0x190b93b9160>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = environment(model_name, vissim_working_directory, sim_length, Single_Cross_Straight_dictionary,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)\n",
    "\n",
    "env.SCUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  384       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  252       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,267\n",
      "Trainable params: 11,267\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.85\n",
    "alpha = 0.00005\n",
    "ID = 0\n",
    "value = 25\n",
    "entropy = 0.05\n",
    "n_step_size = 16\n",
    "state_size = [5]\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 100\n",
    "n_sample = 10\n",
    "\n",
    "Agents = []\n",
    "\n",
    "for idx, info in Single_Cross_Straight_dictionary['junctions'].items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(state_size, len(acts), ID, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 : Predicted Values and True Return : \n",
      " [-60.0, -72.0, -72.0, -72.0, -72.0, -69.0, -72.0, -72.0, -30.0, -40.0] \n",
      " [-439.0, -527.0, -527.0, -527.0, -527.0, -507.0, -527.0, -526.0, -255.0, -324.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.02, 0.98], [0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.5 0.5]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3.0, -5.0, -5.0, -3.0, -5.0, -4.0, -3.0, -13.0, -10.0, -9.0] \n",
      " [-217.0, -527.0, -417.0, -527.0, -72.0, -527.0, -527.0, -243.0, -451.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.37, 0.63], [0.71, 0.29], [0.67, 0.33], [0.36, 0.64], [0.71, 0.29], [0.27, 0.73], [0.37, 0.63], [0.78, 0.22], [0.07, 0.93], [0.15, 0.85]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.49 0.51]\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6.0, -6.0, -11.0, -6.0, -9.0, -14.0, -6.0, -6.0, -13.0, -10.0] \n",
      " [-527.0, -527.0, -527.0, -527.0, -527.0, -525.0, -527.0, -527.0, -527.0, -527.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.76, 0.24], [0.19, 0.81], [0.05, 0.95], [0.76, 0.24], [0.81, 0.19], [0.84, 0.16], [0.76, 0.24], [0.19, 0.81], [0.02, 0.98], [0.04, 0.96]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.4 0.6]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-45ab2e412337>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0maction_required\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSARSDs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_to_next_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maction_required\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36mstep_to_next_action\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_required\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m                         \u001b[0maction_required\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSarsd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    104\u001b[0m \t\t\"\"\"\n\u001b[0;32m    105\u001b[0m                 \u001b[1;31m#global timesteps_per_second\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVissim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunSingleStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m                 \u001b[1;31m# increase the update counter by one each step (until reach simulation length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    514\u001b[0m                         \u001b[0mdebug_attr_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Getting property Id 0x%x from OLE object\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mretEntry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m                         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m                                 \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_oleobj_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvoke\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretEntry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minvoke_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m                         \u001b[1;32mexcept\u001b[0m \u001b[0mpythoncom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcom_error\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdetails\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m                                 \u001b[1;32mif\u001b[0m \u001b[0mdetails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhresult\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mERRORS_BAD_CONTEXT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)\n",
    "\n",
    "for _ in range(100000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn()\n",
    "                \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "    \n",
    "    # For the saving , monitoring of the agent\n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, agent in enumerate(Agents):\n",
    "            predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "            print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "            print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "            print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "            agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n"
     ]
    }
   ],
   "source": [
    "Agents[0].save_agent(vissim_working_directory, model_name, 'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pre-Trained Agent, Architecture and Memory.\n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  384       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  252       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  86        \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 11,267\n",
      "Trainable params: 11,267\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n",
      "Items successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "Agents[0].load_agent(vissim_working_directory, model_name, 'TEST', best = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 4 actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 2000\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary4 =\\\n",
    "{ 'junctions' : {\n",
    "    # Controller Number 0 \n",
    "    0 : {'compatible_actions' : {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                     1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                     2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                     3 : [0,0,0,0,0,0,0,0,0,1,1,1]},\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 10,\n",
    "         'redamber_time' : 1,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [13],\n",
    "         'state_type' : 'QueuesSig',\n",
    "         'reward_type' : 'Queues' \n",
    "         }\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Triple.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 2000 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "0.12496423721313477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: <Vissim_SCU_class.Signal_Control_Unit at 0x1f26b747860>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = environment(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary4,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)\n",
    "\n",
    "env.SCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  896       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  588       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,201\n",
      "Trainable params: 12,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corrected\n"
     ]
    }
   ],
   "source": [
    "# not bad with the first model. need a fonction to decrease entropy now \n",
    "\n",
    "gamma = 0.85\n",
    "alpha = 0.0005\n",
    "ID = 0\n",
    "value = 25\n",
    "entropy = 5000\n",
    "n_step_size = 11\n",
    "state_size = [12]\n",
    "reduce_entropy_every = 100\n",
    "entropy_threshold = 0.5\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 50\n",
    "n_sample = 10\n",
    "\n",
    "Agents = []\n",
    "\n",
    "for idx, info in Single_Cross_Triple_dictionary4['junctions'].items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(info['state_size'], len(acts), ID, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  896       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  588       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,201\n",
      "Trainable params: 12,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 0 : Entropy reduced to 500.0 \n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-87.0, -152.0, -163.0, -103.0, -62.0, -157.0, -120.0, -183.0, -309.0, -60.0] \n",
      " [-103.0, -152.0, -130.0, -100.0, -145.0, -264.0, -139.0, -286.0, -226.0, -159.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.27, 0.25, 0.26, 0.22], [0.19, 0.29, 0.29, 0.23], [0.22, 0.27, 0.23, 0.28], [0.29, 0.24, 0.25, 0.22], [0.24, 0.29, 0.22, 0.24], [0.24, 0.29, 0.28, 0.2], [0.16, 0.31, 0.25, 0.28], [0.29, 0.27, 0.22, 0.22], [0.27, 0.25, 0.25, 0.22], [0.24, 0.24, 0.24, 0.27]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.25 0.25 0.25 0.25]\n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  896       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  588       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,201\n",
      "Trainable params: 12,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 0 : Entropy reduced to 50.0 \n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-447.0, -678.0, -264.0, -324.0, -299.0, -567.0, -126.0, -472.0, -615.0, -660.0] \n",
      " [-150.0, -129.0, -296.0, -151.0, -248.0, -139.0, -159.0, -104.0, -116.0, -120.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.59, 0.0, 0.0, 0.4], [0.45, 0.0, 0.01, 0.54], [0.6, 0.0, 0.03, 0.37], [0.67, 0.0, 0.02, 0.3], [0.48, 0.0, 0.04, 0.48], [0.41, 0.0, 0.01, 0.58], [0.36, 0.19, 0.13, 0.32], [0.39, 0.0, 0.06, 0.55], [0.1, 0.0, 0.0, 0.9], [0.26, 0.0, 0.0, 0.74]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.17 0.28 0.29 0.27]\n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  896       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  588       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,201\n",
      "Trainable params: 12,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 0 : Entropy reduced to 5.0 \n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  896       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  588       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,201\n",
      "Trainable params: 12,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 0 : Entropy reduced to 0.5 \n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2102.0, -2327.0, -1796.0, -1867.0, -1442.0, -2310.0, -1867.0, -187.0, -1211.0, -2339.0] \n",
      " [-138.0, -147.0, -117.0, -129.0, -104.0, -150.0, -129.0, -294.0, -109.0, -143.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0.   0.28 0.03 0.68]\n",
      "Model: \"model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value1 (Dense)               multiple                  896       \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value3 (Dense)               multiple                  2730      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  43        \n",
      "_________________________________________________________________\n",
      "policy_logits1 (Dense)       multiple                  588       \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  1806      \n",
      "_________________________________________________________________\n",
      "policy_logits (Dense)        multiple                  172       \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,201\n",
      "Trainable params: 12,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Agent 0 : Entropy reduced to 0.05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1732.0, -2332.0, -2332.0, -2277.0, -2305.0, -371.0, -2332.0, -2332.0, -1959.0, -2332.0] \n",
      " [-104.0, -221.0, -202.0, -128.0, -138.0, -207.0, -155.0, -166.0, -116.0, -150.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1370.0, -2332.0, -2029.0, -1640.0, -1370.0, -1888.0, -2277.0, -1821.0, -2332.0, -723.0] \n",
      " [-152.0, -160.0, -118.0, -95.0, -152.0, -97.0, -134.0, -88.0, -147.0, -286.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1729.0, -659.0, -1742.0, -2328.0, -1902.0, -2328.0, -2132.0, -1720.0, -1250.0, -1123.0] \n",
      " [-104.0, -264.0, -101.0, -158.0, -99.0, -143.0, -135.0, -104.0, -188.0, -248.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1820.0, -1747.0, -2298.0, -1831.0, -2380.0, -1557.0, -380.0, -1831.0, -2298.0, -2380.0] \n",
      " [-95.0, -103.0, -140.0, -86.0, -166.0, -148.0, -174.0, -86.0, -140.0, -133.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1629.0, -829.0, -2379.0, -986.0, -2113.0, -2126.0, -2286.0, -1057.0, -1355.0, -2379.0] \n",
      " [-101.0, -294.0, -180.0, -301.0, -126.0, -127.0, -147.0, -302.0, -170.0, -142.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2203.0, -2360.0, -1893.0, -1235.0, -2341.0, -141.0, -685.0, -2174.0, -979.0, -1097.0] \n",
      " [-139.0, -156.0, -87.0, -211.0, -133.0, -114.0, -278.0, -130.0, -301.0, -290.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1237.0, -2376.0, -1398.0, -2267.0, -557.0, -518.0, -1703.0, -1401.0, -1949.0, -1833.0] \n",
      " [-211.0, -161.0, -152.0, -152.0, -241.0, -226.0, -91.0, -155.0, -100.0, -95.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2376.0, -1398.0, -2376.0, -2376.0, -1398.0, -2376.0, -1145.0, -2376.0, -2376.0, -2169.0] \n",
      " [-189.0, -152.0, -142.0, -137.0, -152.0, -147.0, -248.0, -137.0, -154.0, -129.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2333.0, -2377.0, -2377.0, -2309.0, -2377.0, -2261.0, -1476.0, -1576.0, -2377.0, -680.0] \n",
      " [-129.0, -139.0, -154.0, -139.0, -161.0, -152.0, -147.0, -141.0, -189.0, -278.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2358.0, -868.0, -517.0, -1547.0, -2376.0, -1960.0, -2376.0, -2376.0, -664.0, -2324.0] \n",
      " [-149.0, -290.0, -226.0, -148.0, -132.0, -99.0, -150.0, -161.0, -264.0, -128.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-999.0, -2000.0, -2232.0, -441.0, -225.0, -1023.0, -2191.0, -288.0, -2107.0, -1546.0] \n",
      " [-306.0, -97.0, -146.0, -189.0, -114.0, -308.0, -135.0, -132.0, -118.0, -148.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-225.0, -999.0, -1959.0, -493.0, -2374.0, -2108.0, -1528.0, -2040.0, -1475.0, -2374.0] \n",
      " [-97.0, -306.0, -100.0, -207.0, -142.0, -118.0, -150.0, -116.0, -147.0, -144.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1051.0, -2374.0, -2091.0, -1935.0, -1683.0, -720.0, -720.0, -1395.0, -2012.0, -336.0] \n",
      " [-302.0, -158.0, -120.0, -99.0, -90.0, -286.0, -286.0, -152.0, -101.0, -145.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2374.0, -2210.0, -1701.0, -2137.0, -2374.0, -1780.0, -2041.0, -2374.0, -2374.0, -817.0] \n",
      " [-133.0, -139.0, -91.0, -127.0, -233.0, -104.0, -116.0, -221.0, -154.0, -294.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-903.0, -1730.0, -2374.0, -2339.0, -1145.0, -1402.0, -2374.0, -1755.0, -1755.0, -1588.0] \n",
      " [-290.0, -101.0, -147.0, -133.0, -248.0, -155.0, -150.0, -103.0, -103.0, -130.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1613.0, -815.0, -2357.0, -1655.0, -1713.0, -865.0, -225.0, -1613.0, -1164.0, -2288.0] \n",
      " [-123.0, -294.0, -156.0, -95.0, -94.0, -290.0, -114.0, -123.0, -226.0, -147.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1052.0, -2262.0, -1475.0, -2211.0, -1633.0, -1545.0, -1431.0, -680.0, -1961.0, -1402.0] \n",
      " [-302.0, -152.0, -147.0, -139.0, -101.0, -148.0, -151.0, -278.0, -100.0, -155.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-287.0, -2374.0, -287.0, -953.0, -2374.0, -2263.0, -2374.0, -225.0, -2374.0, -1165.0] \n",
      " [-132.0, -156.0, -132.0, -296.0, -144.0, -152.0, -147.0, -114.0, -155.0, -226.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1908.0, -1908.0, -2374.0, -680.0, -735.0, -1587.0, -2374.0, -2374.0, -445.0, -2374.0] \n",
      " [-88.0, -88.0, -150.0, -278.0, -292.0, -130.0, -215.0, -154.0, -189.0, -147.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-563.0, -1911.0, -1842.0, -1313.0, -997.0, -1164.0, -621.0, -2373.0, -2094.0, -2356.0] \n",
      " [-241.0, -87.0, -95.0, -188.0, -306.0, -226.0, -255.0, -154.0, -120.0, -149.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2262.0, -1587.0, -1090.0, -1090.0, -2373.0, -1527.0, -2373.0, -2373.0, -2035.0, -1360.0] \n",
      " [-152.0, -130.0, -290.0, -290.0, -143.0, -150.0, -150.0, -132.0, -109.0, -170.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1391.0, -681.0, -1621.0, -2374.0, -2374.0, -225.0, -2374.0, -620.0, -2374.0, -721.0] \n",
      " [-152.0, -278.0, -109.0, -161.0, -202.0, -114.0, -160.0, -255.0, -133.0, -286.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1544.0, -2138.0, -2373.0, -2373.0, -1313.0, -1166.0, -2373.0, -2115.0, -1853.0, -2233.0] \n",
      " [-148.0, -127.0, -133.0, -150.0, -188.0, -226.0, -138.0, -122.0, -86.0, -146.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2374.0, -1853.0, -1587.0, -2094.0, -2139.0, -2374.0, -1053.0, -1919.0, -620.0, -2374.0] \n",
      " [-208.0, -86.0, -130.0, -120.0, -127.0, -202.0, -302.0, -93.0, -255.0, -180.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1794.0, -2183.0, -2147.0, -1911.0, -387.0, -2373.0, -1909.0, -1963.0, -2373.0, -2178.0] \n",
      " [-104.0, -130.0, -131.0, -87.0, -174.0, -155.0, -88.0, -100.0, -150.0, -129.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1730.0, -2373.0, -1526.0, -2373.0, -1429.0, -1114.0, -1544.0, -1782.0, -2373.0, -2234.0] \n",
      " [-101.0, -154.0, -150.0, -154.0, -151.0, -268.0, -148.0, -104.0, -161.0, -146.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2372.0, -2372.0, -1279.0, -2372.0, -1313.0, -2372.0, -2372.0, -2372.0, -2372.0, -2372.0] \n",
      " [-156.0, -215.0, -202.0, -154.0, -188.0, -180.0, -142.0, -144.0, -142.0, -147.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2350.0, -1847.0, -1025.0, -1314.0, -2373.0, -446.0, -1886.0, -2373.0, -681.0, -1799.0] \n",
      " [-138.0, -95.0, -308.0, -188.0, -161.0, -189.0, -88.0, -147.0, -278.0, -104.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1753.0, -1492.0, -1391.0, -2372.0, -2193.0, -1964.0, -2324.0, -1024.0, -1819.0, -225.0] \n",
      " [-103.0, -146.0, -152.0, -133.0, -135.0, -100.0, -128.0, -308.0, -99.0, -114.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2372.0, -1149.0, -2372.0, -681.0, -2372.0, -2289.0, -1929.0, -2146.0, -2304.0, -2259.0] \n",
      " [-138.0, -248.0, -202.0, -278.0, -147.0, -147.0, -97.0, -131.0, -140.0, -152.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1167.0, -2096.0, -1755.0, -1706.0, -2372.0, -2372.0, -2140.0, -2372.0, -1715.0, -2096.0] \n",
      " [-226.0, -120.0, -103.0, -91.0, -143.0, -166.0, -127.0, -139.0, -94.0, -120.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1842.0, -2372.0, -2148.0, -2372.0, -1621.0, -2213.0, -1525.0, -2372.0, -1621.0, -2263.0] \n",
      " [-95.0, -150.0, -131.0, -147.0, -109.0, -139.0, -150.0, -161.0, -109.0, -152.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1846.0, -1914.0, -1943.0, -2373.0, -1472.0, -2373.0, -2011.0, -2119.0, -1149.0, -1823.0] \n",
      " [-95.0, -88.0, -99.0, -208.0, -147.0, -147.0, -97.0, -122.0, -248.0, -99.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2324.0, -1427.0, -1210.0, -899.0, -2372.0, -2306.0, -1685.0, -2372.0, -1706.0, -1114.0] \n",
      " [-128.0, -151.0, -212.0, -290.0, -137.0, -140.0, -90.0, -160.0, -91.0, -268.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-565.0, -1360.0, -1821.0, -735.0, -1885.0, -225.0, -2372.0, -814.0, -2372.0, -1968.0] \n",
      " [-241.0, -170.0, -99.0, -292.0, -88.0, -97.0, -221.0, -294.0, -142.0, -100.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2179.0, -2372.0, -520.0, -859.0, -2148.0, -2372.0, -2372.0, -2372.0, -1612.0, -2372.0] \n",
      " [-129.0, -147.0, -226.0, -290.0, -131.0, -189.0, -139.0, -147.0, -123.0, -138.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1815.0, -2184.0, -2371.0, -288.0, -1359.0, -1706.0, -2371.0, -2371.0, -1815.0, -2039.0] \n",
      " [-101.0, -130.0, -150.0, -132.0, -170.0, -91.0, -142.0, -143.0, -101.0, -109.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1586.0, -1519.0, -2037.0, -2372.0, -1969.0, -1882.0, -2214.0, -1925.0, -2310.0, -2372.0] \n",
      " [-130.0, -145.0, -109.0, -132.0, -99.0, -88.0, -139.0, -97.0, -139.0, -154.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2372.0, -2372.0, -681.0, -2292.0, -2372.0, -2236.0, -2308.0, -2372.0, -1798.0, -2117.0] \n",
      " [-154.0, -154.0, -278.0, -147.0, -180.0, -146.0, -140.0, -144.0, -104.0, -118.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2372.0, -1312.0, -225.0, -1280.0, -2310.0, -1925.0, -2372.0, -2042.0, -1986.0, -2036.0] \n",
      " [-154.0, -188.0, -97.0, -202.0, -139.0, -97.0, -154.0, -116.0, -99.0, -109.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2350.0, -1754.0, -2372.0, -332.0, -2129.0, -2372.0, -388.0, -520.0, -2129.0, -2067.0] \n",
      " [-138.0, -103.0, -166.0, -145.0, -126.0, -215.0, -174.0, -226.0, -126.0, -117.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2372.0, -2162.0, -2141.0, -663.0, -490.0, -1810.0, -2009.0, -2356.0, -2372.0, -1620.0] \n",
      " [-147.0, -131.0, -127.0, -264.0, -207.0, -101.0, -97.0, -149.0, -180.0, -109.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-288.0, -2371.0, -2043.0, -2371.0, -2371.0, -1988.0, -1395.0, -2371.0, -2371.0, -2127.0] \n",
      " [-132.0, -138.0, -116.0, -143.0, -154.0, -99.0, -155.0, -147.0, -208.0, -126.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2335.0, -2018.0, -1940.0, -2184.0, -1966.0, -1716.0, -1840.0, -1925.0, -1053.0, -2372.0] \n",
      " [-129.0, -101.0, -99.0, -130.0, -100.0, -94.0, -95.0, -97.0, -302.0, -144.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1312.0, -2117.0, -2235.0, -225.0, -1916.0, -2372.0, -1246.0, -898.0, -1390.0, -2335.0] \n",
      " [-188.0, -118.0, -146.0, -114.0, -87.0, -215.0, -211.0, -290.0, -152.0, -129.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-721.0, -1396.0, -2372.0, -367.0, -2372.0, -1612.0, -1707.0, -617.0, -997.0, -1467.0] \n",
      " [-286.0, -155.0, -221.0, -159.0, -150.0, -123.0, -91.0, -255.0, -306.0, -147.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2018.0, -1246.0, -1568.0, -2371.0, -1517.0, -2184.0, -859.0, -1689.0, -2371.0, -859.0] \n",
      " [-101.0, -211.0, -141.0, -158.0, -145.0, -130.0, -290.0, -90.0, -146.0, -290.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1359.0, -2037.0, -1489.0, -289.0, -2064.0, -2372.0, -2335.0, -2184.0, -2372.0, -947.0] \n",
      " [-170.0, -109.0, -146.0, -132.0, -117.0, -141.0, -129.0, -130.0, -142.0, -296.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1585.0, -1914.0, -2372.0, -2351.0, -446.0, -2043.0, -2372.0, -2372.0, -2356.0, -1612.0] \n",
      " [-130.0, -87.0, -142.0, -138.0, -189.0, -116.0, -208.0, -221.0, -149.0, -123.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2180.0, -1053.0, -2372.0, -2356.0, -2372.0, -1925.0, -1359.0, -1708.0, -2372.0, -2372.0] \n",
      " [-129.0, -302.0, -142.0, -149.0, -154.0, -97.0, -170.0, -91.0, -147.0, -189.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1986.0, -1390.0, -1150.0, -2274.0, -2371.0, -2371.0, -1358.0, -681.0, -2263.0, -390.0] \n",
      " [-99.0, -152.0, -248.0, -152.0, -141.0, -208.0, -170.0, -278.0, -152.0, -174.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2372.0, -1910.0, -2064.0, -2372.0, -1851.0, -814.0, -2372.0, -2372.0, -1920.0, -2228.0] \n",
      " [-154.0, -88.0, -117.0, -132.0, -86.0, -294.0, -166.0, -146.0, -93.0, -143.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2371.0, -2371.0, -1281.0, -738.0, -488.0, -2341.0, -2371.0, -2371.0, -2292.0, -2275.0] \n",
      " [-143.0, -166.0, -202.0, -292.0, -207.0, -133.0, -154.0, -160.0, -147.0, -152.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2372.0, -2163.0, -662.0, -1540.0, -2372.0, -2035.0, -681.0, -2372.0, -2372.0, -2184.0] \n",
      " [-202.0, -131.0, -264.0, -148.0, -142.0, -109.0, -278.0, -202.0, -199.0, -130.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2063.0, -1911.0, -2356.0, -2372.0, -1395.0, -2275.0, -2372.0, -2327.0, -2327.0, -1584.0] \n",
      " [-117.0, -87.0, -149.0, -132.0, -155.0, -152.0, -150.0, -128.0, -128.0, -130.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2356.0, -2227.0, -1170.0, -330.0, -1986.0, -2126.0, -2356.0, -997.0, -2371.0, -2311.0] \n",
      " [-156.0, -143.0, -226.0, -145.0, -99.0, -126.0, -156.0, -306.0, -142.0, -139.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2372.0, -2228.0, -2372.0, -1151.0, -1151.0, -1358.0, -2372.0, -2180.0, -1170.0, -681.0] \n",
      " [-199.0, -143.0, -208.0, -248.0, -248.0, -170.0, -154.0, -129.0, -226.0, -278.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2035.0, -2308.0, -2327.0, -662.0, -615.0, -1967.0, -2372.0, -2372.0, -2215.0, -1909.0] \n",
      " [-109.0, -140.0, -128.0, -264.0, -255.0, -99.0, -208.0, -143.0, -139.0, -88.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2356.0, -2372.0, -2372.0, -522.0, -1987.0, -2372.0, -2308.0, -289.0, -1691.0, -2116.0] \n",
      " [-149.0, -150.0, -132.0, -226.0, -99.0, -141.0, -140.0, -132.0, -90.0, -118.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2372.0, -2041.0, -1720.0, -330.0, -2372.0, -2372.0, -1488.0, -945.0, -2328.0, -2372.0] \n",
      " [-158.0, -116.0, -94.0, -145.0, -166.0, -150.0, -146.0, -296.0, -134.0, -143.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-572.0, -1214.0, -2372.0, -2356.0, -2372.0, -2356.0, -615.0, -2372.0, -2036.0, -2264.0] \n",
      " [-241.0, -212.0, -139.0, -156.0, -142.0, -156.0, -255.0, -215.0, -109.0, -152.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2357.0, -447.0, -2181.0, -486.0, -2264.0, -1835.0, -447.0, -2264.0, -2372.0, -1488.0] \n",
      " [-149.0, -189.0, -129.0, -207.0, -152.0, -95.0, -189.0, -152.0, -142.0, -146.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2371.0, -2371.0, -1965.0, -2119.0, -962.0, -573.0, -903.0, -2371.0, -1539.0, -2342.0] \n",
      " [-158.0, -199.0, -100.0, -122.0, -301.0, -241.0, -290.0, -142.0, -148.0, -133.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-663.0, -522.0, -2371.0, -1772.0, -741.0, -2371.0, -2035.0, -2371.0, -1610.0, -2327.0] \n",
      " [-264.0, -226.0, -143.0, -104.0, -292.0, -147.0, -109.0, -147.0, -123.0, -128.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2309.0, -1752.0, -2371.0, -2371.0, -1805.0, -2229.0, -2236.0, -2295.0, -2193.0, -2371.0] \n",
      " [-140.0, -103.0, -180.0, -154.0, -101.0, -143.0, -146.0, -147.0, -135.0, -156.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2193.0, -2352.0, -1664.0, -1910.0, -1988.0, -2371.0, -2371.0, -1634.0, -1583.0, -2185.0] \n",
      " [-135.0, -138.0, -95.0, -88.0, -99.0, -221.0, -199.0, -101.0, -130.0, -130.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1312.0, -1312.0, -1732.0, -288.0, -1539.0, -1719.0, -683.0, -2193.0, -1053.0, -1610.0] \n",
      " [-188.0, -188.0, -101.0, -132.0, -148.0, -94.0, -278.0, -135.0, -302.0, -123.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2322.0, -2260.0, -2279.0, -2322.0, -1750.0, -2322.0, -709.0, -1585.0, -885.0, -1977.0] \n",
      " [-143.0, -140.0, -134.0, -138.0, -104.0, -208.0, -286.0, -109.0, -290.0, -101.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2307.0, -2136.0, -2322.0, -2322.0, -1257.0, -977.0, -2247.0, -1772.0, -284.0, -1873.0] \n",
      " [-156.0, -129.0, -202.0, -147.0, -202.0, -306.0, -147.0, -99.0, -132.0, -87.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2323.0, -2323.0, -2323.0, -360.0, -1885.0, -2230.0, -385.0, -2323.0, -1674.0, -2323.0] \n",
      " [-137.0, -143.0, -147.0, -159.0, -97.0, -152.0, -174.0, -208.0, -91.0, -142.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2322.0, -943.0, -2322.0, -1994.0, -2322.0, -2189.0, -1771.0, -1881.0, -2322.0, -1068.0] \n",
      " [-180.0, -301.0, -154.0, -109.0, -158.0, -146.0, -99.0, -93.0, -166.0, -290.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2075.0, -2323.0, -2323.0, -2323.0, -2122.0, -513.0, -977.0, -2323.0, -2107.0, -669.0] \n",
      " [-118.0, -142.0, -154.0, -189.0, -131.0, -226.0, -306.0, -166.0, -131.0, -278.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2077.0, -2077.0, -1874.0, -2077.0, -2322.0, -1695.0, -2322.0, -2184.0, -1286.0, -1695.0] \n",
      " [-122.0, -122.0, -87.0, -122.0, -146.0, -101.0, -154.0, -143.0, -188.0, -101.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-924.0, -2023.0, -1979.0, -223.0, -1509.0, -359.0, -2322.0, -2322.0, -2149.0, -2265.0] \n",
      " [-296.0, -117.0, -101.0, -97.0, -148.0, -159.0, -199.0, -144.0, -135.0, -139.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2321.0, -2321.0, -1995.0, -2321.0, -2321.0, -2231.0, -1842.0, -2141.0, -2321.0, -1366.0] \n",
      " [-154.0, -156.0, -109.0, -143.0, -161.0, -152.0, -88.0, -130.0, -160.0, -155.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1901.0, -2281.0, -1147.0, -886.0, -2190.0, -1979.0, -2322.0, -359.0, -1491.0, -2322.0] \n",
      " [-99.0, -128.0, -226.0, -290.0, -146.0, -101.0, -143.0, -159.0, -150.0, -215.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1223.0, -2321.0, -1771.0, -1995.0, -2321.0, -2321.0, -1128.0, -2123.0, -1949.0, -2321.0] \n",
      " [-211.0, -189.0, -99.0, -109.0, -147.0, -154.0, -248.0, -131.0, -99.0, -143.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2321.0, -708.0, -836.0, -2321.0, -2290.0, -441.0, -1034.0, -2321.0, -2321.0, -2262.0] \n",
      " [-156.0, -286.0, -290.0, -215.0, -129.0, -189.0, -302.0, -143.0, -143.0, -140.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-836.0, -2321.0, -1995.0, -1949.0, -1949.0, -2191.0, -2321.0, -1330.0, -1970.0, -2321.0] \n",
      " [-290.0, -150.0, -109.0, -99.0, -99.0, -146.0, -150.0, -170.0, -97.0, -132.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2321.0, -1771.0, -2321.0, -1873.0, -1970.0, -2321.0, -2191.0, -1360.0, -1585.0, -2321.0] \n",
      " [-160.0, -99.0, -215.0, -88.0, -97.0, -133.0, -146.0, -152.0, -109.0, -166.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1771.0, -1771.0, -2321.0, -2308.0, -1435.0, -2321.0, -976.0, -1550.0, -1970.0, -2078.0] \n",
      " [-99.0, -99.0, -156.0, -156.0, -147.0, -155.0, -306.0, -130.0, -97.0, -122.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2138.0, -2308.0, -1147.0, -2321.0, -886.0, -1223.0, -2321.0, -1885.0, -284.0, -2321.0] \n",
      " [-129.0, -149.0, -226.0, -144.0, -290.0, -211.0, -233.0, -97.0, -132.0, -132.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2023.0, -283.0, -2321.0, -797.0, -2321.0, -1367.0, -2321.0, -283.0, -1577.0, -2250.0] \n",
      " [-117.0, -132.0, -143.0, -294.0, -142.0, -155.0, -161.0, -132.0, -123.0, -147.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n",
      "tomate\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1674.0, -1881.0, -2321.0, -1550.0, -1751.0, -2232.0, -796.0, -1674.0, -2142.0, -669.0] \n",
      " [-91.0, -93.0, -208.0, -130.0, -104.0, -152.0, -294.0, -91.0, -130.0, -278.0]\n",
      "Agent 0 : Proba distribution on those states : \n",
      " [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0]]\n",
      "Agent 0 : Proba distribution on the 0 state : \n",
      " [0. 0. 0. 1.]\n"
     ]
    },
    {
     "ename": "com_error",
     "evalue": "(-2147352562, 'Nombre de paramtres non valide.', None, None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mcom_error\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-cb712339892e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0maction_required\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSARSDs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_to_next_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maction_required\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36mstep_to_next_action\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_required\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m                         \u001b[0maction_required\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSarsd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# Udapte all the SCUs nearly simutaneously\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mscu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSCUs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;31m# not a nice way of doing this,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_env_class.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# Udapte all the SCUs nearly simutaneously\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mscu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSCUs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;31m# not a nice way of doing this,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                                         \u001b[1;31m#Compute the state for the RL agent to find the next best agent action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m                                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36mcalculate_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"QueuesSig\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue_state\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_queue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlane\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlane\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVissim_Lanes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m                         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue_state\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_action_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"QueuesSig\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue_state\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_queue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlane\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlane\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVissim_Lanes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m                         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue_state\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_action_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36mget_queue\u001b[1;34m(lane)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mvehicles_in_lane\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlane\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVehs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;31m# Collecte the attribute in queue of the vehicle of the lane and sum them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[0mqueue_in_lane\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvehicle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'InQueue'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvehicle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvehicles_in_lane\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueue_in_lane\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Vissim_SCU_class.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mvehicles_in_lane\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlane\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVehs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;31m# Collecte the attribute in queue of the vehicle of the lane and sum them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[0mqueue_in_lane\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvehicle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'InQueue'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvehicle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvehicles_in_lane\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueue_in_lane\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36mAttValue\u001b[1;34m(self, Attribut, arg1)\u001b[0m\n",
      "\u001b[1;31mcom_error\u001b[0m: (-2147352562, 'Nombre de paramtres non valide.', None, None)"
     ]
    }
   ],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = int(Agents[idx].choose_action(s))\n",
    "\n",
    "\n",
    "for i in range(30000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required :\n",
    "        \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "            if (i+1)%reduce_entropy_every == 0:\n",
    "                if Agents[idx].params['entropy'] >= entropy_threshold :\n",
    "                    Agents[idx].reduce_entropy()\n",
    "                    print (\"Agent {} : Entropy reduced to {} \" .format(idx, Agents[idx].params['entropy']))\n",
    "        \n",
    "    # For the saving , monitoring of the agent\n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, agent in enumerate(Agents):\n",
    "            predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "            print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "            print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "            print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "            agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Agents[0].model.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Single_Cross_Triple 8 actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name  = 'Single_Cross_Triple'\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "sim_length = 3601\n",
    "\n",
    "# all controller actions\n",
    "Single_Cross_Triple_dictionary8 =\\\n",
    "{\\\n",
    "    # Controller Number 0 \n",
    "    0 : {'compatible_actions' : {    0 : [1,1,1,0,0,0,0,0,0,0,0,0],\n",
    "                                     1 : [0,0,0,1,1,1,0,0,0,0,0,0],\n",
    "                                     2 : [0,0,0,0,0,0,1,1,1,0,0,0],\n",
    "                                     3 : [0,0,0,0,0,0,0,0,0,1,1,1],\n",
    "                                     4 : [1,0,0,0,0,0,1,0,0,0,0,0],\n",
    "                                     5 : [0,0,0,1,0,0,0,0,0,1,0,0],\n",
    "                                     6 : [0,1,1,0,0,0,0,1,1,0,0,0],\n",
    "                                     7 : [0,0,0,0,1,1,0,0,0,0,1,1]},\n",
    " \n",
    "         'link' : [1, 3, 5, 7],\n",
    "         'lane' : ['1-1', '1-2', '1-3', '3-1', '3-2', '3-3', '5-1', '5-2', '5-3', '7-1', '7-2', '7-3'],\n",
    "         \n",
    "         'controled_by_com' : True,\n",
    "         'green_time' : 6,\n",
    "         'redamber_time' : 0,\n",
    "         'amber_time' : 3, \n",
    "         'red_time' : 0,\n",
    "         'state_size' : [12],\n",
    "         'state_type' : 'Queues',\n",
    "         'reward_type' : 'Queues' \n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = env(model_name, vissim_working_directory, sim_length, Single_Cross_Triple_dictionary8,\\\n",
    "            timesteps_per_second = 1, mode = 'training', delete_results = True, verbose = True)\n",
    "\n",
    "env.SCUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.85\n",
    "alpha = 0.00005\n",
    "ID = 0\n",
    "value = 25\n",
    "entropy = 2000\n",
    "n_step_size = 16\n",
    "state_size = [12]\n",
    "reduce_entropy_every = 100\n",
    "\n",
    "\n",
    "\n",
    "# for the monitoring\n",
    "horizon = 100\n",
    "n_sample = 10\n",
    "\n",
    "Agents = []\n",
    "\n",
    "for idx, info in Single_Cross_Triple_dictionary8.items():\n",
    "        acts = info['compatible_actions']\n",
    "        Agent = ACAgent(state_size, len(acts), ID, n_step_size, gamma, alpha, entropy, value)\n",
    "        Agents.append(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_state = env.get_state()\n",
    "actions = {}\n",
    "for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)\n",
    "\n",
    "for i in range(100000):\n",
    "    action_required, SARSDs = env.step_to_next_action(actions)\n",
    "    if action_required : \n",
    "        actions = dict()\n",
    "        for idx , sarsd in SARSDs.items():\n",
    "            s,a,r,ns,d = sarsd\n",
    "            \n",
    "            #print(sarsd)\n",
    "            Agents[idx].remember(s,a,r,ns,d)\n",
    "            if len(Agents[idx].memory) >= Agents[idx].n_step_size :\n",
    "                Agents[idx].learn() \n",
    "            \n",
    "            # in order to find the next action you need to evaluate the \"next_state\" because it is the current state of the simulator\n",
    "            actions[idx] = int(Agents[idx].choose_action(ns))\n",
    "            #print(actions)\n",
    "            \n",
    "            if (i+1)%reduce_entropy_every == 0:\n",
    "                Agents[idx].reduce_entropy()\n",
    "                print (\"Agent {} : Entropy reduced to {} \" .format(idx, Agents[idx].params['entropy']))\n",
    "            \n",
    "    \n",
    "    # For the saving , monitoring of the agent\n",
    "    if env.done :\n",
    "        env.reset()\n",
    "        for idx, agent in enumerate(Agents):\n",
    "                    predicted_values, true_values, proba0, probas = agent.value_check(horizon, n_sample)\n",
    "                    print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(idx, predicted_values, true_values))\n",
    "                    print (\"Agent {} : Proba distribution on those states : \\n {}\" .format(idx, probas))\n",
    "                    print (\"Agent {} : Proba distribution on the 0 state : \\n {}\" .format(idx, proba0))\n",
    "                    agent.reset()\n",
    "                    \n",
    "        \n",
    "        for idx, s in start_state.items():\n",
    "            actions[idx] = Agents[idx].choose_action(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
