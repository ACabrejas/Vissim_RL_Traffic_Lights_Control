{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (DQN) - Cartpole Task\n",
    "\n",
    "* The goal is for agent to learn keep the pole attached to a cart upright as long as possible. The agent does so by choosing between two actions: 'move the cart left or right'.\n",
    "\n",
    "* In reinforcement learning the agent learns how to act or take action by trail and error. In other words, by trying out and action and receving a feedback/reward. It uses the feedback/reward to learn if taking an action in a particular senario (state) was good or bad. The model aim to maximise the total accumulated reward. \n",
    "\n",
    "* Current version just uses CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name np_utils",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5df16bb94776>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeque\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\acabrejasegea\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissim\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\acabrejasegea\\AppData\\Local\\Continuum\\anaconda3\\envs\\vissim\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name np_utils"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from time import time\n",
    "from keras.callbacks import TensorBoard\n",
    "from collections import deque, namedtuple\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Multiply, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 0.99\n",
    "TARGET_UPDATE = 100\n",
    "LEARNING_RATE = 0.001\n",
    "EPISODES = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP transition represented as a named tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay memory (a database to store input data and to sample from to create a training batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.memory = deque(maxlen = capacity)\n",
    "        \n",
    "    def add(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size=10):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def get_memory(self):\n",
    "        return self.memory\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Huber Loss Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _huber_loss(y_true, y_pred, clip_delta=1.0):\n",
    "    error = y_true - y_pred\n",
    "    cond  = K.abs(error) <= clip_delta\n",
    "    squared_loss = 0.5 * K.square(error)\n",
    "    quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
    "    return K.mean(tf.where(cond, squared_loss, quadratic_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "states (InputLayer)             (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           250         states[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 50)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            102         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mask (InputLayer)               (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 2)            0           dense_2[0][0]                    \n",
      "                                                                 mask[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 352\n",
      "Trainable params: 352\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, state_size, action_size):\n",
    "\n",
    "        # create the neural network.\n",
    "        states_input = Input((state_size,), name='states')\n",
    "        actions_input = Input((action_size,), name='mask')\n",
    "\n",
    "        fc1 = Dense(24, activation='tanh')(states_input)\n",
    "        #dropout_1 = Dropout(0.2)(fc1)\n",
    "        fc2 = Dense(24, activation='tanh')(fc1)\n",
    "        #dropout_2 = Dropout(0.2)(fc2)\n",
    "        out = Dense(action_size, activation='linear')(fc2)\n",
    "        filtered_output = Multiply()([out, actions_input])\n",
    "        self.model = Model(inputs=[states_input, actions_input], outputs=filtered_output)\n",
    "        self.model.compile(loss=_huber_loss,optimizer=Adam(lr=LEARNING_RATE, clipnorm=1.))\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "        \n",
    "\n",
    "#Test\n",
    "net = Network(4, 2)\n",
    "model = net.get_model()\n",
    "print(model.summary())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[99m Episode 0 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7923\n",
      "\u001b[99m Episode 1 finished after 63 steps  and with total reward -38.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5732\n",
      "\u001b[99m Episode 2 finished after 59 steps  and with total reward -42.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0092\n",
      "\u001b[99m Episode 3 finished after 69 steps  and with total reward -32.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7917\n",
      "\u001b[99m Episode 4 finished after 88 steps  and with total reward -13.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0050\n",
      "\u001b[99m Episode 5 finished after 64 steps  and with total reward -37.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7875\n",
      "\u001b[99m Episode 6 finished after 79 steps  and with total reward -22.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0028\n",
      "\u001b[99m Episode 7 finished after 83 steps  and with total reward -18.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7900\n",
      "\u001b[99m Episode 8 finished after 49 steps  and with total reward -52.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7879\n",
      "\u001b[99m Episode 9 finished after 96 steps  and with total reward -5.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 0.7872\n",
      "\u001b[99m Episode 10 finished after 63 steps  and with total reward -38.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5799\n",
      "\u001b[99m Episode 11 finished after 73 steps  and with total reward -28.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7898\n",
      "\u001b[99m Episode 12 finished after 86 steps  and with total reward -15.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7934\n",
      "\u001b[99m Episode 13 finished after 69 steps  and with total reward -32.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7873\n",
      "\u001b[99m Episode 14 finished after 71 steps  and with total reward -30.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7913\n",
      "\u001b[99m Episode 15 finished after 43 steps  and with total reward -58.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7883\n",
      "\u001b[99m Episode 16 finished after 61 steps  and with total reward -40.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0037\n",
      "\u001b[99m Episode 17 finished after 60 steps  and with total reward -41.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7929\n",
      "\u001b[99m Episode 18 finished after 61 steps  and with total reward -40.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0039\n",
      "\u001b[99m Episode 19 finished after 98 steps  and with total reward -3.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7868\n",
      "\u001b[99m Episode 20 finished after 78 steps  and with total reward -23.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0043\n",
      "\u001b[99m Episode 21 finished after 64 steps  and with total reward -37.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7915\n",
      "\u001b[99m Episode 22 finished after 83 steps  and with total reward -18.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0051\n",
      "\u001b[99m Episode 23 finished after 89 steps  and with total reward -12.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7932\n",
      "\u001b[99m Episode 24 finished after 75 steps  and with total reward -26.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0052\n",
      "\u001b[99m Episode 25 finished after 77 steps  and with total reward -24.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0050\n",
      "\u001b[99m Episode 26 finished after 59 steps  and with total reward -42.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.0039\n",
      "\u001b[99m Episode 27 finished after 62 steps  and with total reward -39.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 62us/step - loss: 0.7876\n",
      "\u001b[99m Episode 28 finished after 83 steps  and with total reward -18.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7907\n",
      "\u001b[99m Episode 29 finished after 96 steps  and with total reward -5.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7864\n",
      "\u001b[99m Episode 30 finished after 73 steps  and with total reward -28.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5639\n",
      "\u001b[99m Episode 31 finished after 94 steps  and with total reward -7.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7828\n",
      "\u001b[99m Episode 32 finished after 57 steps  and with total reward -44.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7913\n",
      "\u001b[99m Episode 33 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0025\n",
      "\u001b[99m Episode 34 finished after 85 steps  and with total reward -16.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7853\n",
      "\u001b[99m Episode 35 finished after 93 steps  and with total reward -8.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7921\n",
      "\u001b[99m Episode 36 finished after 58 steps  and with total reward -43.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0033\n",
      "\u001b[99m Episode 37 finished after 58 steps  and with total reward -43.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 2.3557\n",
      "\u001b[99m Episode 38 finished after 60 steps  and with total reward -41.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 62us/step - loss: 0.0022\n",
      "\u001b[99m Episode 39 finished after 67 steps  and with total reward -34.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7875\n",
      "\u001b[99m Episode 40 finished after 85 steps  and with total reward -16.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7897\n",
      "\u001b[99m Episode 41 finished after 63 steps  and with total reward -38.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0022\n",
      "\u001b[99m Episode 42 finished after 72 steps  and with total reward -29.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.0029\n",
      "\u001b[99m Episode 43 finished after 100 steps  and with total reward -1.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7905\n",
      "\u001b[99m Episode 44 finished after 62 steps  and with total reward -39.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5787\n",
      "\u001b[99m Episode 45 finished after 69 steps  and with total reward -32.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0021\n",
      "\u001b[99m Episode 46 finished after 55 steps  and with total reward -46.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7903\n",
      "\u001b[99m Episode 47 finished after 57 steps  and with total reward -44.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 2.3640\n",
      "\u001b[99m Episode 48 finished after 76 steps  and with total reward -25.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0040\n",
      "\u001b[99m Episode 49 finished after 64 steps  and with total reward -37.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7834\n",
      "\u001b[99m Episode 50 finished after 91 steps  and with total reward -10.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5716\n",
      "\u001b[99m Episode 51 finished after 93 steps  and with total reward -8.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0023\n",
      "\u001b[99m Episode 52 finished after 88 steps  and with total reward -13.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 0.0039\n",
      "\u001b[99m Episode 53 finished after 70 steps  and with total reward -31.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0028\n",
      "\u001b[99m Episode 54 finished after 75 steps  and with total reward -26.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 55 finished after 80 steps  and with total reward -21.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0041\n",
      "\u001b[99m Episode 56 finished after 74 steps  and with total reward -27.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0037\n",
      "\u001b[99m Episode 57 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5688\n",
      "\u001b[99m Episode 58 finished after 89 steps  and with total reward -12.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0026\n",
      "\u001b[99m Episode 59 finished after 85 steps  and with total reward -16.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5769\n",
      "\u001b[99m Episode 60 finished after 66 steps  and with total reward -35.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0049\n",
      "\u001b[99m Episode 61 finished after 63 steps  and with total reward -38.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0026\n",
      "\u001b[99m Episode 62 finished after 77 steps  and with total reward -24.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7926\n",
      "\u001b[99m Episode 63 finished after 67 steps  and with total reward -34.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7924\n",
      "\u001b[99m Episode 64 finished after 75 steps  and with total reward -26.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5702\n",
      "\u001b[99m Episode 65 finished after 79 steps  and with total reward -22.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7932\n",
      "\u001b[99m Episode 66 finished after 58 steps  and with total reward -43.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7842\n",
      "\u001b[99m Episode 67 finished after 80 steps  and with total reward -21.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0049\n",
      "\u001b[99m Episode 68 finished after 78 steps  and with total reward -23.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7862\n",
      "\u001b[99m Episode 69 finished after 70 steps  and with total reward -31.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7920\n",
      "\u001b[99m Episode 70 finished after 72 steps  and with total reward -29.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0022\n",
      "\u001b[99m Episode 71 finished after 58 steps  and with total reward -43.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5701\n",
      "\u001b[99m Episode 72 finished after 67 steps  and with total reward -34.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.7888\n",
      "\u001b[99m Episode 73 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7886\n",
      "\u001b[99m Episode 74 finished after 64 steps  and with total reward -37.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5772\n",
      "\u001b[99m Episode 75 finished after 75 steps  and with total reward -26.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 1.5720\n",
      "\u001b[99m Episode 76 finished after 74 steps  and with total reward -27.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7906\n",
      "\u001b[99m Episode 77 finished after 58 steps  and with total reward -43.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7843\n",
      "\u001b[99m Episode 78 finished after 94 steps  and with total reward -7.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0022\n",
      "\u001b[99m Episode 79 finished after 81 steps  and with total reward -20.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0016\n",
      "\u001b[99m Episode 80 finished after 76 steps  and with total reward -25.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7887\n",
      "\u001b[99m Episode 81 finished after 75 steps  and with total reward -26.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0034\n",
      "\u001b[99m Episode 82 finished after 86 steps  and with total reward -15.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5697\n",
      "\u001b[99m Episode 83 finished after 48 steps  and with total reward -53.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7876\n",
      "\u001b[99m Episode 84 finished after 70 steps  and with total reward -31.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.7868\n",
      "\u001b[99m Episode 85 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 2.3515\n",
      "\u001b[99m Episode 86 finished after 60 steps  and with total reward -41.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5691\n",
      "\u001b[99m Episode 87 finished after 55 steps  and with total reward -46.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 3.9327\n",
      "\u001b[99m Episode 88 finished after 74 steps  and with total reward -27.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0024\n",
      "\u001b[99m Episode 89 finished after 74 steps  and with total reward -27.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.0038\n",
      "\u001b[99m Episode 90 finished after 67 steps  and with total reward -34.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 62us/step - loss: 2.3579\n",
      "\u001b[99m Episode 91 finished after 71 steps  and with total reward -30.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7872\n",
      "\u001b[99m Episode 92 finished after 53 steps  and with total reward -48.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7841\n",
      "\u001b[99m Episode 93 finished after 73 steps  and with total reward -28.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 2.3642\n",
      "\u001b[99m Episode 94 finished after 62 steps  and with total reward -39.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 0.0026\n",
      "\u001b[99m Episode 95 finished after 62 steps  and with total reward -39.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 30us/step - loss: 0.7889\n",
      "\u001b[99m Episode 96 finished after 68 steps  and with total reward -33.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7900\n",
      "\u001b[99m Episode 97 finished after 76 steps  and with total reward -25.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7864\n",
      "\u001b[99m Episode 98 finished after 61 steps  and with total reward -40.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0028\n",
      "\u001b[99m Episode 99 finished after 48 steps  and with total reward -53.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0038\n",
      "\u001b[99m Episode 100 finished after 58 steps  and with total reward -43.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 1.5704\n",
      "\u001b[99m Episode 101 finished after 60 steps  and with total reward -41.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0026\n",
      "\u001b[99m Episode 102 finished after 85 steps  and with total reward -16.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5686\n",
      "\u001b[99m Episode 103 finished after 64 steps  and with total reward -37.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.7878\n",
      "\u001b[99m Episode 104 finished after 73 steps  and with total reward -28.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0047\n",
      "\u001b[99m Episode 105 finished after 63 steps  and with total reward -38.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0046\n",
      "\u001b[99m Episode 106 finished after 105 steps  and with total reward 4.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0020\n",
      "\u001b[99m Episode 107 finished after 56 steps  and with total reward -45.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.7851\n",
      "\u001b[99m Episode 108 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0028\n",
      "\u001b[99m Episode 109 finished after 70 steps  and with total reward -31.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 110 finished after 44 steps  and with total reward -57.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 1.5651\n",
      "\u001b[99m Episode 111 finished after 52 steps  and with total reward -49.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7892\n",
      "\u001b[99m Episode 112 finished after 87 steps  and with total reward -14.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0032\n",
      "\u001b[99m Episode 113 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7839\n",
      "\u001b[99m Episode 114 finished after 60 steps  and with total reward -41.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7868\n",
      "\u001b[99m Episode 115 finished after 91 steps  and with total reward -10.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.0037\n",
      "\u001b[99m Episode 116 finished after 52 steps  and with total reward -49.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0029\n",
      "\u001b[99m Episode 117 finished after 53 steps  and with total reward -48.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7863\n",
      "\u001b[99m Episode 118 finished after 84 steps  and with total reward -17.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0018\n",
      "\u001b[99m Episode 119 finished after 57 steps  and with total reward -44.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5715\n",
      "\u001b[99m Episode 120 finished after 85 steps  and with total reward -16.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0022\n",
      "\u001b[99m Episode 121 finished after 59 steps  and with total reward -42.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5634\n",
      "\u001b[99m Episode 122 finished after 93 steps  and with total reward -8.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.0036\n",
      "\u001b[99m Episode 123 finished after 72 steps  and with total reward -29.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 62us/step - loss: 0.7928\n",
      "\u001b[99m Episode 124 finished after 82 steps  and with total reward -19.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0017\n",
      "\u001b[99m Episode 125 finished after 63 steps  and with total reward -38.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0027\n",
      "\u001b[99m Episode 126 finished after 55 steps  and with total reward -46.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7827\n",
      "\u001b[99m Episode 127 finished after 85 steps  and with total reward -16.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7866\n",
      "\u001b[99m Episode 128 finished after 114 steps  and with total reward 13.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7823\n",
      "\u001b[99m Episode 129 finished after 75 steps  and with total reward -26.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7892\n",
      "\u001b[99m Episode 130 finished after 49 steps  and with total reward -52.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0034\n",
      "\u001b[99m Episode 131 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5753\n",
      "\u001b[99m Episode 132 finished after 66 steps  and with total reward -35.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5671\n",
      "\u001b[99m Episode 133 finished after 69 steps  and with total reward -32.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7897\n",
      "\u001b[99m Episode 134 finished after 63 steps  and with total reward -38.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 0.7889\n",
      "\u001b[99m Episode 135 finished after 75 steps  and with total reward -26.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0018\n",
      "\u001b[99m Episode 136 finished after 63 steps  and with total reward -38.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0024\n",
      "\u001b[99m Episode 137 finished after 64 steps  and with total reward -37.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.0026\n",
      "\u001b[99m Episode 138 finished after 57 steps  and with total reward -44.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 2.3507\n",
      "\u001b[99m Episode 139 finished after 66 steps  and with total reward -35.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 30us/step - loss: 0.0020\n",
      "\u001b[99m Episode 140 finished after 77 steps  and with total reward -24.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7855\n",
      "\u001b[99m Episode 141 finished after 51 steps  and with total reward -50.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5708\n",
      "\u001b[99m Episode 142 finished after 59 steps  and with total reward -42.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.0020\n",
      "\u001b[99m Episode 143 finished after 76 steps  and with total reward -25.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5686\n",
      "\u001b[99m Episode 144 finished after 77 steps  and with total reward -24.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0025\n",
      "\u001b[99m Episode 145 finished after 76 steps  and with total reward -25.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.7893\n",
      "\u001b[99m Episode 146 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.7821\n",
      "\u001b[99m Episode 147 finished after 67 steps  and with total reward -34.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 1.5682\n",
      "\u001b[99m Episode 148 finished after 67 steps  and with total reward -34.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0025\n",
      "\u001b[99m Episode 149 finished after 64 steps  and with total reward -37.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 0.7908\n",
      "\u001b[99m Episode 150 finished after 61 steps  and with total reward -40.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 2.3543\n",
      "\u001b[99m Episode 151 finished after 94 steps  and with total reward -7.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7872\n",
      "\u001b[99m Episode 152 finished after 86 steps  and with total reward -15.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0017\n",
      "\u001b[99m Episode 153 finished after 75 steps  and with total reward -26.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7880\n",
      "\u001b[99m Episode 154 finished after 113 steps  and with total reward 12.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7821\n",
      "\u001b[99m Episode 155 finished after 74 steps  and with total reward -27.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7881\n",
      "\u001b[99m Episode 156 finished after 61 steps  and with total reward -40.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.7845\n",
      "\u001b[99m Episode 157 finished after 73 steps  and with total reward -28.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0031\n",
      "\u001b[99m Episode 158 finished after 81 steps  and with total reward -20.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0027\n",
      "\u001b[99m Episode 159 finished after 55 steps  and with total reward -46.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5721\n",
      "\u001b[99m Episode 160 finished after 63 steps  and with total reward -38.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 62us/step - loss: 0.7882\n",
      "\u001b[99m Episode 161 finished after 55 steps  and with total reward -46.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0023\n",
      "\u001b[99m Episode 162 finished after 70 steps  and with total reward -31.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7913\n",
      "\u001b[99m Episode 163 finished after 90 steps  and with total reward -11.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5683\n",
      "\u001b[99m Episode 164 finished after 97 steps  and with total reward -4.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 165 finished after 73 steps  and with total reward -28.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.0014\n",
      "\u001b[99m Episode 166 finished after 75 steps  and with total reward -26.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7852\n",
      "\u001b[99m Episode 167 finished after 61 steps  and with total reward -40.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 0.0023\n",
      "\u001b[99m Episode 168 finished after 61 steps  and with total reward -40.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7893\n",
      "\u001b[99m Episode 169 finished after 73 steps  and with total reward -28.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 2.3570\n",
      "\u001b[99m Episode 170 finished after 83 steps  and with total reward -18.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0020\n",
      "\u001b[99m Episode 171 finished after 77 steps  and with total reward -24.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0036\n",
      "\u001b[99m Episode 172 finished after 69 steps  and with total reward -32.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 32us/step - loss: 0.7819\n",
      "\u001b[99m Episode 173 finished after 92 steps  and with total reward -9.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0015\n",
      "\u001b[99m Episode 174 finished after 66 steps  and with total reward -35.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 2.3599\n",
      "\u001b[99m Episode 175 finished after 52 steps  and with total reward -49.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0036\n",
      "\u001b[99m Episode 176 finished after 66 steps  and with total reward -35.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0026\n",
      "\u001b[99m Episode 177 finished after 59 steps  and with total reward -42.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 0.0026\n",
      "\u001b[99m Episode 178 finished after 64 steps  and with total reward -37.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0023\n",
      "\u001b[99m Episode 179 finished after 61 steps  and with total reward -40.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7901\n",
      "\u001b[99m Episode 180 finished after 51 steps  and with total reward -50.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5724\n",
      "\u001b[99m Episode 181 finished after 59 steps  and with total reward -42.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5668\n",
      "\u001b[99m Episode 182 finished after 69 steps  and with total reward -32.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7849\n",
      "\u001b[99m Episode 183 finished after 78 steps  and with total reward -23.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7878\n",
      "\u001b[99m Episode 184 finished after 63 steps  and with total reward -38.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5744\n",
      "\u001b[99m Episode 185 finished after 59 steps  and with total reward -42.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0025\n",
      "\u001b[99m Episode 186 finished after 71 steps  and with total reward -30.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5685\n",
      "\u001b[99m Episode 187 finished after 75 steps  and with total reward -26.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0020\n",
      "\u001b[99m Episode 188 finished after 57 steps  and with total reward -44.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 2.3625\n",
      "\u001b[99m Episode 189 finished after 53 steps  and with total reward -48.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7844\n",
      "\u001b[99m Episode 190 finished after 62 steps  and with total reward -39.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 63us/step - loss: 0.7921\n",
      "\u001b[99m Episode 191 finished after 56 steps  and with total reward -45.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.0037\n",
      "\u001b[99m Episode 192 finished after 43 steps  and with total reward -58.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.7901\n",
      "\u001b[99m Episode 193 finished after 77 steps  and with total reward -24.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0028\n",
      "\u001b[99m Episode 194 finished after 87 steps  and with total reward -14.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0021\n",
      "\u001b[99m Episode 195 finished after 41 steps  and with total reward -60.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 62us/step - loss: 0.7858\n",
      "\u001b[99m Episode 196 finished after 100 steps  and with total reward -1.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 1.5683\n",
      "\u001b[99m Episode 197 finished after 74 steps  and with total reward -27.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.0020\n",
      "\u001b[99m Episode 198 finished after 70 steps  and with total reward -31.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7915\n",
      "\u001b[99m Episode 199 finished after 55 steps  and with total reward -46.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0025\n",
      "\u001b[99m Episode 200 finished after 71 steps  and with total reward -30.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0023\n",
      "\u001b[99m Episode 201 finished after 57 steps  and with total reward -44.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 62us/step - loss: 0.7886\n",
      "\u001b[99m Episode 202 finished after 76 steps  and with total reward -25.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 1.5709\n",
      "\u001b[99m Episode 203 finished after 77 steps  and with total reward -24.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.7901\n",
      "\u001b[99m Episode 204 finished after 69 steps  and with total reward -32.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 0.0027\n",
      "\u001b[99m Episode 205 finished after 75 steps  and with total reward -26.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.0020\n",
      "\u001b[99m Episode 206 finished after 63 steps  and with total reward -38.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 46us/step - loss: 0.0017\n",
      "\u001b[99m Episode 207 finished after 77 steps  and with total reward -24.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 0.0035\n",
      "\u001b[99m Episode 208 finished after 92 steps  and with total reward -9.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0018\n",
      "\u001b[99m Episode 209 finished after 61 steps  and with total reward -40.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0026\n",
      "\u001b[99m Episode 210 finished after 48 steps  and with total reward -53.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7888\n",
      "\u001b[99m Episode 211 finished after 56 steps  and with total reward -45.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 63us/step - loss: 0.0023\n",
      "\u001b[99m Episode 212 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7875\n",
      "\u001b[99m Episode 213 finished after 83 steps  and with total reward -18.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 30us/step - loss: 2.3566\n",
      "\u001b[99m Episode 214 finished after 84 steps  and with total reward -17.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0020\n",
      "\u001b[99m Episode 215 finished after 75 steps  and with total reward -26.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0017\n",
      "\u001b[99m Episode 216 finished after 80 steps  and with total reward -21.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7830\n",
      "\u001b[99m Episode 217 finished after 89 steps  and with total reward -12.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5746\n",
      "\u001b[99m Episode 218 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7847\n",
      "\u001b[99m Episode 219 finished after 67 steps  and with total reward -34.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.7828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 220 finished after 71 steps  and with total reward -30.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5766\n",
      "\u001b[99m Episode 221 finished after 61 steps  and with total reward -40.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7830\n",
      "\u001b[99m Episode 222 finished after 79 steps  and with total reward -22.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0022\n",
      "\u001b[99m Episode 223 finished after 92 steps  and with total reward -9.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0027\n",
      "\u001b[99m Episode 224 finished after 79 steps  and with total reward -22.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5732\n",
      "\u001b[99m Episode 225 finished after 57 steps  and with total reward -44.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0015\n",
      "\u001b[99m Episode 226 finished after 62 steps  and with total reward -39.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7892\n",
      "\u001b[99m Episode 227 finished after 66 steps  and with total reward -35.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 2.3468\n",
      "\u001b[99m Episode 228 finished after 79 steps  and with total reward -22.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5751\n",
      "\u001b[99m Episode 229 finished after 79 steps  and with total reward -22.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0029\n",
      "\u001b[99m Episode 230 finished after 80 steps  and with total reward -21.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7880\n",
      "\u001b[99m Episode 231 finished after 73 steps  and with total reward -28.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.7866\n",
      "\u001b[99m Episode 232 finished after 71 steps  and with total reward -30.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0030\n",
      "\u001b[99m Episode 233 finished after 77 steps  and with total reward -24.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0020\n",
      "\u001b[99m Episode 234 finished after 79 steps  and with total reward -22.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0027\n",
      "\u001b[99m Episode 235 finished after 88 steps  and with total reward -13.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0020\n",
      "\u001b[99m Episode 236 finished after 66 steps  and with total reward -35.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7820\n",
      "\u001b[99m Episode 237 finished after 69 steps  and with total reward -32.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 2.3543\n",
      "\u001b[99m Episode 238 finished after 81 steps  and with total reward -20.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5831\n",
      "\u001b[99m Episode 239 finished after 74 steps  and with total reward -27.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7833\n",
      "\u001b[99m Episode 240 finished after 67 steps  and with total reward -34.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 3.1420\n",
      "\u001b[99m Episode 241 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 2.3570\n",
      "\u001b[99m Episode 242 finished after 81 steps  and with total reward -20.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 0.7855\n",
      "\u001b[99m Episode 243 finished after 64 steps  and with total reward -37.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7873\n",
      "\u001b[99m Episode 244 finished after 75 steps  and with total reward -26.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7911\n",
      "\u001b[99m Episode 245 finished after 101 steps  and with total reward 0.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7913\n",
      "\u001b[99m Episode 246 finished after 64 steps  and with total reward -37.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0025\n",
      "\u001b[99m Episode 247 finished after 90 steps  and with total reward -11.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7872\n",
      "\u001b[99m Episode 248 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0029\n",
      "\u001b[99m Episode 249 finished after 57 steps  and with total reward -44.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5649\n",
      "\u001b[99m Episode 250 finished after 82 steps  and with total reward -19.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7885\n",
      "\u001b[99m Episode 251 finished after 72 steps  and with total reward -29.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0030\n",
      "\u001b[99m Episode 252 finished after 60 steps  and with total reward -41.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7885\n",
      "\u001b[99m Episode 253 finished after 87 steps  and with total reward -14.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5807\n",
      "\u001b[99m Episode 254 finished after 51 steps  and with total reward -50.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5731\n",
      "\u001b[99m Episode 255 finished after 76 steps  and with total reward -25.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0015\n",
      "\u001b[99m Episode 256 finished after 51 steps  and with total reward -50.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5662\n",
      "\u001b[99m Episode 257 finished after 74 steps  and with total reward -27.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7848\n",
      "\u001b[99m Episode 258 finished after 81 steps  and with total reward -20.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7844\n",
      "\u001b[99m Episode 259 finished after 67 steps  and with total reward -34.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5637\n",
      "\u001b[99m Episode 260 finished after 81 steps  and with total reward -20.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7907\n",
      "\u001b[99m Episode 261 finished after 76 steps  and with total reward -25.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0028\n",
      "\u001b[99m Episode 262 finished after 61 steps  and with total reward -40.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7842\n",
      "\u001b[99m Episode 263 finished after 82 steps  and with total reward -19.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5708\n",
      "\u001b[99m Episode 264 finished after 67 steps  and with total reward -34.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0023\n",
      "\u001b[99m Episode 265 finished after 75 steps  and with total reward -26.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0021\n",
      "\u001b[99m Episode 266 finished after 67 steps  and with total reward -34.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5696\n",
      "\u001b[99m Episode 267 finished after 46 steps  and with total reward -55.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5763\n",
      "\u001b[99m Episode 268 finished after 76 steps  and with total reward -25.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0017\n",
      "\u001b[99m Episode 269 finished after 78 steps  and with total reward -23.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 2.3615\n",
      "\u001b[99m Episode 270 finished after 64 steps  and with total reward -37.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0024\n",
      "\u001b[99m Episode 271 finished after 77 steps  and with total reward -24.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0026\n",
      "\u001b[99m Episode 272 finished after 66 steps  and with total reward -35.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0018\n",
      "\u001b[99m Episode 273 finished after 63 steps  and with total reward -38.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5701\n",
      "\u001b[99m Episode 274 finished after 64 steps  and with total reward -37.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 275 finished after 67 steps  and with total reward -34.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.7849\n",
      "\u001b[99m Episode 276 finished after 71 steps  and with total reward -30.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0028\n",
      "\u001b[99m Episode 277 finished after 87 steps  and with total reward -14.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5740\n",
      "\u001b[99m Episode 278 finished after 73 steps  and with total reward -28.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7921\n",
      "\u001b[99m Episode 279 finished after 84 steps  and with total reward -17.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5718\n",
      "\u001b[99m Episode 280 finished after 52 steps  and with total reward -49.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0031\n",
      "\u001b[99m Episode 281 finished after 99 steps  and with total reward -2.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 0.7886\n",
      "\u001b[99m Episode 282 finished after 67 steps  and with total reward -34.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 0.7889\n",
      "\u001b[99m Episode 283 finished after 72 steps  and with total reward -29.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5722\n",
      "\u001b[99m Episode 284 finished after 70 steps  and with total reward -31.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7880\n",
      "\u001b[99m Episode 285 finished after 103 steps  and with total reward 2.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0023\n",
      "\u001b[99m Episode 286 finished after 60 steps  and with total reward -41.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7875\n",
      "\u001b[99m Episode 287 finished after 52 steps  and with total reward -49.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5664\n",
      "\u001b[99m Episode 288 finished after 82 steps  and with total reward -19.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0031\n",
      "\u001b[99m Episode 289 finished after 48 steps  and with total reward -53.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5656\n",
      "\u001b[99m Episode 290 finished after 51 steps  and with total reward -50.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 0.0030\n",
      "\u001b[99m Episode 291 finished after 76 steps  and with total reward -25.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7860\n",
      "\u001b[99m Episode 292 finished after 72 steps  and with total reward -29.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7883\n",
      "\u001b[99m Episode 293 finished after 59 steps  and with total reward -42.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7843\n",
      "\u001b[99m Episode 294 finished after 68 steps  and with total reward -33.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5720\n",
      "\u001b[99m Episode 295 finished after 62 steps  and with total reward -39.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 0.7895\n",
      "\u001b[99m Episode 296 finished after 61 steps  and with total reward -40.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0022\n",
      "\u001b[99m Episode 297 finished after 66 steps  and with total reward -35.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0023\n",
      "\u001b[99m Episode 298 finished after 46 steps  and with total reward -55.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0025\n",
      "\u001b[99m Episode 299 finished after 74 steps  and with total reward -27.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0013\n",
      "\u001b[99m Episode 300 finished after 62 steps  and with total reward -39.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0019\n",
      "\u001b[99m Episode 301 finished after 75 steps  and with total reward -26.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7838\n",
      "\u001b[99m Episode 302 finished after 75 steps  and with total reward -26.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7874\n",
      "\u001b[99m Episode 303 finished after 60 steps  and with total reward -41.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0019\n",
      "\u001b[99m Episode 304 finished after 66 steps  and with total reward -35.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5729\n",
      "\u001b[99m Episode 305 finished after 68 steps  and with total reward -33.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7851\n",
      "\u001b[99m Episode 306 finished after 83 steps  and with total reward -18.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 0.0020\n",
      "\u001b[99m Episode 307 finished after 72 steps  and with total reward -29.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 1.5690\n",
      "\u001b[99m Episode 308 finished after 87 steps  and with total reward -14.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7873\n",
      "\u001b[99m Episode 309 finished after 91 steps  and with total reward -10.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.0020\n",
      "\u001b[99m Episode 310 finished after 91 steps  and with total reward -10.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0023\n",
      "\u001b[99m Episode 311 finished after 68 steps  and with total reward -33.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.0015\n",
      "\u001b[99m Episode 312 finished after 61 steps  and with total reward -40.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 1.5729\n",
      "\u001b[99m Episode 313 finished after 66 steps  and with total reward -35.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7835\n",
      "\u001b[99m Episode 314 finished after 90 steps  and with total reward -11.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5773\n",
      "\u001b[99m Episode 315 finished after 76 steps  and with total reward -25.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5740\n",
      "\u001b[99m Episode 316 finished after 66 steps  and with total reward -35.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0020\n",
      "\u001b[99m Episode 317 finished after 62 steps  and with total reward -39.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7862\n",
      "\u001b[99m Episode 318 finished after 80 steps  and with total reward -21.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0024\n",
      "\u001b[99m Episode 319 finished after 85 steps  and with total reward -16.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0021\n",
      "\u001b[99m Episode 320 finished after 49 steps  and with total reward -52.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5736\n",
      "\u001b[99m Episode 321 finished after 72 steps  and with total reward -29.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7901\n",
      "\u001b[99m Episode 322 finished after 108 steps  and with total reward 7.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7899\n",
      "\u001b[99m Episode 323 finished after 73 steps  and with total reward -28.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7889\n",
      "\u001b[99m Episode 324 finished after 77 steps  and with total reward -24.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0024\n",
      "\u001b[99m Episode 325 finished after 56 steps  and with total reward -45.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7866\n",
      "\u001b[99m Episode 326 finished after 73 steps  and with total reward -28.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0035\n",
      "\u001b[99m Episode 327 finished after 76 steps  and with total reward -25.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7905\n",
      "\u001b[99m Episode 328 finished after 87 steps  and with total reward -14.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.7850\n",
      "\u001b[99m Episode 329 finished after 54 steps  and with total reward -47.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 330 finished after 70 steps  and with total reward -31.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0027\n",
      "\u001b[99m Episode 331 finished after 74 steps  and with total reward -27.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7907\n",
      "\u001b[99m Episode 332 finished after 61 steps  and with total reward -40.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0017\n",
      "\u001b[99m Episode 333 finished after 58 steps  and with total reward -43.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7917\n",
      "\u001b[99m Episode 334 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0022\n",
      "\u001b[99m Episode 335 finished after 84 steps  and with total reward -17.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7861\n",
      "\u001b[99m Episode 336 finished after 85 steps  and with total reward -16.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7914\n",
      "\u001b[99m Episode 337 finished after 80 steps  and with total reward -21.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7871\n",
      "\u001b[99m Episode 338 finished after 78 steps  and with total reward -23.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7878\n",
      "\u001b[99m Episode 339 finished after 55 steps  and with total reward -46.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5729\n",
      "\u001b[99m Episode 340 finished after 45 steps  and with total reward -56.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0018\n",
      "\u001b[99m Episode 341 finished after 84 steps  and with total reward -17.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5824\n",
      "\u001b[99m Episode 342 finished after 80 steps  and with total reward -21.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7851\n",
      "\u001b[99m Episode 343 finished after 47 steps  and with total reward -54.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0016\n",
      "\u001b[99m Episode 344 finished after 64 steps  and with total reward -37.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0016\n",
      "\u001b[99m Episode 345 finished after 75 steps  and with total reward -26.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.7906\n",
      "\u001b[99m Episode 346 finished after 67 steps  and with total reward -34.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7909\n",
      "\u001b[99m Episode 347 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7856\n",
      "\u001b[99m Episode 348 finished after 79 steps  and with total reward -22.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7829\n",
      "\u001b[99m Episode 349 finished after 87 steps  and with total reward -14.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7910\n",
      "\u001b[99m Episode 350 finished after 94 steps  and with total reward -7.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7886\n",
      "\u001b[99m Episode 351 finished after 64 steps  and with total reward -37.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7874\n",
      "\u001b[99m Episode 352 finished after 57 steps  and with total reward -44.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5761\n",
      "\u001b[99m Episode 353 finished after 80 steps  and with total reward -21.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0014\n",
      "\u001b[99m Episode 354 finished after 99 steps  and with total reward -2.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5661\n",
      "\u001b[99m Episode 355 finished after 73 steps  and with total reward -28.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0030\n",
      "\u001b[99m Episode 356 finished after 60 steps  and with total reward -41.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0016\n",
      "\u001b[99m Episode 357 finished after 81 steps  and with total reward -20.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0021\n",
      "\u001b[99m Episode 358 finished after 71 steps  and with total reward -30.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0026\n",
      "\u001b[99m Episode 359 finished after 66 steps  and with total reward -35.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7838\n",
      "\u001b[99m Episode 360 finished after 91 steps  and with total reward -10.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5746\n",
      "\u001b[99m Episode 361 finished after 71 steps  and with total reward -30.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7816\n",
      "\u001b[99m Episode 362 finished after 64 steps  and with total reward -37.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5752\n",
      "\u001b[99m Episode 363 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5723\n",
      "\u001b[99m Episode 364 finished after 59 steps  and with total reward -42.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0017\n",
      "\u001b[99m Episode 365 finished after 69 steps  and with total reward -32.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7840\n",
      "\u001b[99m Episode 366 finished after 90 steps  and with total reward -11.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0017\n",
      "\u001b[99m Episode 367 finished after 109 steps  and with total reward 8.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5725\n",
      "\u001b[99m Episode 368 finished after 47 steps  and with total reward -54.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0017\n",
      "\u001b[99m Episode 369 finished after 115 steps  and with total reward 14.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5690\n",
      "\u001b[99m Episode 370 finished after 84 steps  and with total reward -17.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7890\n",
      "\u001b[99m Episode 371 finished after 66 steps  and with total reward -35.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0017\n",
      "\u001b[99m Episode 372 finished after 72 steps  and with total reward -29.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0019\n",
      "\u001b[99m Episode 373 finished after 74 steps  and with total reward -27.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5679\n",
      "\u001b[99m Episode 374 finished after 88 steps  and with total reward -13.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5737\n",
      "\u001b[99m Episode 375 finished after 43 steps  and with total reward -58.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 0.7844\n",
      "\u001b[99m Episode 376 finished after 68 steps  and with total reward -33.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7888\n",
      "\u001b[99m Episode 377 finished after 97 steps  and with total reward -4.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7896\n",
      "\u001b[99m Episode 378 finished after 83 steps  and with total reward -18.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 2.3603\n",
      "\u001b[99m Episode 379 finished after 71 steps  and with total reward -30.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7861\n",
      "\u001b[99m Episode 380 finished after 60 steps  and with total reward -41.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0027\n",
      "\u001b[99m Episode 381 finished after 73 steps  and with total reward -28.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0019\n",
      "\u001b[99m Episode 382 finished after 72 steps  and with total reward -29.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0017\n",
      "\u001b[99m Episode 383 finished after 75 steps  and with total reward -26.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 0.7888\n",
      "\u001b[99m Episode 384 finished after 80 steps  and with total reward -21.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 385 finished after 80 steps  and with total reward -21.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7881\n",
      "\u001b[99m Episode 386 finished after 74 steps  and with total reward -27.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7903\n",
      "\u001b[99m Episode 387 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5760\n",
      "\u001b[99m Episode 388 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5747\n",
      "\u001b[99m Episode 389 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7877\n",
      "\u001b[99m Episode 390 finished after 60 steps  and with total reward -41.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7888\n",
      "\u001b[99m Episode 391 finished after 85 steps  and with total reward -16.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7853\n",
      "\u001b[99m Episode 392 finished after 92 steps  and with total reward -9.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0023\n",
      "\u001b[99m Episode 393 finished after 84 steps  and with total reward -17.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5660\n",
      "\u001b[99m Episode 394 finished after 75 steps  and with total reward -26.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0015\n",
      "\u001b[99m Episode 395 finished after 59 steps  and with total reward -42.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7830\n",
      "\u001b[99m Episode 396 finished after 62 steps  and with total reward -39.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0018\n",
      "\u001b[99m Episode 397 finished after 80 steps  and with total reward -21.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 0.7879\n",
      "\u001b[99m Episode 398 finished after 84 steps  and with total reward -17.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0017\n",
      "\u001b[99m Episode 399 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7847\n",
      "\u001b[99m Episode 400 finished after 57 steps  and with total reward -44.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7918\n",
      "\u001b[99m Episode 401 finished after 77 steps  and with total reward -24.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5753\n",
      "\u001b[99m Episode 402 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 32us/step - loss: 1.5681\n",
      "\u001b[99m Episode 403 finished after 64 steps  and with total reward -37.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7876\n",
      "\u001b[99m Episode 404 finished after 66 steps  and with total reward -35.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7882\n",
      "\u001b[99m Episode 405 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0019\n",
      "\u001b[99m Episode 406 finished after 71 steps  and with total reward -30.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0025\n",
      "\u001b[99m Episode 407 finished after 61 steps  and with total reward -40.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5685\n",
      "\u001b[99m Episode 408 finished after 67 steps  and with total reward -34.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0021\n",
      "\u001b[99m Episode 409 finished after 77 steps  and with total reward -24.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7886\n",
      "\u001b[99m Episode 410 finished after 58 steps  and with total reward -43.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 46us/step - loss: 1.5731\n",
      "\u001b[99m Episode 411 finished after 83 steps  and with total reward -18.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0019\n",
      "\u001b[99m Episode 412 finished after 74 steps  and with total reward -27.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7908\n",
      "\u001b[99m Episode 413 finished after 57 steps  and with total reward -44.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5662\n",
      "\u001b[99m Episode 414 finished after 60 steps  and with total reward -41.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0012\n",
      "\u001b[99m Episode 415 finished after 63 steps  and with total reward -38.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7840\n",
      "\u001b[99m Episode 416 finished after 63 steps  and with total reward -38.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0031\n",
      "\u001b[99m Episode 417 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 0.7900\n",
      "\u001b[99m Episode 418 finished after 55 steps  and with total reward -46.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7866\n",
      "\u001b[99m Episode 419 finished after 92 steps  and with total reward -9.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 2.3604\n",
      "\u001b[99m Episode 420 finished after 64 steps  and with total reward -37.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7827\n",
      "\u001b[99m Episode 421 finished after 63 steps  and with total reward -38.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 2.3567\n",
      "\u001b[99m Episode 422 finished after 54 steps  and with total reward -47.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5672\n",
      "\u001b[99m Episode 423 finished after 70 steps  and with total reward -31.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5720\n",
      "\u001b[99m Episode 424 finished after 68 steps  and with total reward -33.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.0017\n",
      "\u001b[99m Episode 425 finished after 69 steps  and with total reward -32.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0016\n",
      "\u001b[99m Episode 426 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5735\n",
      "\u001b[99m Episode 427 finished after 71 steps  and with total reward -30.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.7908\n",
      "\u001b[99m Episode 428 finished after 68 steps  and with total reward -33.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7850\n",
      "\u001b[99m Episode 429 finished after 97 steps  and with total reward -4.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5747\n",
      "\u001b[99m Episode 430 finished after 59 steps  and with total reward -42.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5700\n",
      "\u001b[99m Episode 431 finished after 67 steps  and with total reward -34.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5722\n",
      "\u001b[99m Episode 432 finished after 87 steps  and with total reward -14.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5706\n",
      "\u001b[99m Episode 433 finished after 103 steps  and with total reward 2.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5686\n",
      "\u001b[99m Episode 434 finished after 112 steps  and with total reward 11.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7865\n",
      "\u001b[99m Episode 435 finished after 61 steps  and with total reward -40.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5791\n",
      "\u001b[99m Episode 436 finished after 87 steps  and with total reward -14.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 32us/step - loss: 0.0016\n",
      "\u001b[99m Episode 437 finished after 73 steps  and with total reward -28.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7873\n",
      "\u001b[99m Episode 438 finished after 86 steps  and with total reward -15.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.7833\n",
      "\u001b[99m Episode 439 finished after 80 steps  and with total reward -21.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 440 finished after 80 steps  and with total reward -21.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7873\n",
      "\u001b[99m Episode 441 finished after 75 steps  and with total reward -26.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 2.3572\n",
      "\u001b[99m Episode 442 finished after 61 steps  and with total reward -40.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7927\n",
      "\u001b[99m Episode 443 finished after 71 steps  and with total reward -30.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5710\n",
      "\u001b[99m Episode 444 finished after 69 steps  and with total reward -32.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7840\n",
      "\u001b[99m Episode 445 finished after 79 steps  and with total reward -22.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7872\n",
      "\u001b[99m Episode 446 finished after 70 steps  and with total reward -31.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0021\n",
      "\u001b[99m Episode 447 finished after 56 steps  and with total reward -45.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7843\n",
      "\u001b[99m Episode 448 finished after 64 steps  and with total reward -37.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7826\n",
      "\u001b[99m Episode 449 finished after 48 steps  and with total reward -53.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7864\n",
      "\u001b[99m Episode 450 finished after 95 steps  and with total reward -6.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7812\n",
      "\u001b[99m Episode 451 finished after 81 steps  and with total reward -20.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7855\n",
      "\u001b[99m Episode 452 finished after 75 steps  and with total reward -26.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0021\n",
      "\u001b[99m Episode 453 finished after 68 steps  and with total reward -33.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0014\n",
      "\u001b[99m Episode 454 finished after 79 steps  and with total reward -22.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 2.3560\n",
      "\u001b[99m Episode 455 finished after 77 steps  and with total reward -24.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5706\n",
      "\u001b[99m Episode 456 finished after 73 steps  and with total reward -28.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7887\n",
      "\u001b[99m Episode 457 finished after 121 steps  and with total reward 20.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7861\n",
      "\u001b[99m Episode 458 finished after 77 steps  and with total reward -24.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7877\n",
      "\u001b[99m Episode 459 finished after 60 steps  and with total reward -41.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0022\n",
      "\u001b[99m Episode 460 finished after 76 steps  and with total reward -25.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 1.5711\n",
      "\u001b[99m Episode 461 finished after 45 steps  and with total reward -56.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 1.5695\n",
      "\u001b[99m Episode 462 finished after 61 steps  and with total reward -40.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7884\n",
      "\u001b[99m Episode 463 finished after 93 steps  and with total reward -8.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7870\n",
      "\u001b[99m Episode 464 finished after 86 steps  and with total reward -15.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5686\n",
      "\u001b[99m Episode 465 finished after 57 steps  and with total reward -44.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0020\n",
      "\u001b[99m Episode 466 finished after 87 steps  and with total reward -14.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7860\n",
      "\u001b[99m Episode 467 finished after 73 steps  and with total reward -28.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0020\n",
      "\u001b[99m Episode 468 finished after 95 steps  and with total reward -6.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7871\n",
      "\u001b[99m Episode 469 finished after 71 steps  and with total reward -30.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5766\n",
      "\u001b[99m Episode 470 finished after 85 steps  and with total reward -16.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0020\n",
      "\u001b[99m Episode 471 finished after 94 steps  and with total reward -7.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 2.3560\n",
      "\u001b[99m Episode 472 finished after 76 steps  and with total reward -25.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0017\n",
      "\u001b[99m Episode 473 finished after 86 steps  and with total reward -15.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 15us/step - loss: 0.7894\n",
      "\u001b[99m Episode 474 finished after 74 steps  and with total reward -27.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7861\n",
      "\u001b[99m Episode 475 finished after 43 steps  and with total reward -58.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7834\n",
      "\u001b[99m Episode 476 finished after 57 steps  and with total reward -44.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0014\n",
      "\u001b[99m Episode 477 finished after 65 steps  and with total reward -36.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7806\n",
      "\u001b[99m Episode 478 finished after 70 steps  and with total reward -31.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7832\n",
      "\u001b[99m Episode 479 finished after 70 steps  and with total reward -31.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0016\n",
      "\u001b[99m Episode 480 finished after 51 steps  and with total reward -50.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.0021\n",
      "\u001b[99m Episode 481 finished after 66 steps  and with total reward -35.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0030\n",
      "\u001b[99m Episode 482 finished after 58 steps  and with total reward -43.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0023\n",
      "\u001b[99m Episode 483 finished after 66 steps  and with total reward -35.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7907\n",
      "\u001b[99m Episode 484 finished after 58 steps  and with total reward -43.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 0.0036\n",
      "\u001b[99m Episode 485 finished after 45 steps  and with total reward -56.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0014\n",
      "\u001b[99m Episode 486 finished after 60 steps  and with total reward -41.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0018\n",
      "\u001b[99m Episode 487 finished after 79 steps  and with total reward -22.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 47us/step - loss: 2.3558\n",
      "\u001b[99m Episode 488 finished after 73 steps  and with total reward -28.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0024\n",
      "\u001b[99m Episode 489 finished after 50 steps  and with total reward -51.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0024\n",
      "\u001b[99m Episode 490 finished after 80 steps  and with total reward -21.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.0025\n",
      "\u001b[99m Episode 491 finished after 50 steps  and with total reward -51.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.0014\n",
      "\u001b[99m Episode 492 finished after 45 steps  and with total reward -56.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7880\n",
      "\u001b[99m Episode 493 finished after 62 steps  and with total reward -39.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 0.7821\n",
      "\u001b[99m Episode 494 finished after 64 steps  and with total reward -37.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 495 finished after 89 steps  and with total reward -12.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5664\n",
      "\u001b[99m Episode 496 finished after 68 steps  and with total reward -33.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5772\n",
      "\u001b[99m Episode 497 finished after 93 steps  and with total reward -8.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 31us/step - loss: 1.5704\n",
      "\u001b[99m Episode 498 finished after 69 steps  and with total reward -32.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 0.7827\n",
      "\u001b[99m Episode 499 finished after 83 steps  and with total reward -18.0\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 16us/step - loss: 1.5624\n"
     ]
    }
   ],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        #self.tensorboard = TensorBoard(log_dir='logs/{}'.format(time()))\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.network = Network(self.state_size, self.action_size)\n",
    "        self.target_net = Network(self.state_size, self.action_size)\n",
    "        self.memory = ReplayMemory()\n",
    "        self.steps = 0\n",
    "        self.epsilon = EPS_START\n",
    "        self.duration = [0] * 100\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_net.get_model().set_weights(self.network.get_model().get_weights())\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        if self.epsilon > EPS_END:\n",
    "            self.epsilon *= EPS_DECAY        \n",
    "        self.steps += 1\n",
    "        #Epsilon greedy exploration/exploitation.\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.network.get_model().predict([state, np.ones((1,2))])\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def exploit_action(self, state):\n",
    "        act_values = self.network.get_model().predict([state, np.ones((1,2))])\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        #Check if we have generated enough data to train.\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        #sample the minibatch to train network on.\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        state_batch = np.asarray(batch.state).reshape(BATCH_SIZE,self.state_size)\n",
    "        action_batch = np.asarray(batch.action).reshape(BATCH_SIZE,-1)\n",
    "        reward_batch = np.asarray(batch.reward).reshape(BATCH_SIZE,-1)\n",
    "        next_state_batch = np.asarray(batch.next_state).reshape(BATCH_SIZE,self.state_size)\n",
    "        done_batch = np.asarray(batch.done).reshape(BATCH_SIZE,-1)\n",
    "        \n",
    "        #one hot encoding of action space.\n",
    "        one_hot_targets = (np.eye(self.action_size)[action_batch]).reshape(BATCH_SIZE,-1)\n",
    "        \n",
    "        # Compute max V(s_{t+1}) for all next states.\n",
    "        next_state_values = self.target_net.get_model().predict([next_state_batch, np.ones(one_hot_targets.shape)])\n",
    "        \n",
    "        # Compute the expected Q values\n",
    "        end_multiplier = -(done_batch - 1) \n",
    "        expected_state_action_values = ((np.max(next_state_values, axis=1, keepdims=True) * GAMMA)*end_multiplier) + reward_batch\n",
    "        # Fit the keras model.\n",
    "        model.fit([state_batch, one_hot_targets], one_hot_targets * expected_state_action_values,\n",
    "                   epochs=1, batch_size=BATCH_SIZE, verbose=1)\n",
    "        #callbacks=[self.tensorboard]  \n",
    "        \n",
    "    def run(self, episode_no):\n",
    "        #reset the environment to start a fresh trial.\n",
    "        state = self.env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        self.steps = 0\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, self.state_size])\n",
    "            if done and self.steps < 195:\n",
    "                reward = -100\n",
    "            total_reward += reward\n",
    "            #add transition to replay memory.\n",
    "            self.memory.add(state, action, next_state, reward, done)\n",
    "            \n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                print(\"{2} Episode {0} finished after {1} steps  and with total reward {3}\"\n",
    "                      .format(episode_no, self.steps, '\\033[92m' if self.steps >= 195 else '\\033[99m',\n",
    "                              total_reward))\n",
    "                break\n",
    "        self.duration[(episode_no%100)] = self.steps\n",
    "        #train for mini batch.\n",
    "        self.train()\n",
    "\n",
    "        if episode_no % TARGET_UPDATE == 0:\n",
    "            self.update_target_model()\n",
    "\n",
    "    def test(self, number_of_episodes):\n",
    "        for e in range(number_of_episodes):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            t = 0\n",
    "            total_reward = 0\n",
    "            while True:\n",
    "                t += 1\n",
    "                self.env.render()\n",
    "                action = self.exploit_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                state = np.reshape(next_state, [1, self.state_size])\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            print(\"number of steps : \" + str(t))\n",
    "            print(\"total_reward : \" + str(total_reward))\n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "agent = DQN()\n",
    "\n",
    "agent.update_target_model()\n",
    "\n",
    "for i in range(500):\n",
    "    agent.run(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.test(10)\n",
    "print('Complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.network.get_model().save_weights(\"cart.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
