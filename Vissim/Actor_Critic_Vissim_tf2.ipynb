{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VISSIM Modules\n",
    "import win32com.client as com\n",
    "import os\n",
    "\n",
    "## RL Modules\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "    \n",
    "## Data Management Modules\n",
    "import pickle\n",
    "\n",
    "## User Defined Modules\n",
    "import math\n",
    "import Simulator_Functions as SF\n",
    "\n",
    "from Actor_Critic_Agents import ACAgent\n",
    "\n",
    "\n",
    "\n",
    "from NParser import NetworkParser\n",
    "from COMServer import COMServerDispatch, COMServerReload\n",
    "from TupleToList import toList\n",
    "from Utilities import log_progress, pltlive\n",
    "## Other Modules\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import PER\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RL Hyperparamenters\n",
    "# Number of simulations, save every \"n\" episodes \n",
    "episodes = 400\n",
    "partial_save_at = 25 #50\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "Surtrac = False\n",
    "AC = True\n",
    "PER_activated = False\n",
    "\n",
    "\n",
    "alpha   = 0.0000005\n",
    "gamma   = 0.85  #0.85 # 0.99\n",
    "entropy = 0.000001 # exploration\n",
    "value = 5 #0.5 # weight attributed in the value loss during gradient descent\n",
    "\n",
    "n_sample = 10 # number of sample for the value check\n",
    "horizon = 100 # horizon of the value check, the number of step forward uses to compute the return\n",
    "\n",
    "# In order to reduce entropy during training (not implemented yet)\n",
    "reduce_entropy = False\n",
    "reduce_entropy_every = 1000\n",
    "\n",
    "n_step_size = 8 #16 # number of step in the n step learning 32\n",
    "# Do not work if n_step_size is aver 31\n",
    "\n",
    "# Timesteps per simulation (1 timestep = 0.1 sec), length for random population is a multiple of episode\n",
    "timesteps_per_second = 1\n",
    "seconds_per_green = 6\n",
    "seconds_per_yellow = 3\n",
    "simulation_length = 3600*1 + 1 # worked with 2400\n",
    "\n",
    "\n",
    "\n",
    "## State-Action Parameters\n",
    "action_type = \"phases\"        # options are \"phases\" and \"programs\"\n",
    "state_size = (2,4,22)  #5 # 4 queues or 5 queues + signal state    #49 53 (1,8,6) for conv\n",
    "action_size = 2\n",
    "\n",
    "# Demand Schedule (times in seconds, demand in cars/hour as PPP) # worked with 600\n",
    "demand_change_timesteps = 450\n",
    "demand = {\"h\":600, 'm':300, 'l':150}\n",
    "demand_list = [[demand['l'], demand['l']], [demand['m'], demand['l']],\\\n",
    "              [demand['h'], demand['l']], [demand['h'], demand['m']],\\\n",
    "              [demand['h'], demand['h']], [demand['m'], demand['h']],\n",
    "              [demand['l'], demand['h']], [demand['l'], demand['m']]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\n"
     ]
    }
   ],
   "source": [
    "## Operation mode (selects functionalities)\n",
    "mode = \"training\"\n",
    "# \"training\" = training agents, maximum speed, frozen UI, mid amount of messages\n",
    "# \"retraining\" = continue the training of previous agent\n",
    "# \"debug\"    = trains for 1 episode, minimum speed, working UI, all messages\n",
    "# \"demo\"     = loads pretrained agent, minimum speed, working UI\n",
    "# \"test\"     = executes evaluation, maximum speed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Network Model Parameters\n",
    "model_name  = 'Single_Cross_Mod2'\n",
    "# 'Single_Cross_Straight'\n",
    "# 'Single_Cross_Triple'\n",
    "# 'Single_Cross_Triple_Mod'\n",
    "# 'Single_Cross_Mod2'\n",
    "# 'Balance'\n",
    "\n",
    "vissim_working_directory =  'C:\\\\Users\\\\Rzhang\\\\Desktop\\\\MLforFlowOptimisationOrigine\\\\Vissim\\\\'\n",
    "agent_type = 'AC' # DQN, DuelingDQN, DDQN, DuelingDDQN AC\n",
    "reward_type = 'Queues'\n",
    "\n",
    "state_type  = 'CellsT'\n",
    "#CellsSpeedOccSig'    # 'Queues', 'Delays', 'QueuesDifference' 'QueuesSpeedavrOccuperateSig' 'QueuesSig' CellsSpeedSig \n",
    "#CellsSpeedOccSig 'CellsOccSig' 'CellsT'\n",
    "# 'Queues', 'Delays', 'QueuesDifference' 'QueuesSpeedavrOccuperateSig' 'QueuesSig'\n",
    "Random_Seed = 42\n",
    "\n",
    "## Use of additional files?\n",
    "flag_read_additionally  = True\n",
    "SaveResultsAgent = True\n",
    "# Random demand\n",
    "Random_Demand = False\n",
    "\n",
    "# Loading the best agent during demo and training\n",
    "best = True\n",
    "\n",
    "\n",
    "\n",
    "# Session ID\n",
    "#Session_ID = 'Ep_'+str(episodes)+'_A_'+agent_type+\"_Act_\"+action_type+\"_Rew_\"+reward_type\n",
    "\n",
    "# Adding the state type to the Session_ID\n",
    "Session_ID = 'Ep_'+str(episodes)+'_A_'+agent_type+\"_State_\"+state_type+\"_Act_\"+action_type+\"_Rew_\"+reward_type\n",
    "print(Session_ID)\n",
    "\n",
    "if mode == 'demo' :\n",
    "    simulation_length = 3601\n",
    "    demand_list = [[demand['l'], demand['l']]]\n",
    "    demand_change_timesteps = simulation_length\n",
    "\n",
    "if mode == 'test' : \n",
    "    simulation_length = 3601\n",
    "    demand_change_timesteps = 450\n",
    "    demand = {\"h\":800, 'm':400, 'l':200}\n",
    "    demand_list = [[demand['l'], demand['l']], [demand['m'], demand['l']],\\\n",
    "                  [demand['h'], demand['l']], [demand['h'], demand['m']],\\\n",
    "                  [demand['h'], demand['h']], [demand['m'], demand['h']],\n",
    "                  [demand['l'], demand['h']], [demand['l'], demand['m']]]\n",
    "    Random_Seed = 1\n",
    "    # Loading the best agent\n",
    "    best = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\\n",
      "Generating Cache...\n",
      "Cache generated.\n",
      "\n",
      "****************************\n",
      "*   COM Server dispatched  *\n",
      "****************************\n",
      "\n",
      "Attempting to load Model File: Single_Cross_Mod2.inpx ...\n",
      "Load process successful\n",
      "Simulation length set to 3601 seconds.\n",
      "Results from Previous Simulations: Deleted. Fresh Start Available.\n",
      "Fetched and containerized Simulation Object\n",
      "Fetched and containerized Network Object \n",
      "\n",
      "*******************************************************\n",
      "*                                                     *\n",
      "*                 SETUP COMPLETE                      *\n",
      "*                                                     *\n",
      "*******************************************************\n",
      "\n",
      "Random seed set in simulator. Random Seed = 42\n",
      "NetworkParser has succesfully crawled the model network.\n",
      "Deploying instance of Actor_Critic Agent(s) !!! TENSORFLOW 2 IS NEEDED !!! \n",
      "Model: \"modelconv\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "value_conv1 (Conv2D)         multiple                  8480      \n",
      "_________________________________________________________________\n",
      "value_conv2 (Conv2D)         multiple                  12320     \n",
      "_________________________________________________________________\n",
      "value_conv3 (Conv2D)         multiple                  6160      \n",
      "_________________________________________________________________\n",
      "value1 (Dense)               multiple                  1056      \n",
      "_________________________________________________________________\n",
      "value2 (Dense)               multiple                  528       \n",
      "_________________________________________________________________\n",
      "value_last (Dense)           multiple                  17        \n",
      "_________________________________________________________________\n",
      "value_flat (Flatten)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "policy_conv1 (Conv2D)        multiple                  8480      \n",
      "_________________________________________________________________\n",
      "policy_conv2 (Conv2D)        multiple                  6160      \n",
      "_________________________________________________________________\n",
      "policy_conv3 (Conv2D)        multiple                  3088      \n",
      "_________________________________________________________________\n",
      "policy_logits2 (Dense)       multiple                  1056      \n",
      "_________________________________________________________________\n",
      "policy_logits3 (Dense)       multiple                  528       \n",
      "_________________________________________________________________\n",
      "policy_last (Dense)          multiple                  34        \n",
      "_________________________________________________________________\n",
      "policy_flat (Flatten)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "probability_distribution (Pr multiple                  0         \n",
      "=================================================================\n",
      "Total params: 47,907\n",
      "Trainable params: 47,907\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "To be corected\n",
      "Deployed 1 agent(s) of the Class AC.\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a334d9144eb44a32a5f6632fc130b18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=400)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/400, Epsilon:0, Average reward: -65.38\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Mod2\\Agents_Results\\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0] \n",
      " [-206.0, -1225.0, -407.0, -175.0, -616.0, -323.0, -176.0, -345.0, -1100.0, -37.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.51, 0.31], [-0.01, 0.06], [0.09, -0.39], [0.23, -0.14], [-0.14, 0.36], [0.0, -0.0], [0.01, 0.23], [0.05, -0.1], [0.59, -0.53], [0.01, -0.01]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 2/400, Epsilon:0, Average reward: -56.24\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Mod2\\Agents_Results\\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, 1.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0] \n",
      " [-61.0, -87.0, -458.0, -175.0, -397.0, -1046.0, -269.0, -41.0, -152.0, -191.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.0, -0.0], [-0.06, 0.05], [0.03, 0.03], [0.13, -0.08], [-0.21, -0.07], [0.56, -0.4], [0.47, -0.43], [-0.12, 0.12], [0.01, -0.17], [-0.11, 0.01]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 3/400, Epsilon:0, Average reward: -60.9\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0] \n",
      " [-486.0, -1260.0, -182.0, -1260.0, -42.0, -1335.0, -304.0, -507.0, -73.0, -128.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.25, -0.12], [-0.11, 0.21], [0.04, 0.09], [-0.11, 0.21], [-0.03, -0.11], [0.58, -0.71], [0.14, -0.33], [0.52, -0.48], [0.27, 0.11], [-0.58, 0.37]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 4/400, Epsilon:0, Average reward: -38.91\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Mod2\\Agents_Results\\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [0.0, -0.0, 0.0, 0.0, -0.0, -0.0, -0.0, 0.0, -0.0, -0.0] \n",
      " [-267.0, -128.0, -531.0, -190.0, -110.0, -94.0, -927.0, -120.0, -45.0, -665.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.26, -0.17], [0.13, -0.21], [0.04, -0.13], [0.18, -0.0], [-0.0, 0.04], [0.51, -0.0], [-0.39, 0.46], [0.04, -0.05], [0.04, 0.01], [-0.2, 0.33]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 5/400, Epsilon:0, Average reward: -51.86\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [0.0, -0.0, 1.0, 0.0, -0.0, 0.0, 0.0, -0.0, -0.0, -0.0] \n",
      " [-32.0, -123.0, -1495.0, -50.0, -201.0, -44.0, -1465.0, -217.0, -138.0, -1251.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.18, -0.11], [0.39, -0.16], [0.2, -0.1], [-0.09, 0.2], [0.06, 0.1], [0.38, -0.09], [0.07, -0.01], [-0.01, -0.17], [-0.05, 0.11], [0.06, -0.26]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 6/400, Epsilon:0, Average reward: -45.31\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, -0.0, -0.0] \n",
      " [-72.0, -143.0, -699.0, -42.0, -471.0, -339.0, -363.0, -220.0, -792.0, -354.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.07, -0.07], [0.21, -0.34], [-0.15, -0.08], [0.2, -0.24], [0.27, -0.21], [0.63, -0.66], [0.12, -0.12], [0.09, -0.15], [0.29, -0.8], [0.35, -0.74]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 7/400, Epsilon:0, Average reward: -49.07\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0] \n",
      " [-43.0, -87.0, -186.0, -1006.0, -64.0, -72.0, -74.0, -140.0, -646.0, -423.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.02, -0.12], [0.11, 0.19], [0.42, -0.57], [0.57, -0.54], [0.49, -0.39], [0.12, -0.17], [-0.07, 0.23], [0.01, -0.22], [0.08, -0.11], [0.14, -0.23]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 8/400, Epsilon:0, Average reward: -92.25\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, 0.0, -0.0, -1.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0] \n",
      " [-643.0, -65.0, -38.0, -346.0, -889.0, -357.0, -652.0, -591.0, -2202.0, -81.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.53, -0.55], [0.13, -0.09], [0.0, -0.0], [-0.25, 0.04], [0.18, -0.01], [0.36, -0.2], [0.05, 0.1], [0.11, 0.03], [0.24, -0.44], [0.59, -0.27]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 9/400, Epsilon:0, Average reward: -56.76\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0] \n",
      " [-307.0, -52.0, -47.0, -299.0, -268.0, -1269.0, -26.0, -355.0, -480.0, -106.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.94, -0.72], [0.0, -0.0], [-0.04, 0.08], [-0.13, -0.16], [0.4, -0.65], [0.66, -0.58], [0.0, -0.0], [0.01, -0.04], [-0.05, 0.07], [0.2, 0.15]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 10/400, Epsilon:0, Average reward: -49.72\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0] \n",
      " [-32.0, -347.0, -162.0, -437.0, -453.0, -860.0, -1051.0, -431.0, -26.0, -224.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.01, 0.0], [0.5, -0.51], [-0.47, 0.51], [-0.03, 0.09], [0.3, 0.22], [0.48, -0.37], [0.7, -0.81], [-0.3, -0.05], [0.01, -0.08], [-0.13, 0.26]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 11/400, Epsilon:0, Average reward: -44.2\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -0.0, -0.0, -0.0, -1.0, -0.0, -0.0, -0.0, -0.0, -0.0] \n",
      " [-576.0, -687.0, -138.0, -482.0, -312.0, -195.0, -495.0, -54.0, -54.0, -292.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.66, -0.45], [0.01, -0.01], [1.17, -0.38], [0.1, -0.1], [0.17, -0.24], [0.2, -0.1], [0.29, -0.22], [0.48, -0.22], [0.14, -0.27], [-0.06, -0.21]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 12/400, Epsilon:0, Average reward: -63.38\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -0.0, -0.0, 0.0, 0.0, -0.0, -0.0, -0.0, -0.0, -0.0] \n",
      " [-365.0, -1038.0, -122.0, -637.0, -303.0, -339.0, -1294.0, -1260.0, -365.0, -228.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.02, 0.1], [-0.08, 0.03], [0.07, -0.14], [0.01, -0.1], [0.48, -0.42], [0.67, -0.93], [0.19, -0.07], [0.34, -0.24], [0.02, 0.1], [-0.03, -0.03]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 13/400, Epsilon:0, Average reward: -51.67\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0] \n",
      " [-93.0, -55.0, -345.0, -75.0, -782.0, -133.0, -1010.0, -140.0, -92.0, -303.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.6, -0.72], [0.09, -0.21], [1.06, -1.23], [0.0, 0.0], [0.02, 0.2], [-0.21, -0.01], [-0.07, 0.39], [-0.08, -0.08], [0.07, 0.02], [0.67, -0.61]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 14/400, Epsilon:0, Average reward: -43.6\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -0.0, -0.0, -1.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0] \n",
      " [-32.0, -235.0, -36.0, -533.0, -331.0, -836.0, -77.0, -223.0, -88.0, -754.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.05, -0.04], [-0.08, -0.13], [0.14, -0.26], [0.23, -0.22], [0.02, -0.29], [-0.04, -0.06], [0.1, -0.1], [0.22, -0.19], [0.55, -0.23], [-0.01, 0.05]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 15/400, Epsilon:0, Average reward: -41.42\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -1.0, -0.0, -0.0, -1.0, -0.0, -1.0, -0.0, -1.0, -0.0] \n",
      " [-854.0, -350.0, -408.0, -572.0, -372.0, -96.0, -458.0, -88.0, -651.0, -565.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.03, -0.05], [0.1, -0.17], [-0.1, 0.13], [-0.07, -0.01], [0.06, -0.24], [-0.03, -0.06], [-0.18, 0.24], [0.26, 0.0], [0.5, -0.48], [0.33, -0.28]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 16/400, Epsilon:0, Average reward: -69.12\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -0.0, -0.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.0, -0.0] \n",
      " [-264.0, -93.0, -148.0, -1435.0, -533.0, -893.0, -743.0, -151.0, -366.0, -93.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.31, -0.11], [-0.04, -0.36], [0.01, -0.01], [0.26, -0.39], [0.06, -0.19], [0.26, 0.16], [-0.12, -0.53], [0.69, 0.21], [0.03, -0.02], [-0.04, -0.36]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 17/400, Epsilon:0, Average reward: -60.67\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0] \n",
      " [-682.0, -56.0, -391.0, -56.0, -264.0, -299.0, -1110.0, -162.0, -62.0, -760.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.1, 0.15], [-0.19, -0.11], [0.13, -0.18], [0.24, -0.26], [-0.12, 0.06], [0.84, -0.64], [0.12, 0.08], [0.04, 0.07], [-0.03, -0.19], [-0.27, -0.12]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 18/400, Epsilon:0, Average reward: -67.58\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0] \n",
      " [-827.0, -55.0, -721.0, -55.0, -58.0, -1368.0, -57.0, -67.0, -281.0, -56.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.17, -0.25], [-0.1, 0.03], [0.56, -0.43], [0.09, -0.07], [0.01, -0.0], [0.68, -0.45], [-0.5, 0.11], [0.38, -0.25], [0.06, -0.01], [0.32, -0.32]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 19/400, Epsilon:0, Average reward: -46.7\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -0.0, -0.0, -0.0, -0.0, -1.0, -1.0, -0.0, -0.0, -0.0] \n",
      " [-1279.0, -105.0, -76.0, -46.0, -350.0, -276.0, -173.0, -122.0, -161.0, -48.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.01, -0.12], [0.08, -0.01], [0.02, -0.07], [0.08, -0.09], [-0.02, -0.06], [0.75, -0.38], [0.42, -0.56], [0.17, 0.02], [-0.09, -0.14], [0.08, -0.36]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 20/400, Epsilon:0, Average reward: -59.79\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -1.0, -0.0, -0.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.0] \n",
      " [-31.0, -835.0, -29.0, -293.0, -268.0, -642.0, -224.0, -254.0, -224.0, -81.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.0, -0.0], [0.61, -0.39], [-0.05, 0.07], [-0.04, -0.07], [-0.05, 0.28], [0.14, 0.04], [0.15, -0.11], [-0.04, 0.12], [0.15, -0.11], [0.06, -0.15]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 21/400, Epsilon:0, Average reward: -64.92\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -0.0, -1.0, -0.0, -1.0, -1.0, -1.0, -1.0, -0.0, -0.0] \n",
      " [-51.0, -592.0, -353.0, -30.0, -94.0, -52.0, -44.0, -47.0, -550.0, -780.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.16, 0.02], [0.14, -0.27], [-0.09, 0.17], [0.0, -0.29], [0.0, -0.01], [0.34, -0.43], [0.26, -0.3], [-0.01, -0.05], [0.04, -0.05], [-0.0, -0.27]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 22/400, Epsilon:0, Average reward: -53.76\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1.0, -0.0, -1.0, -1.0, -1.0, -1.0, -0.0, -1.0, -1.0, -1.0] \n",
      " [-488.0, -279.0, -245.0, -83.0, -257.0, -640.0, -37.0, -541.0, -1014.0, -173.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.12, 0.19], [-0.03, 0.21], [-0.14, -0.07], [0.48, -0.62], [-0.19, 0.39], [0.25, -0.19], [-0.05, 0.06], [0.35, -0.46], [-0.25, -0.13], [-0.26, -0.11]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 23/400, Epsilon:0, Average reward: -53.08\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.0, -0.0, -1.0, -0.0] \n",
      " [-57.0, -177.0, -972.0, -22.0, -228.0, -205.0, -34.0, -1155.0, -102.0, -103.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.01, -0.14], [0.25, -0.29], [-0.03, -0.06], [0.11, -0.19], [0.16, -0.41], [0.21, -0.3], [-0.05, -0.01], [-0.0, 0.09], [0.23, -0.11], [0.15, -0.19]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 24/400, Epsilon:0, Average reward: -50.91\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1.0, -1.0, -0.0, -1.0, -0.0, -1.0, -1.0, -0.0, -1.0, -1.0] \n",
      " [-343.0, -1125.0, -18.0, -1168.0, -176.0, -574.0, -302.0, -11.0, -203.0, -461.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.51, -0.35], [-0.12, -0.02], [-0.04, 0.05], [-0.19, 0.08], [-0.0, 0.21], [0.57, -0.58], [0.42, -0.37], [0.0, -0.0], [0.53, -0.34], [0.18, -0.34]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 25/400, Epsilon:0, Average reward: -54.16\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1.0, -0.0, -1.0, -0.0, -1.0, -1.0, -0.0, -1.0, -0.0, -0.0] \n",
      " [-237.0, -30.0, -285.0, -64.0, -603.0, -573.0, -28.0, -1042.0, -519.0, -38.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.04, -0.21], [0.15, 0.03], [0.03, -0.22], [0.0, -0.01], [0.5, -0.63], [0.22, -0.02], [0.1, -0.19], [0.22, 0.13], [0.97, -0.83], [0.05, -0.06]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saved Partial results at the end of episode 25.\n",
      "Episode: 26/400, Epsilon:0, Average reward: -84.85\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1.0, -0.0, -1.0, -1.0, -1.0, -1.0, -0.0, -1.0, -0.0, -1.0] \n",
      " [-1486.0, -147.0, -1191.0, -366.0, -365.0, -130.0, -855.0, -309.0, -67.0, -450.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.02, -0.02], [-0.07, 0.06], [0.33, -0.05], [0.09, -0.04], [-0.22, 0.49], [-0.19, -0.05], [0.22, -0.04], [0.4, -0.28], [0.0, -0.0], [0.5, -0.26]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 27/400, Epsilon:0, Average reward: -77.81\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -0.0, -1.0, -1.0, -1.0, -1.0, -1.0, -0.0, -0.0, -0.0] \n",
      " [-151.0, -54.0, -49.0, -49.0, -134.0, -422.0, -138.0, -46.0, -587.0, -118.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.04, -0.11], [0.0, -0.0], [-0.35, 0.1], [-0.15, 0.0], [0.17, -0.01], [0.42, -0.38], [-0.12, -0.19], [0.06, -0.01], [-0.03, 0.04], [0.06, -0.05]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 28/400, Epsilon:0, Average reward: -58.51\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -1.0, -1.0, -1.0, -2.0, -0.0, -0.0, -1.0, -1.0, -0.0] \n",
      " [-175.0, -257.0, -98.0, -141.0, -404.0, -1342.0, -96.0, -107.0, -271.0, -26.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.01, -0.16], [0.1, 0.19], [0.12, -0.18], [0.47, -0.56], [0.24, -0.14], [0.27, -0.13], [-0.17, 0.04], [0.09, -0.09], [0.09, 0.2], [0.0, -0.0]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 29/400, Epsilon:0, Average reward: -41.23\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -1.0, -2.0, -1.0, -0.0, -1.0, -1.0, -0.0, -2.0, -1.0] \n",
      " [-181.0, -845.0, -308.0, -80.0, -33.0, -80.0, -79.0, -46.0, -371.0, -89.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.28, -0.1], [0.32, -0.39], [0.5, -0.34], [0.25, -0.09], [0.15, -0.14], [0.25, -0.09], [0.29, -0.41], [0.0, -0.0], [0.35, -0.69], [0.14, -0.28]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 30/400, Epsilon:0, Average reward: -80.01\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1.0, -2.0, -1.0, -0.0, -1.0, -1.0, -1.0, -1.0, -2.0, -1.0] \n",
      " [-76.0, -1041.0, -2179.0, -47.0, -342.0, -1301.0, -87.0, -218.0, -1041.0, -57.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.03, -0.04], [-0.25, -0.26], [-0.07, -0.08], [0.0, -0.0], [0.11, 0.0], [1.23, -0.88], [-0.24, -0.24], [0.79, -0.79], [-0.25, -0.26], [0.28, -0.26]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 31/400, Epsilon:0, Average reward: -71.96\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1.0, -0.0, -1.0, -1.0, -1.0, -0.0, -1.0, -0.0, -1.0, -1.0] \n",
      " [-1384.0, -43.0, -509.0, -896.0, -57.0, -15.0, -1004.0, -242.0, -93.0, -175.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.11, 0.17], [0.03, -0.07], [-0.24, 0.22], [0.09, -0.11], [0.1, -0.04], [0.08, -0.01], [0.31, -0.3], [0.05, -0.06], [-0.02, 0.01], [0.35, -0.42]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 32/400, Epsilon:0, Average reward: -45.65\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -1.0, -1.0, -0.0, -1.0, -1.0, -2.0, -1.0, -2.0, -1.0] \n",
      " [-85.0, -71.0, -147.0, -40.0, -6.0, -167.0, -324.0, -686.0, -76.0, -301.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.18, -0.0], [-0.23, -0.05], [0.06, 0.06], [-0.02, 0.24], [0.13, -0.24], [0.15, -0.25], [0.35, -0.42], [0.08, -0.15], [-0.14, -0.09], [-0.29, 0.08]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 33/400, Epsilon:0, Average reward: -116.75\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1.0, -1.0, -1.0, -1.0, -2.0, -2.0, -1.0, -1.0, -1.0, -1.0] \n",
      " [-1817.0, -279.0, -57.0, -2226.0, -1866.0, -363.0, -2091.0, -1883.0, -1142.0, -26.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.05, -0.16], [0.7, -0.53], [0.34, -0.11], [0.1, 0.06], [0.11, -0.19], [0.28, -0.33], [0.03, -0.05], [0.55, -0.41], [0.15, -0.36], [-0.07, -0.15]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 34/400, Epsilon:0, Average reward: -55.12\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -2.0, -3.0, -1.0, -0.0, -1.0, -1.0, -0.0, -2.0, -1.0] \n",
      " [-876.0, -486.0, -516.0, -27.0, -32.0, -479.0, -615.0, -290.0, -112.0, -541.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.07, -0.04], [-0.14, 0.27], [0.09, -0.22], [0.06, -0.22], [0.0, -0.0], [0.25, -0.37], [0.28, -0.23], [-0.03, 0.15], [0.13, -0.04], [0.23, -0.12]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 35/400, Epsilon:0, Average reward: -42.01\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1.0, -2.0, -2.0, -0.0, -1.0, -1.0, -2.0, -2.0, -3.0, -2.0] \n",
      " [-745.0, -112.0, -350.0, -18.0, -321.0, -758.0, -425.0, -349.0, -282.0, -150.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.58, -0.56], [0.2, -0.34], [0.13, -0.12], [0.04, -0.03], [0.08, -0.15], [0.09, -0.06], [0.47, -0.22], [0.27, -0.28], [-0.04, 0.16], [-0.11, -0.14]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 36/400, Epsilon:0, Average reward: -51.81\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -0.0, -0.0, -3.0, -2.0, -2.0, -0.0, -1.0, -2.0, -2.0] \n",
      " [-613.0, -230.0, -67.0, -271.0, -315.0, -283.0, -822.0, -217.0, -422.0, -24.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.32, -0.17], [0.29, -0.18], [0.02, -0.01], [0.89, -0.97], [0.25, -0.42], [0.4, -0.28], [0.28, -0.18], [-0.02, -0.18], [0.52, -0.22], [-0.1, -0.09]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 37/400, Epsilon:0, Average reward: -62.27\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -1.0, -2.0, -1.0, -3.0, -2.0, -1.0, -2.0, -2.0, -2.0] \n",
      " [-66.0, -295.0, -1078.0, -367.0, -2211.0, -300.0, -87.0, -734.0, -623.0, -2052.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.7, -0.43], [0.03, -0.03], [0.23, -0.65], [-0.04, -0.1], [0.19, -0.69], [0.22, -0.24], [0.34, -0.01], [0.31, -0.08], [0.01, -0.11], [0.35, -0.07]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 38/400, Epsilon:0, Average reward: -66.82\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -5.0, -4.0, -1.0, -2.0, -1.0, -2.0, -1.0, -1.0, -0.0] \n",
      " [-822.0, -1221.0, -347.0, -63.0, -1005.0, -319.0, -319.0, -327.0, -108.0, -102.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.28, -0.15], [0.07, -0.09], [0.36, -0.17], [-0.19, -0.08], [0.39, -0.41], [-0.04, 0.04], [-0.1, -0.13], [0.14, 0.1], [-0.07, -0.26], [0.0, -0.0]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 39/400, Epsilon:0, Average reward: -78.9\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -1.0, -2.0, -0.0, -2.0, -1.0, -1.0, -2.0, -0.0, -1.0] \n",
      " [-1753.0, -144.0, -212.0, -82.0, -177.0, -167.0, -787.0, -59.0, -82.0, -345.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[1.41, -0.84], [-0.01, 0.1], [0.26, -0.22], [0.0, -0.0], [0.29, -0.43], [0.38, -0.36], [-0.02, 0.14], [0.2, -0.22], [0.0, -0.0], [0.08, -0.15]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 40/400, Epsilon:0, Average reward: -53.01\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1.0, -2.0, -1.0, -2.0, -2.0, -4.0, -1.0, -1.0, -0.0, -2.0] \n",
      " [-70.0, -255.0, -435.0, -806.0, -114.0, -166.0, -435.0, -173.0, -31.0, -55.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.24, -0.31], [0.24, -0.21], [0.06, -0.11], [-0.09, 0.05], [0.09, 0.06], [0.41, -0.26], [0.06, -0.11], [-0.11, -0.13], [0.0, -0.0], [-0.09, -0.11]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 41/400, Epsilon:0, Average reward: -58.44\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3.0, -3.0, -1.0, -1.0, -1.0, -4.0, -0.0, -1.0, -2.0, -3.0] \n",
      " [-341.0, -250.0, -242.0, -125.0, -45.0, -1831.0, -77.0, -197.0, -226.0, -118.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.44, -0.19], [0.27, -0.52], [0.2, 0.19], [0.12, -0.0], [0.16, -0.23], [0.1, -0.28], [-0.1, 0.09], [-0.12, 0.09], [0.37, -0.19], [0.95, -0.9]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 42/400, Epsilon:0, Average reward: -65.98\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1.0, -4.0, -1.0, -2.0, -1.0, -1.0, -1.0, -2.0, -3.0, -2.0] \n",
      " [-163.0, -700.0, -223.0, -80.0, -442.0, -257.0, -107.0, -320.0, -288.0, -85.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.08, -0.06], [0.56, -0.6], [-0.03, -0.06], [0.02, 0.08], [-0.06, 0.03], [0.2, -0.37], [0.01, -0.02], [0.1, -0.28], [0.37, -0.12], [-0.15, -0.03]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 43/400, Epsilon:0, Average reward: -51.93\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -3.0, -2.0, -2.0, -0.0, -3.0, -4.0, -2.0, -3.0, -1.0] \n",
      " [-569.0, -679.0, -251.0, -160.0, -48.0, -97.0, -708.0, -332.0, -489.0, -328.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.06, -0.11], [0.15, -0.23], [0.54, -0.38], [0.63, -0.73], [0.0, -0.0], [0.16, -0.51], [0.36, -0.48], [0.3, -0.43], [0.12, -0.02], [0.39, -0.36]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 44/400, Epsilon:0, Average reward: -42.68\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3.0, -2.0, -2.0, -1.0, -4.0, -2.0, -0.0, -1.0, -1.0, -2.0] \n",
      " [-241.0, -143.0, -196.0, -61.0, -375.0, -270.0, -24.0, -443.0, -41.0, -278.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.02, -0.5], [0.1, 0.24], [0.53, -0.48], [-0.58, 0.32], [-0.17, -0.17], [-0.13, -0.32], [0.0, -0.0], [0.03, -0.14], [0.34, -0.35], [0.5, -0.54]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 45/400, Epsilon:0, Average reward: -75.23\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3.0, -1.0, -2.0, -4.0, -1.0, -1.0, -1.0, -2.0, -2.0, -3.0] \n",
      " [-352.0, -33.0, -68.0, -912.0, -447.0, -56.0, -924.0, -214.0, -436.0, -630.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.17, 0.22], [0.45, -0.31], [0.2, -0.65], [-0.13, -0.2], [0.36, -0.17], [0.07, -0.26], [0.14, -0.16], [-0.07, -0.11], [0.6, -0.37], [0.04, -0.42]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 46/400, Epsilon:0, Average reward: -159.03\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -2.0, -3.0, -0.0, -1.0, -2.0, -2.0, -4.0, -2.0, -3.0] \n",
      " [-3315.0, -362.0, -1521.0, -3107.0, -53.0, -887.0, -427.0, -429.0, -438.0, -240.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.03, 0.06], [0.2, -0.03], [0.17, -0.65], [0.13, -0.22], [0.1, -0.21], [-0.37, 0.32], [0.26, -0.23], [0.11, -0.12], [0.5, -0.82], [0.17, -0.13]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 47/400, Epsilon:0, Average reward: -45.21\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3.0, -3.0, -3.0, -0.0, -3.0, -1.0, -2.0, -4.0, -2.0, -4.0] \n",
      " [-278.0, -1076.0, -77.0, -37.0, -197.0, -299.0, -37.0, -717.0, -285.0, -268.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.21, 0.02], [0.15, -0.2], [-0.1, 0.12], [0.0, -0.0], [-0.14, 0.12], [0.17, -0.21], [0.25, 0.1], [0.2, -0.08], [0.07, -0.04], [0.2, -0.11]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 48/400, Epsilon:0, Average reward: -87.23\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3.0, -1.0, -0.0, -2.0, -4.0, -2.0, -4.0, -1.0, -2.0, -1.0] \n",
      " [-268.0, -1342.0, -125.0, -317.0, -1624.0, -958.0, -54.0, -807.0, -368.0, -1250.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.13, -0.24], [0.08, -0.09], [0.03, -0.01], [0.11, -0.1], [0.72, -0.25], [0.3, -0.41], [1.41, -1.21], [0.22, -0.12], [0.2, -0.25], [0.12, -0.12]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 49/400, Epsilon:0, Average reward: -43.26\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -2.0, -5.0, -3.0, -4.0, -1.0, -3.0, -4.0, -0.0, -3.0] \n",
      " [-238.0, -378.0, -333.0, -242.0, -600.0, -55.0, -157.0, -334.0, -88.0, -339.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.05, 0.12], [0.73, -0.28], [0.23, -0.18], [0.41, -0.12], [0.17, -0.28], [0.17, -0.19], [0.22, 0.25], [0.13, 0.48], [0.0, -0.0], [0.26, -0.43]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 50/400, Epsilon:0, Average reward: -58.69\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3.0, -2.0, -5.0, -3.0, -5.0, -2.0, -5.0, -0.0, -2.0, -3.0] \n",
      " [-36.0, -1713.0, -599.0, -327.0, -627.0, -440.0, -280.0, -244.0, -904.0, -298.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.02, -0.09], [0.39, -0.56], [0.07, 0.27], [0.4, -0.22], [0.0, -0.26], [0.22, -0.07], [-0.25, -0.44], [0.04, -0.12], [0.14, -0.06], [0.3, -0.35]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saved Partial results at the end of episode 50.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 51/400, Epsilon:0, Average reward: -79.69\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -3.0, -2.0, -0.0, -6.0, -3.0, -2.0, -1.0, -2.0, -3.0] \n",
      " [-53.0, -62.0, -525.0, -25.0, -403.0, -62.0, -1109.0, -537.0, -45.0, -908.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.0, -0.0], [0.28, -0.35], [0.14, -0.29], [0.05, -0.02], [0.05, 0.02], [0.06, 0.04], [-0.21, -0.05], [0.1, -0.14], [0.32, -0.24], [0.04, -0.17]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 52/400, Epsilon:0, Average reward: -89.01\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-3.0, -6.0, -1.0, -2.0, -2.0, -6.0, -0.0, -2.0, -2.0, -2.0] \n",
      " [-1571.0, -1986.0, -1720.0, -236.0, -27.0, -1986.0, -49.0, -1501.0, -397.0, -328.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.08, -0.06], [0.4, -0.15], [0.04, -0.0], [0.06, -0.02], [0.03, -0.09], [0.4, -0.15], [0.0, -0.0], [0.04, 0.01], [0.25, -0.44], [0.27, -0.21]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 53/400, Epsilon:0, Average reward: -49.41\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1.0, -4.0, -4.0, -5.0, -0.0, -3.0, -4.0, -3.0, -1.0, -2.0] \n",
      " [-109.0, -559.0, -318.0, -701.0, -48.0, -572.0, -557.0, -78.0, -238.0, -414.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.08, 0.1], [0.08, -0.21], [0.41, -0.34], [0.09, -0.38], [0.0, -0.0], [0.18, 0.42], [0.19, -0.25], [-0.02, 0.34], [0.14, -0.12], [-0.11, 0.07]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 54/400, Epsilon:0, Average reward: -53.76\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -3.0, -4.0, -3.0, -2.0, -4.0, -2.0, -6.0, -4.0, -1.0] \n",
      " [-1223.0, -281.0, -292.0, -191.0, -148.0, -92.0, -295.0, -201.0, -118.0, -117.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.35, -0.23], [0.11, 0.02], [0.4, -0.05], [0.43, -0.34], [0.15, -0.12], [0.2, 0.11], [0.45, -0.16], [0.38, 0.13], [0.09, -0.07], [0.07, -0.07]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 55/400, Epsilon:0, Average reward: -50.03\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -6.0, -0.0, -6.0, -5.0, -5.0, -5.0, -3.0, -2.0, -5.0] \n",
      " [-378.0, -923.0, -80.0, -290.0, -621.0, -621.0, -807.0, -361.0, -143.0, -242.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.35, -0.28], [0.58, -0.99], [0.0, 0.0], [0.18, -0.35], [-0.52, 0.2], [-0.52, 0.2], [-0.03, 0.29], [0.23, -0.24], [0.36, -0.17], [0.17, -0.27]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 56/400, Epsilon:0, Average reward: -57.76\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7.0, -1.0, -2.0, -8.0, -0.0, -3.0, -0.0, -1.0, -9.0, -5.0] \n",
      " [-302.0, -1397.0, -107.0, -175.0, -60.0, -473.0, -38.0, -199.0, -322.0, -234.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.17, -0.49], [0.03, -0.08], [0.17, -0.16], [0.38, -0.2], [0.14, -0.07], [0.04, -0.3], [-0.0, -0.0], [0.09, -0.12], [0.55, -0.36], [0.41, -0.34]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 57/400, Epsilon:0, Average reward: -56.29\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -5.0, -2.0, -2.0, -4.0, -5.0, -1.0, -6.0, -4.0, -4.0] \n",
      " [-251.0, -930.0, -106.0, -677.0, -184.0, -1472.0, -76.0, -1098.0, -103.0, -110.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.16, -0.09], [0.14, -0.26], [0.31, -0.33], [0.19, -0.16], [0.14, 0.42], [0.19, -0.14], [-0.04, -0.02], [0.26, -0.21], [0.71, -0.48], [0.2, -0.12]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 58/400, Epsilon:0, Average reward: -44.4\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4.0, -5.0, -4.0, -4.0, -2.0, -3.0, -7.0, -0.0, -3.0, -6.0] \n",
      " [-360.0, -249.0, -97.0, -423.0, -199.0, -83.0, -494.0, -39.0, -183.0, -116.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.12, 0.18], [0.89, -0.88], [0.65, -0.64], [0.18, -0.27], [-0.08, 0.03], [-0.05, 0.0], [0.42, -0.29], [0.0, -0.0], [-0.02, 0.12], [0.62, -0.72]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 59/400, Epsilon:0, Average reward: -48.08\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -5.0, -5.0, -4.0, -1.0, -1.0, -2.0, -6.0, -7.0, -4.0] \n",
      " [-227.0, -704.0, -357.0, -179.0, -76.0, -48.0, -568.0, -299.0, -482.0, -447.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.03, -0.01], [0.49, -0.3], [0.16, -0.17], [0.38, -0.21], [0.03, -0.05], [0.15, 0.17], [0.07, 0.24], [-0.12, -0.2], [0.36, -0.42], [0.06, 0.06]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 60/400, Epsilon:0, Average reward: -67.0\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5.0, -3.0, -0.0, -5.0, -0.0, -4.0, -10.0, -3.0, -2.0, -4.0] \n",
      " [-146.0, -84.0, -12.0, -330.0, -28.0, -1499.0, -660.0, -85.0, -592.0, -230.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.04, -0.18], [0.1, -0.1], [-0.0, -0.0], [-0.11, -0.22], [0.0, -0.0], [0.31, -0.1], [1.14, -1.13], [0.67, -0.25], [0.24, -0.27], [0.47, -0.21]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 61/400, Epsilon:0, Average reward: -62.83\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5.0, -8.0, -1.0, -5.0, -8.0, -6.0, -2.0, -4.0, -5.0, -3.0] \n",
      " [-399.0, -560.0, -55.0, -392.0, -440.0, -142.0, -816.0, -140.0, -864.0, -72.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.22, -0.28], [-0.23, 0.03], [-0.02, 0.1], [0.3, -0.28], [0.8, -0.45], [0.21, -0.28], [0.18, -0.06], [0.21, -0.12], [-0.1, -0.03], [-0.1, 0.11]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 62/400, Epsilon:0, Average reward: -66.69\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4.0, -6.0, -7.0, -3.0, -2.0, -8.0, -7.0, -8.0, -4.0, -5.0] \n",
      " [-227.0, -492.0, -480.0, -142.0, -801.0, -452.0, -239.0, -92.0, -287.0, -161.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.11, -0.18], [-0.31, -0.0], [0.5, -0.34], [0.07, -0.42], [0.01, -0.16], [0.03, -0.04], [0.44, -0.01], [0.68, -0.59], [0.16, -0.06], [0.43, -0.14]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 63/400, Epsilon:0, Average reward: -34.26\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Mod2\\Agents_Results\\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -5.0, -3.0, -5.0, -2.0, -4.0, -8.0, -0.0, -0.0, -4.0] \n",
      " [-273.0, -46.0, -50.0, -134.0, -53.0, -33.0, -214.0, -38.0, -63.0, -362.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.11, -0.2], [-0.07, 0.11], [0.07, -0.01], [0.35, -0.5], [0.17, 0.06], [-0.05, 0.01], [-0.0, -0.02], [0.14, -0.11], [0.0, -0.0], [-0.08, 0.09]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 64/400, Epsilon:0, Average reward: -50.28\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -6.0, -6.0, -5.0, -1.0, -11.0, -7.0, -3.0, -6.0, -0.0] \n",
      " [-1444.0, -13.0, -628.0, -36.0, -22.0, -402.0, -214.0, -226.0, -65.0, -169.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.03, -0.12], [0.05, 0.2], [-0.11, 0.14], [-0.19, 0.25], [0.11, -0.2], [0.15, -0.26], [0.25, -0.32], [0.14, -0.04], [0.18, -0.15], [0.0, 0.0]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 65/400, Epsilon:0, Average reward: -70.12\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12.0, -0.0, -6.0, -10.0, -5.0, -9.0, -2.0, -0.0, -4.0, -0.0] \n",
      " [-531.0, -63.0, -62.0, -964.0, -377.0, -226.0, -779.0, -66.0, -90.0, -56.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.02, -0.05], [0.01, -0.01], [0.19, -0.22], [0.13, -0.29], [-0.37, -0.07], [0.27, -0.21], [0.22, -0.22], [-0.0, -0.0], [-0.09, 0.02], [0.0, -0.0]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 66/400, Epsilon:0, Average reward: -57.4\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8.0, -5.0, -5.0, -7.0, -3.0, -6.0, -2.0, -7.0, -2.0, -8.0] \n",
      " [-195.0, -735.0, -58.0, -456.0, -281.0, -130.0, -52.0, -381.0, -28.0, -590.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.12, 0.08], [0.19, -0.07], [0.08, -0.01], [0.3, -0.17], [0.25, -0.33], [0.28, -0.35], [0.26, -0.24], [-0.41, 0.1], [-0.04, -0.05], [-0.06, 0.06]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 67/400, Epsilon:0, Average reward: -50.01\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5.0, -4.0, -4.0, -5.0, -1.0, -10.0, -2.0, -10.0, -1.0, -6.0] \n",
      " [-7.0, -68.0, -809.0, -419.0, -22.0, -370.0, -221.0, -213.0, -105.0, -572.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.08, -0.1], [0.22, 0.07], [0.13, -0.1], [0.38, 0.02], [0.15, 0.01], [0.05, -0.28], [0.22, -0.05], [0.03, -0.22], [-0.12, 0.11], [0.34, -0.28]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 68/400, Epsilon:0, Average reward: -46.09\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6.0, -9.0, -8.0, -5.0, -12.0, -4.0, -1.0, -7.0, -4.0, -10.0] \n",
      " [-113.0, -385.0, -163.0, -60.0, -484.0, -119.0, -55.0, -56.0, -58.0, -125.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.05, -0.12], [0.11, -0.18], [0.47, -0.43], [0.56, -0.29], [0.34, 0.32], [0.04, -0.12], [0.09, -0.23], [0.0, 0.11], [-0.1, -0.01], [-0.22, 0.06]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 69/400, Epsilon:0, Average reward: -42.81\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9.0, -7.0, -0.0, -1.0, -7.0, -4.0, -7.0, -6.0, -12.0, -1.0] \n",
      " [-37.0, -342.0, -107.0, -45.0, -542.0, -27.0, -172.0, -58.0, -172.0, -175.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.23, -0.39], [0.07, 0.11], [0.0, 0.0], [0.11, -0.07], [0.24, -0.31], [0.57, -0.04], [-0.01, -0.35], [-0.0, -0.1], [-0.02, -0.14], [0.26, -0.21]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 70/400, Epsilon:0, Average reward: -139.61\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6.0, -11.0, -3.0, -3.0, -1.0, -10.0, -12.0, -6.0, -6.0, -6.0] \n",
      " [-2533.0, -511.0, -47.0, -220.0, -60.0, -112.0, -1463.0, -3066.0, -2499.0, -2258.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.13, -0.03], [0.37, -0.5], [0.03, 0.05], [0.27, -0.3], [0.06, -0.1], [-0.2, 0.19], [-0.34, 0.22], [0.38, -0.52], [0.28, -0.14], [0.09, -0.11]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 71/400, Epsilon:0, Average reward: -40.32\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4.0, -9.0, -7.0, -7.0, -4.0, -5.0, -11.0, -7.0, -8.0, -10.0] \n",
      " [-71.0, -298.0, -40.0, -341.0, -485.0, -269.0, -412.0, -576.0, -176.0, -244.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.14, -0.33], [0.24, -0.24], [0.24, 0.08], [0.38, -0.41], [0.14, 0.04], [0.01, 0.01], [0.56, -0.5], [-0.01, -0.24], [0.59, -0.65], [0.16, -0.13]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 72/400, Epsilon:0, Average reward: -47.31\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-1.0, -8.0, -7.0, -1.0, -7.0, -5.0, -9.0, -12.0, -9.0, -10.0] \n",
      " [-133.0, -85.0, -98.0, -108.0, -134.0, -81.0, -750.0, -164.0, -750.0, -420.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.02, 0.05], [0.33, -0.29], [0.15, 0.18], [0.12, -0.11], [0.06, -0.2], [-0.06, 0.01], [-0.1, -0.16], [0.25, -0.3], [-0.1, -0.16], [0.29, -0.43]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 73/400, Epsilon:0, Average reward: -58.63\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11.0, -18.0, -7.0, -8.0, -13.0, -8.0, -12.0, -1.0, -10.0, -7.0] \n",
      " [-900.0, -759.0, -662.0, -1115.0, -100.0, -247.0, -357.0, -61.0, -185.0, -662.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.23, -0.38], [0.48, 0.18], [0.88, -0.78], [0.3, -0.19], [0.05, 0.04], [0.08, -0.09], [0.21, -0.26], [0.03, -0.02], [0.37, -0.29], [0.88, -0.78]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 74/400, Epsilon:0, Average reward: -38.25\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12.0, -5.0, -10.0, -13.0, -17.0, -4.0, -1.0, -2.0, -8.0, -8.0] \n",
      " [-339.0, -347.0, -233.0, -381.0, -371.0, -32.0, -18.0, -74.0, -1017.0, -194.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.08, 0.01], [0.02, -0.2], [0.59, -0.18], [0.29, -0.26], [0.57, -0.28], [0.57, -0.4], [0.26, -0.21], [-0.15, 0.22], [0.0, -0.15], [0.1, -0.31]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 75/400, Epsilon:0, Average reward: -61.76\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5.0, -9.0, -11.0, -9.0, -7.0, -8.0, -9.0, -7.0, -5.0, -9.0] \n",
      " [-105.0, -1376.0, -1266.0, -71.0, -1424.0, -229.0, -225.0, -429.0, -123.0, -1463.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.0, 0.04], [0.5, -0.31], [0.26, -0.23], [0.54, -0.7], [0.29, -0.33], [-0.1, -0.2], [0.13, -0.29], [0.05, -0.16], [-0.04, 0.01], [0.04, -0.08]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saved Partial results at the end of episode 75.\n",
      "Episode: 76/400, Epsilon:0, Average reward: -58.67\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-17.0, -14.0, -6.0, -5.0, -6.0, -10.0, -9.0, -11.0, -6.0, -8.0] \n",
      " [-2357.0, -1103.0, -384.0, -450.0, -34.0, -71.0, -378.0, -540.0, -24.0, -316.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.07, 0.01], [-0.07, 0.27], [0.19, -0.21], [0.15, 0.0], [-0.16, -0.1], [-0.14, 0.02], [-0.15, -0.1], [0.08, -0.21], [-0.0, -0.06], [-0.0, 0.42]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 77/400, Epsilon:0, Average reward: -61.68\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4.0, -6.0, -10.0, -9.0, -11.0, -11.0, -11.0, -0.0, -10.0, -10.0] \n",
      " [-177.0, -28.0, -463.0, -437.0, -157.0, -529.0, -1544.0, -26.0, -392.0, -78.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.12, -0.07], [0.11, -0.1], [-0.02, -0.07], [-0.1, 0.12], [0.05, -0.18], [0.47, -0.32], [0.23, -0.26], [0.0, -0.0], [0.27, -0.29], [-0.37, -0.03]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 78/400, Epsilon:0, Average reward: -67.36\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-8.0, -15.0, -11.0, -11.0, -9.0, -11.0, -4.0, -7.0, -14.0, -8.0] \n",
      " [-1357.0, -179.0, -74.0, -1116.0, -285.0, -709.0, -15.0, -1093.0, -442.0, -1357.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.2, -0.07], [0.18, -0.34], [-0.21, -0.09], [-0.1, 0.07], [0.04, -0.26], [0.08, 0.21], [0.0, -0.05], [0.17, -0.14], [0.29, -0.47], [0.2, -0.07]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 79/400, Epsilon:0, Average reward: -35.9\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-10.0, -13.0, -11.0, -10.0, -14.0, -7.0, -2.0, -12.0, -2.0, -10.0] \n",
      " [-251.0, -203.0, -285.0, -274.0, -684.0, -774.0, -199.0, -190.0, -58.0, -356.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.43, -0.5], [0.29, 0.06], [0.19, -0.2], [0.03, -0.22], [-0.27, 0.23], [0.03, 0.01], [-0.03, -0.01], [0.25, 0.27], [-0.04, -0.02], [-0.02, -0.03]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 80/400, Epsilon:0, Average reward: -43.98\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6.0, -7.0, -8.0, -9.0, -8.0, -6.0, -8.0, -16.0, -12.0, -8.0] \n",
      " [-579.0, -32.0, -182.0, -333.0, -592.0, -351.0, -77.0, -364.0, -335.0, -511.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.17, -0.19], [-0.04, 0.0], [0.05, 0.06], [0.18, -0.29], [-0.05, -0.26], [0.43, -0.25], [-0.22, 0.01], [0.95, -0.77], [-0.07, 0.06], [0.51, -0.2]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 81/400, Epsilon:0, Average reward: -64.6\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-7.0, -13.0, -11.0, -10.0, -10.0, -11.0, -11.0, -6.0, -9.0, -15.0] \n",
      " [-17.0, -178.0, -272.0, -121.0, -127.0, -220.0, -336.0, -229.0, -131.0, -514.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.1, -0.03], [0.42, -0.37], [0.32, -0.33], [0.31, 0.09], [0.22, -0.41], [0.18, -0.2], [0.17, -0.17], [0.24, -0.19], [0.05, 0.22], [0.76, -0.33]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 82/400, Epsilon:0, Average reward: -46.3\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-14.0, -21.0, -10.0, -2.0, -11.0, -12.0, -3.0, -2.0, -21.0, -8.0] \n",
      " [-61.0, -510.0, -763.0, -47.0, -54.0, -295.0, -340.0, -333.0, -603.0, -145.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.02, 0.12], [0.1, 0.47], [0.33, -0.12], [0.0, -0.04], [0.09, -0.2], [0.42, -0.15], [0.13, -0.13], [-0.05, -0.01], [0.03, 0.02], [-0.02, -0.1]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 83/400, Epsilon:0, Average reward: -53.49\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-2.0, -10.0, -9.0, -15.0, -11.0, -10.0, -18.0, -12.0, -8.0, -13.0] \n",
      " [-62.0, -93.0, -789.0, -213.0, -135.0, -312.0, -322.0, -1087.0, -72.0, -89.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.09, -0.0], [-0.07, -0.06], [0.13, 0.14], [0.23, -0.13], [0.27, -0.34], [0.18, -0.1], [0.21, 0.19], [0.03, -0.14], [0.15, -0.15], [0.45, -0.22]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 84/400, Epsilon:0, Average reward: -46.93\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9.0, -20.0, -21.0, -6.0, -7.0, -11.0, -14.0, -4.0, -13.0, -0.0] \n",
      " [-56.0, -425.0, -1330.0, -308.0, -94.0, -36.0, -470.0, -59.0, -262.0, -77.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.02, 0.06], [0.14, -0.56], [0.98, 0.59], [0.03, -0.11], [0.04, -0.14], [-0.17, 0.02], [-0.07, -0.15], [0.01, -0.13], [-0.26, -0.15], [0.04, -0.0]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 85/400, Epsilon:0, Average reward: -76.42\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -9.0, -12.0, -21.0, -1.0, -6.0, -9.0, -7.0, -2.0, -15.0] \n",
      " [-33.0, -1178.0, -656.0, -698.0, -436.0, -644.0, -1565.0, -93.0, -103.0, -43.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.0, -0.0], [0.07, -0.07], [0.23, -0.19], [0.09, 0.09], [0.05, -0.0], [0.15, -0.11], [0.04, -0.19], [0.15, -0.28], [0.1, -0.15], [-0.07, -0.06]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 86/400, Epsilon:0, Average reward: -35.9\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-17.0, -23.0, -13.0, -0.0, -12.0, -14.0, -17.0, -20.0, -0.0, -9.0] \n",
      " [-531.0, -493.0, -316.0, -89.0, -82.0, -549.0, -248.0, -246.0, -89.0, -103.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.68, -0.23], [0.14, -0.18], [-0.04, -0.11], [0.0, -0.0], [0.19, 0.15], [0.76, -0.59], [-0.1, 0.18], [0.46, -0.57], [0.0, -0.0], [0.37, -0.26]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 87/400, Epsilon:0, Average reward: -39.06\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-9.0, -8.0, -17.0, -6.0, -19.0, -15.0, -12.0, -0.0, -15.0, -21.0] \n",
      " [-177.0, -191.0, -155.0, -95.0, -913.0, -61.0, -103.0, -60.0, -123.0, -834.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.19, 0.02], [-0.06, -0.06], [0.1, -0.11], [-0.13, -0.0], [0.27, -0.43], [-0.14, -0.08], [0.07, -0.15], [0.0, -0.0], [0.28, -0.31], [-0.11, -0.25]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 88/400, Epsilon:0, Average reward: -69.09\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11.0, -9.0, -7.0, -23.0, -3.0, -6.0, -0.0, -18.0, -9.0, -17.0] \n",
      " [-507.0, -77.0, -34.0, -486.0, -74.0, -31.0, -53.0, -96.0, -1803.0, -2173.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.22, 0.11], [0.21, -0.16], [0.08, -0.08], [0.24, 0.25], [0.2, -0.29], [0.37, -0.29], [0.0, -0.0], [0.14, -0.31], [0.1, -0.16], [-0.17, 0.0]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 89/400, Epsilon:0, Average reward: -63.66\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4.0, -12.0, -13.0, -12.0, -0.0, -5.0, -3.0, -9.0, -16.0, -9.0] \n",
      " [-64.0, -1415.0, -212.0, -115.0, -38.0, -166.0, -26.0, -161.0, -126.0, -26.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.0, -0.14], [-0.08, -0.01], [0.29, -0.31], [0.02, 0.12], [0.0, -0.0], [-0.13, -0.0], [0.15, -0.12], [0.34, -0.2], [0.98, -0.48], [0.14, -0.06]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 90/400, Epsilon:0, Average reward: -49.52\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12.0, -15.0, -16.0, -3.0, -8.0, -9.0, -19.0, -2.0, -8.0, -6.0] \n",
      " [-1135.0, -161.0, -996.0, -37.0, -1502.0, -177.0, -182.0, -59.0, -153.0, -249.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.09, -0.18], [-0.16, 0.51], [-0.15, 0.16], [0.0, -0.05], [-0.01, -0.03], [0.19, -0.27], [0.06, -0.27], [0.33, -0.17], [0.2, 0.03], [0.25, -0.06]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 91/400, Epsilon:0, Average reward: -50.42\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13.0, -14.0, -6.0, -5.0, -10.0, -11.0, -13.0, -7.0, -17.0, -0.0] \n",
      " [-505.0, -49.0, -72.0, -58.0, -97.0, -232.0, -472.0, -251.0, -325.0, -23.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.23, 0.05], [-0.06, -0.05], [-0.08, -0.03], [0.7, -0.21], [0.17, -0.12], [-0.12, -0.03], [0.29, -0.42], [-0.24, 0.13], [-0.08, -0.11], [0.0, -0.0]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 92/400, Epsilon:0, Average reward: -45.3\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-15.0, -17.0, -22.0, -30.0, -32.0, -5.0, -16.0, -14.0, -22.0, -21.0] \n",
      " [-289.0, -362.0, -1193.0, -1074.0, -698.0, -50.0, -179.0, -53.0, -432.0, -1008.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.21, -0.13], [-0.07, 0.1], [-0.27, 0.06], [-0.26, -0.1], [-0.32, 0.25], [0.14, -0.19], [0.48, -0.35], [0.05, -0.06], [0.06, -0.17], [0.14, -0.33]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 93/400, Epsilon:0, Average reward: -43.43\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13.0, -8.0, -22.0, -23.0, -15.0, -10.0, -18.0, -1.0, -4.0, -21.0] \n",
      " [-761.0, -92.0, -225.0, -220.0, -152.0, -69.0, -98.0, -72.0, -390.0, -170.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.12, -0.09], [0.16, 0.04], [0.38, -0.33], [0.4, -0.46], [0.17, -0.47], [-0.09, -0.01], [0.05, -0.15], [0.04, -0.03], [-0.05, 0.1], [0.1, 0.08]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 94/400, Epsilon:0, Average reward: -55.69\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-12.0, -22.0, -17.0, -21.0, -13.0, -25.0, -18.0, -22.0, -21.0, -12.0] \n",
      " [-924.0, -177.0, -828.0, -591.0, -64.0, -661.0, -174.0, -376.0, -87.0, -72.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.13, -0.06], [0.29, -0.41], [0.03, -0.15], [0.04, -0.06], [0.16, 0.21], [0.34, -0.48], [-0.1, -0.11], [-0.03, 0.1], [0.07, -0.29], [0.23, -0.15]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 95/400, Epsilon:0, Average reward: -63.49\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-25.0, -29.0, -14.0, -23.0, -2.0, -11.0, -39.0, -13.0, -7.0, -20.0] \n",
      " [-1025.0, -247.0, -184.0, -1209.0, -61.0, -521.0, -1426.0, -44.0, -59.0, -78.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.19, 0.02], [0.6, -0.21], [0.42, -0.59], [0.01, -0.56], [0.07, -0.0], [0.14, -0.13], [0.08, 0.06], [0.09, -0.14], [-0.06, -0.1], [0.35, -0.16]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 96/400, Epsilon:0, Average reward: -48.34\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-20.0, -22.0, -25.0, -11.0, -17.0, -19.0, -25.0, -10.0, -18.0, -25.0] \n",
      " [-752.0, -285.0, -219.0, -124.0, -482.0, -109.0, -903.0, -97.0, -864.0, -519.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.02, 0.25], [0.61, -0.44], [0.02, -0.3], [-0.08, 0.08], [-0.04, 0.03], [0.37, -0.39], [0.34, -0.3], [0.16, 0.0], [0.18, 0.05], [0.48, -0.44]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 97/400, Epsilon:0, Average reward: -35.98\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-29.0, -7.0, -11.0, -17.0, -22.0, -9.0, -39.0, -21.0, -39.0, -9.0] \n",
      " [-665.0, -308.0, -197.0, -78.0, -525.0, -52.0, -977.0, -781.0, -964.0, -68.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.75, -0.57], [0.15, -0.14], [0.07, -0.17], [0.27, -0.14], [0.26, -0.22], [0.09, -0.11], [0.03, -0.32], [0.1, 0.06], [0.04, -0.11], [-0.17, -0.08]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 98/400, Epsilon:0, Average reward: -59.1\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-21.0, -3.0, -14.0, -6.0, -17.0, -19.0, -17.0, -11.0, -22.0, -3.0] \n",
      " [-67.0, -43.0, -183.0, -197.0, -1209.0, -948.0, -1096.0, -28.0, -1001.0, -47.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.32, 0.29], [0.33, -0.34], [0.26, 0.14], [0.1, -0.1], [-0.05, -0.07], [0.15, 0.08], [-0.08, -0.08], [0.23, -0.18], [0.08, -0.33], [-0.03, 0.06]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 99/400, Epsilon:0, Average reward: -48.86\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-19.0, -22.0, -30.0, -24.0, -29.0, -19.0, -19.0, -29.0, -33.0, -34.0] \n",
      " [-119.0, -193.0, -1451.0, -381.0, -482.0, -356.0, -72.0, -224.0, -291.0, -62.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.28, -0.35], [0.51, -0.52], [0.05, -0.08], [0.5, -0.36], [0.04, -0.17], [0.1, -0.5], [0.18, 0.01], [-0.04, -0.02], [-0.03, -0.1], [0.2, -0.35]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 100/400, Epsilon:0, Average reward: -41.84\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-20.0, -22.0, -31.0, -31.0, -2.0, -40.0, -2.0, -19.0, -14.0, -29.0] \n",
      " [-269.0, -312.0, -390.0, -265.0, -36.0, -212.0, -16.0, -1031.0, -157.0, -486.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.15, -0.1], [0.5, -0.34], [0.29, 0.14], [1.33, -0.49], [0.17, -0.05], [-0.0, 0.26], [0.01, -0.04], [0.2, -0.06], [0.23, -0.26], [0.64, -0.11]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saved Partial results at the end of episode 100.\n",
      "Episode: 101/400, Epsilon:0, Average reward: -66.39\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-40.0, -22.0, -9.0, -10.0, -18.0, -11.0, -28.0, -12.0, -31.0, -10.0] \n",
      " [-407.0, -193.0, -257.0, -171.0, -311.0, -1825.0, -1681.0, -1444.0, -234.0, -171.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.05, -0.17], [0.08, -0.05], [0.04, -0.01], [0.2, -0.09], [0.36, -0.15], [0.27, -0.2], [-0.17, -0.22], [0.26, -0.41], [0.56, -0.43], [0.2, -0.09]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 102/400, Epsilon:0, Average reward: -72.18\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-37.0, -12.0, -40.0, -49.0, -13.0, -35.0, -13.0, -1.0, -24.0, -25.0] \n",
      " [-536.0, -84.0, -442.0, -476.0, -229.0, -475.0, -16.0, -124.0, -1882.0, -1631.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.12, -0.07], [0.08, -0.25], [-0.1, -0.2], [0.07, -0.01], [0.46, -0.03], [-0.11, -0.04], [0.21, 0.13], [0.09, -0.04], [0.56, -0.63], [0.24, -0.33]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 103/400, Epsilon:0, Average reward: -49.0\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-14.0, -36.0, -29.0, -22.0, -8.0, -12.0, -26.0, -18.0, -13.0, -10.0] \n",
      " [-199.0, -920.0, -787.0, -335.0, -57.0, -87.0, -508.0, -94.0, -628.0, -63.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.01, -0.09], [0.75, -0.53], [0.25, -0.4], [0.07, -0.1], [-0.0, -0.06], [-0.02, -0.28], [-0.32, 0.2], [-0.09, 0.04], [0.19, -0.28], [-0.16, -0.08]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 104/400, Epsilon:0, Average reward: -56.62\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-5.0, -13.0, -15.0, -13.0, -24.0, -24.0, -21.0, -25.0, -21.0, -38.0] \n",
      " [-96.0, -94.0, -1003.0, -18.0, -22.0, -505.0, -1145.0, -397.0, -466.0, -206.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.19, -0.07], [0.02, 0.01], [0.18, -0.25], [0.07, -0.14], [0.21, 0.09], [0.01, 0.02], [-0.19, -0.07], [-0.05, 0.07], [-0.22, 0.35], [0.6, -0.66]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 105/400, Epsilon:0, Average reward: -54.95\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-34.0, -52.0, -20.0, -32.0, -16.0, -34.0, -30.0, -39.0, -44.0, -6.0] \n",
      " [-145.0, -771.0, -203.0, -645.0, -534.0, -312.0, -112.0, -67.0, -1212.0, -34.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.11, -0.3], [0.05, -0.47], [0.21, -0.06], [0.01, 0.03], [0.46, -0.39], [0.07, -0.17], [0.35, -0.35], [0.06, -0.26], [0.84, -0.99], [-0.06, 0.1]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 106/400, Epsilon:0, Average reward: -85.91\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-15.0, -21.0, -20.0, -18.0, -23.0, -13.0, -0.0, -21.0, -21.0, -33.0] \n",
      " [-26.0, -47.0, -1158.0, -94.0, -65.0, -171.0, -34.0, -47.0, -283.0, -177.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.1, -0.4], [0.3, -0.03], [0.12, -0.1], [0.57, -0.52], [-0.17, 0.08], [0.08, -0.13], [0.0, -0.0], [0.3, -0.03], [-0.16, 0.09], [0.55, -0.32]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 107/400, Epsilon:0, Average reward: -58.95\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-15.0, -18.0, -35.0, -16.0, -25.0, -6.0, -48.0, -25.0, -36.0, -24.0] \n",
      " [-54.0, -26.0, -473.0, -43.0, -734.0, -20.0, -975.0, -252.0, -310.0, -539.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.09, 0.03], [0.13, -0.04], [-0.25, 0.3], [-0.07, -0.0], [-0.05, 0.04], [0.16, -0.06], [0.33, -0.03], [0.06, -0.21], [0.51, -0.45], [-0.05, 0.09]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 108/400, Epsilon:0, Average reward: -80.72\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-34.0, -18.0, -7.0, -0.0, -41.0, -45.0, -41.0, -50.0, -18.0, -33.0] \n",
      " [-343.0, -57.0, -88.0, -41.0, -147.0, -833.0, -250.0, -994.0, -721.0, -123.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.06, 0.26], [0.32, 0.16], [0.11, -0.0], [0.0, -0.0], [0.67, -0.49], [-0.1, -0.1], [0.28, -0.29], [0.1, -0.02], [0.1, -0.14], [0.1, -0.04]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [ 0. -0.]\n",
      "Episode: 109/400, Epsilon:0, Average reward: -44.62\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-4.0, -17.0, -24.0, -38.0, -34.0, -3.0, -62.0, -24.0, -3.0, -22.0] \n",
      " [-161.0, -25.0, -255.0, -908.0, -660.0, -169.0, -250.0, -293.0, -35.0, -731.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.02, 0.08], [0.11, -0.08], [0.05, -0.23], [-0.08, 0.16], [-0.25, 0.0], [-0.01, -0.03], [0.03, -0.17], [0.1, 0.02], [-0.12, 0.11], [0.18, -0.14]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 110/400, Epsilon:0, Average reward: -94.37\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-31.0, -0.0, -10.0, -40.0, -41.0, -8.0, -26.0, -52.0, -26.0, -28.0] \n",
      " [-198.0, -21.0, -1545.0, -427.0, -845.0, -1037.0, -915.0, -886.0, -1152.0, -760.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.26, -0.24], [0.01, -0.01], [0.02, -0.05], [0.68, -0.72], [-0.21, -0.44], [0.06, -0.07], [0.21, -0.1], [0.37, -0.34], [0.18, -0.09], [0.7, -0.76]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 111/400, Epsilon:0, Average reward: -53.53\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-15.0, -15.0, -21.0, -53.0, -32.0, -30.0, -15.0, -11.0, -36.0, -5.0] \n",
      " [-75.0, -276.0, -96.0, -1396.0, -256.0, -125.0, -693.0, -66.0, -219.0, -104.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.27, -0.23], [-0.08, 0.07], [-0.17, 0.24], [-0.42, 0.19], [-0.22, 0.0], [0.22, -0.06], [-0.08, 0.11], [0.26, -0.22], [-0.45, 0.1], [0.06, -0.08]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 112/400, Epsilon:0, Average reward: -47.12\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-20.0, -30.0, -30.0, -27.0, -10.0, -62.0, -19.0, -62.0, -12.0, -21.0] \n",
      " [-1102.0, -396.0, -76.0, -530.0, -568.0, -282.0, -363.0, -282.0, -45.0, -830.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.09, 0.04], [-0.05, 0.12], [-0.09, -0.05], [0.48, -0.58], [0.15, -0.17], [0.03, 0.24], [0.23, -0.08], [0.03, 0.24], [-0.05, -0.03], [0.2, -0.12]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 113/400, Epsilon:0, Average reward: -58.67\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-11.0, -52.0, -25.0, -46.0, -15.0, -25.0, -0.0, -11.0, -37.0, -19.0] \n",
      " [-36.0, -1050.0, -38.0, -994.0, -623.0, -140.0, -116.0, -38.0, -300.0, -720.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.0, 0.06], [-0.5, 0.05], [0.16, 0.07], [0.17, -0.1], [0.1, -0.11], [0.37, -0.23], [0.0, -0.0], [-0.11, 0.03], [-0.43, 0.09], [0.05, 0.02]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 114/400, Epsilon:0, Average reward: -40.75\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-29.0, -25.0, -58.0, -46.0, -16.0, -20.0, -21.0, -18.0, -35.0, -23.0] \n",
      " [-195.0, -244.0, -274.0, -702.0, -299.0, -548.0, -137.0, -41.0, -296.0, -177.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.01, -0.15], [-0.07, -0.17], [0.19, -0.13], [0.27, -0.13], [0.14, 0.11], [0.05, 0.08], [-0.14, 0.12], [-0.03, 0.08], [-0.07, 0.13], [-0.35, 0.16]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 115/400, Epsilon:0, Average reward: -38.69\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-36.0, -7.0, -47.0, -67.0, -23.0, -11.0, -56.0, -17.0, -21.0, -46.0] \n",
      " [-467.0, -141.0, -521.0, -413.0, -477.0, -81.0, -114.0, -28.0, -46.0, -532.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.18, 0.07], [0.17, -0.04], [0.3, -0.08], [-0.14, -0.15], [0.22, -0.17], [-0.03, 0.01], [-0.02, 0.06], [-0.19, -0.08], [0.05, -0.12], [0.29, -0.27]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 116/400, Epsilon:0, Average reward: -37.55\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-15.0, -22.0, -0.0, -14.0, -33.0, -74.0, -41.0, -64.0, -37.0, -52.0] \n",
      " [-278.0, -36.0, -68.0, -66.0, -176.0, -254.0, -455.0, -475.0, -741.0, -68.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.0, 0.02], [-0.0, -0.18], [0.0, 0.0], [-0.05, -0.06], [-0.07, -0.06], [0.1, 0.01], [-0.43, 0.17], [0.29, -0.05], [0.61, -0.48], [0.02, -0.04]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 117/400, Epsilon:0, Average reward: -39.88\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-28.0, -26.0, -0.0, -56.0, -43.0, -41.0, -42.0, -23.0, -28.0, -19.0] \n",
      " [-666.0, -193.0, -79.0, -562.0, -532.0, -110.0, -289.0, -714.0, -781.0, -183.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.14, -0.05], [0.11, 0.07], [0.0, -0.0], [0.01, -0.11], [0.72, 0.75], [0.22, -0.33], [0.03, -0.35], [-0.04, -0.1], [-0.13, 0.04], [0.04, -0.14]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 118/400, Epsilon:0, Average reward: -47.38\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-29.0, -54.0, -64.0, -2.0, -17.0, -23.0, -28.0, -51.0, -50.0, -81.0] \n",
      " [-611.0, -369.0, -661.0, -95.0, -70.0, -71.0, -414.0, -798.0, -91.0, -160.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.01, 0.15], [-0.01, 0.09], [0.27, -0.22], [0.06, 0.0], [0.19, -0.16], [0.43, -0.23], [-0.14, 0.06], [0.03, 0.17], [0.04, 0.03], [0.23, 0.38]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 119/400, Epsilon:0, Average reward: -73.53\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-21.0, -39.0, -38.0, -9.0, -64.0, -39.0, -53.0, -10.0, -37.0, -41.0] \n",
      " [-294.0, -1743.0, -329.0, -176.0, -450.0, -1743.0, -76.0, -401.0, -176.0, -279.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.14, 0.15], [0.01, -0.04], [0.21, 0.05], [0.43, -0.49], [-0.34, 0.09], [0.01, -0.04], [0.61, -0.27], [-0.1, 0.2], [0.32, -0.19], [-0.1, 0.12]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 120/400, Epsilon:0, Average reward: -49.88\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-101.0, -16.0, -38.0, -26.0, -14.0, -4.0, -20.0, -34.0, -51.0, -0.0] \n",
      " [-720.0, -855.0, -788.0, -43.0, -75.0, -191.0, -17.0, -353.0, -901.0, -18.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.04, -0.38], [0.23, -0.11], [-0.14, 0.03], [-0.25, -0.2], [0.0, 0.02], [0.14, -0.14], [-0.03, -0.32], [0.13, -0.1], [-0.39, 0.35], [0.0, -0.0]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 121/400, Epsilon:0, Average reward: -51.83\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-47.0, -77.0, -41.0, -51.0, -0.0, -32.0, -38.0, -60.0, -40.0, -12.0] \n",
      " [-1026.0, -579.0, -196.0, -1160.0, -35.0, -187.0, -298.0, -540.0, -538.0, -430.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.18, -0.28], [0.24, -0.18], [-0.18, -0.14], [-0.02, 0.19], [-0.0, 0.0], [0.17, -0.08], [0.08, 0.01], [0.16, -0.21], [0.53, -0.31], [0.05, -0.03]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 122/400, Epsilon:0, Average reward: -64.74\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-46.0, -87.0, -9.0, -56.0, -29.0, -18.0, -24.0, -12.0, -52.0, -28.0] \n",
      " [-158.0, -844.0, -209.0, -148.0, -300.0, -74.0, -899.0, -20.0, -73.0, -49.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.5, -0.44], [0.29, -0.28], [0.09, -0.12], [0.02, 0.27], [-0.22, 0.35], [-0.18, -0.22], [0.1, 0.03], [-0.08, 0.14], [0.12, 0.02], [0.17, 0.04]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 123/400, Epsilon:0, Average reward: -51.86\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-48.0, -54.0, -89.0, -20.0, -45.0, -3.0, -70.0, -44.0, -57.0, -71.0] \n",
      " [-135.0, -528.0, -888.0, -221.0, -888.0, -30.0, -376.0, -296.0, -907.0, -1020.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.29, -0.15], [-0.08, 0.14], [-0.09, 0.27], [-0.2, 0.11], [0.03, 0.23], [0.02, 0.0], [0.17, 0.07], [-0.18, 0.15], [0.06, -0.18], [0.4, -0.58]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 124/400, Epsilon:0, Average reward: -74.21\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-40.0, -33.0, -17.0, -60.0, -56.0, -5.0, -61.0, -36.0, -36.0, -19.0] \n",
      " [-278.0, -1065.0, -180.0, -1122.0, -298.0, -190.0, -949.0, -1735.0, -49.0, -1207.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.05, 0.09], [0.41, -0.25], [-0.02, 0.04], [0.11, -0.24], [0.17, -0.01], [0.04, -0.05], [0.0, -0.09], [0.03, -0.1], [0.15, -0.16], [-0.09, -0.05]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 125/400, Epsilon:0, Average reward: -41.65\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-22.0, -0.0, -34.0, -32.0, -50.0, -49.0, -23.0, -69.0, -16.0, -49.0] \n",
      " [-231.0, -87.0, -603.0, -81.0, -190.0, -662.0, -503.0, -71.0, -48.0, -124.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.02, 0.05], [0.0, -0.0], [-0.2, 0.22], [0.21, -0.14], [0.13, -0.21], [0.09, 0.02], [0.02, 0.0], [0.36, -0.28], [-0.05, -0.12], [-0.18, 0.28]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saved Partial results at the end of episode 125.\n",
      "Episode: 126/400, Epsilon:0, Average reward: -46.42\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-50.0, -54.0, -55.0, -0.0, -49.0, -36.0, -39.0, -62.0, -0.0, -108.0] \n",
      " [-451.0, -597.0, -494.0, -57.0, -116.0, -406.0, -80.0, -277.0, -25.0, -718.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.31, -0.25], [0.14, 0.13], [0.08, -0.16], [-0.0, -0.0], [0.31, -0.19], [0.22, -0.16], [0.1, -0.35], [-0.35, 0.13], [-0.0, 0.0], [-0.06, -0.0]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 127/400, Epsilon:0, Average reward: -49.5\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-19.0, -4.0, -0.0, -21.0, -36.0, -22.0, -52.0, -19.0, -50.0, -80.0] \n",
      " [-282.0, -35.0, -74.0, -52.0, -300.0, -202.0, -144.0, -282.0, -718.0, -531.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.07, -0.28], [-0.11, 0.11], [0.0, 0.0], [0.14, 0.06], [0.23, 0.01], [0.22, -0.06], [-0.08, 0.29], [0.07, -0.28], [0.33, -0.32], [-0.07, -0.32]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 128/400, Epsilon:0, Average reward: -78.29\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-28.0, -17.0, -37.0, -53.0, -32.0, -103.0, -70.0, -37.0, -16.0, -6.0] \n",
      " [-428.0, -205.0, -614.0, -447.0, -1487.0, -652.0, -355.0, -191.0, -114.0, -91.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.18, -0.25], [0.02, 0.01], [0.19, -0.1], [0.27, -0.67], [0.24, -0.03], [-0.4, 0.14], [0.42, -0.13], [0.09, -0.33], [0.02, -0.07], [-0.0, -0.01]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0. -0.]\n",
      "Episode: 129/400, Epsilon:0, Average reward: -42.23\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-54.0, -62.0, -0.0, -58.0, -65.0, -73.0, -52.0, -74.0, -72.0, -23.0] \n",
      " [-377.0, -63.0, -59.0, -267.0, -668.0, -636.0, -374.0, -296.0, -243.0, -1079.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.04, -0.06], [0.28, 0.21], [0.0, -0.0], [-0.0, -0.13], [0.36, -0.27], [0.63, -0.23], [0.16, 0.44], [-0.31, 0.12], [0.22, -0.01], [-0.11, 0.09]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 130/400, Epsilon:0, Average reward: -61.22\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-33.0, -48.0, -32.0, -30.0, -29.0, -13.0, -62.0, -32.0, -47.0, -14.0] \n",
      " [-1690.0, -104.0, -315.0, -575.0, -97.0, -61.0, -607.0, -111.0, -86.0, -174.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.03, 0.1], [0.13, -0.14], [-0.1, -0.14], [-0.07, -0.03], [0.11, -0.32], [-0.12, 0.12], [0.09, -0.24], [-0.1, 0.18], [0.01, -0.31], [0.13, 0.04]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 131/400, Epsilon:0, Average reward: -106.53\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-77.0, -72.0, -127.0, -63.0, -48.0, -55.0, -127.0, -41.0, -30.0, -66.0] \n",
      " [-301.0, -2981.0, -525.0, -1103.0, -276.0, -1099.0, -756.0, -604.0, -57.0, -711.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.41, -0.26], [0.3, -0.04], [0.53, -0.38], [0.46, -0.27], [0.05, -0.03], [0.13, -0.15], [-0.08, -0.14], [0.13, -0.1], [0.11, 0.23], [-0.0, 0.04]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 132/400, Epsilon:0, Average reward: -33.51\n",
      "Saving architecture, weights, optimizer state for best agent-0\n",
      "New best agent found. Saved in C:\\Users\\Rzhang\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\Single_Cross_Mod2\\Agents_Results\\Ep_400_A_AC_State_CellsT_Act_phases_Rew_Queues\\BestAgent0_Memory.p\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-82.0, -65.0, -77.0, -5.0, -90.0, -97.0, -42.0, -89.0, -98.0, -11.0] \n",
      " [-268.0, -296.0, -381.0, -33.0, -260.0, -215.0, -140.0, -289.0, -214.0, -63.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.34, -0.4], [0.38, -0.42], [-0.17, -0.29], [-0.11, 0.1], [0.26, 0.24], [-0.08, -0.04], [0.35, -0.37], [0.13, -0.05], [0.49, -0.24], [0.03, 0.08]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 133/400, Epsilon:0, Average reward: -46.46\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-0.0, -5.0, -65.0, -54.0, -53.0, -52.0, -100.0, -26.0, -45.0, -37.0] \n",
      " [-13.0, -31.0, -227.0, -250.0, -1408.0, -327.0, -331.0, -45.0, -549.0, -359.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.0, 0.0], [0.09, -0.09], [0.13, -0.14], [0.02, -0.05], [0.09, -0.15], [-0.05, -0.05], [0.11, -0.14], [0.41, -0.19], [0.17, -0.2], [-0.06, -0.04]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 134/400, Epsilon:0, Average reward: -52.7\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-35.0, -65.0, -57.0, -28.0, -45.0, -29.0, -30.0, -79.0, -108.0, -56.0] \n",
      " [-46.0, -228.0, -709.0, -33.0, -1146.0, -48.0, -827.0, -479.0, -775.0, -65.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.03, 0.06], [0.54, -0.75], [0.06, -0.05], [0.16, -0.1], [-0.03, 0.13], [-0.14, 0.09], [0.17, 0.17], [0.1, 0.08], [0.29, -0.21], [0.39, -0.21]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 135/400, Epsilon:0, Average reward: -40.39\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-119.0, -60.0, -0.0, -0.0, -78.0, -62.0, -59.0, -84.0, -76.0, -26.0] \n",
      " [-358.0, -119.0, -75.0, -46.0, -730.0, -174.0, -545.0, -473.0, -701.0, -72.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.52, -0.12], [0.51, -0.5], [0.0, -0.0], [-0.0, 0.0], [-0.22, -0.05], [-0.11, 0.16], [0.02, -0.11], [-0.11, -0.02], [-0.14, -0.03], [0.17, -0.12]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 136/400, Epsilon:0, Average reward: -45.03\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-22.0, -53.0, -102.0, -21.0, -54.0, -33.0, -62.0, -29.0, -71.0, -14.0] \n",
      " [-47.0, -85.0, -472.0, -686.0, -718.0, -121.0, -479.0, -222.0, -858.0, -446.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.18, -0.2], [-0.19, -0.14], [0.19, -0.23], [0.01, 0.08], [-0.05, 0.09], [-0.05, -0.03], [0.25, -0.58], [-0.02, -0.04], [0.03, -0.19], [0.06, 0.09]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 137/400, Epsilon:0, Average reward: -60.46\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-48.0, -29.0, -66.0, -27.0, -31.0, -86.0, -116.0, -7.0, -13.0, -21.0] \n",
      " [-281.0, -54.0, -284.0, -43.0, -126.0, -264.0, -267.0, -71.0, -52.0, -911.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.1, 0.05], [0.05, -0.07], [0.77, -0.27], [0.17, 0.11], [-0.03, 0.0], [0.09, -0.26], [0.25, -0.33], [-0.02, -0.04], [-0.01, -0.05], [0.02, 0.03]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 138/400, Epsilon:0, Average reward: -41.75\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-102.0, -30.0, -7.0, -49.0, -86.0, -0.0, -57.0, -7.0, -81.0, -52.0] \n",
      " [-658.0, -209.0, -501.0, -649.0, -754.0, -80.0, -445.0, -501.0, -603.0, -184.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.2, -0.23], [0.22, -0.12], [0.0, -0.0], [0.01, 0.2], [-0.16, 0.17], [-0.0, 0.0], [0.02, 0.05], [0.0, -0.0], [-0.09, -0.02], [-0.04, 0.26]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 139/400, Epsilon:0, Average reward: -50.64\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-54.0, -60.0, -12.0, -65.0, -1.0, -47.0, -116.0, -57.0, -149.0, -96.0] \n",
      " [-28.0, -729.0, -33.0, -728.0, -14.0, -317.0, -329.0, -516.0, -700.0, -121.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.28, 0.11], [0.38, -0.34], [0.12, -0.1], [0.06, 0.12], [0.0, -0.0], [-0.12, 0.11], [0.08, 0.13], [-0.01, -0.05], [0.36, -0.19], [-0.25, -0.11]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 140/400, Epsilon:0, Average reward: -37.64\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-13.0, -94.0, -59.0, -149.0, -57.0, -140.0, -129.0, -50.0, -56.0, -118.0] \n",
      " [-78.0, -320.0, -102.0, -461.0, -59.0, -510.0, -329.0, -167.0, -88.0, -292.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.1, -0.21], [0.25, -0.22], [0.32, -0.52], [0.56, -0.68], [0.06, -0.18], [0.07, -0.11], [-0.14, 0.08], [0.25, 0.31], [0.38, -0.06], [0.31, -0.09]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 141/400, Epsilon:0, Average reward: -46.58\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-44.0, -52.0, -83.0, -42.0, -11.0, -17.0, -70.0, -71.0, -40.0, -60.0] \n",
      " [-57.0, -288.0, -85.0, -840.0, -44.0, -59.0, -219.0, -301.0, -385.0, -300.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.67, -0.42], [-0.16, -0.02], [0.11, 0.14], [-0.06, 0.03], [0.14, -0.05], [-0.13, 0.13], [0.49, -0.17], [0.22, -0.25], [0.1, 0.01], [0.11, -0.04]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 142/400, Epsilon:0, Average reward: -46.03\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-15.0, -69.0, -64.0, -42.0, -67.0, -63.0, -48.0, -62.0, -46.0, -60.0] \n",
      " [-312.0, -98.0, -873.0, -764.0, -109.0, -1095.0, -584.0, -137.0, -105.0, -831.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.05, 0.03], [-0.02, 0.1], [0.65, -0.37], [0.1, -0.04], [0.15, 0.02], [0.53, -0.2], [0.13, -0.12], [-0.05, -0.19], [0.1, -0.2], [-0.13, 0.15]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 143/400, Epsilon:0, Average reward: -51.98\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-48.0, -122.0, -34.0, -102.0, -54.0, -112.0, -1.0, -42.0, -38.0, -66.0] \n",
      " [-141.0, -236.0, -96.0, -663.0, -581.0, -331.0, -56.0, -26.0, -54.0, -58.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.32, 0.14], [0.35, -0.29], [0.04, -0.06], [0.43, 0.2], [0.26, -0.31], [-0.11, -0.12], [0.0, 0.0], [0.16, 0.11], [0.2, -0.27], [0.25, 0.41]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 144/400, Epsilon:0, Average reward: -43.95\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-58.0, -0.0, -67.0, -32.0, -84.0, -96.0, -114.0, -1.0, -113.0, -80.0] \n",
      " [-61.0, -58.0, -455.0, -660.0, -378.0, -177.0, -658.0, -42.0, -197.0, -347.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.39, -0.25], [-0.0, 0.0], [0.08, -0.1], [0.33, -0.31], [0.03, -0.07], [-0.07, 0.15], [0.15, -0.15], [-0.0, -0.0], [0.7, -0.25], [0.17, -0.51]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 145/400, Epsilon:0, Average reward: -60.11\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-41.0, -32.0, -52.0, -70.0, -43.0, -44.0, -54.0, -101.0, -114.0, -83.0] \n",
      " [-185.0, -732.0, -91.0, -546.0, -34.0, -148.0, -328.0, -637.0, -1021.0, -478.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.1, 0.06], [0.14, -0.06], [0.03, 0.06], [0.07, -0.48], [-0.06, -0.13], [-0.05, 0.09], [0.17, -0.17], [-0.31, 0.13], [0.3, 0.2], [-0.08, 0.15]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 146/400, Epsilon:0, Average reward: -54.64\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-6.0, -52.0, -1.0, -108.0, -41.0, -65.0, -94.0, -38.0, -73.0, -63.0] \n",
      " [-102.0, -129.0, -33.0, -136.0, -56.0, -210.0, -466.0, -27.0, -551.0, -595.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.08, -0.07], [-0.01, 0.08], [0.0, -0.0], [0.75, -0.4], [0.32, -0.29], [0.57, -0.38], [0.28, 0.13], [-0.05, 0.07], [0.13, 0.08], [-0.06, 0.38]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 147/400, Epsilon:0, Average reward: -48.83\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-27.0, -9.0, -162.0, -61.0, -4.0, -69.0, -74.0, -23.0, -8.0, -51.0] \n",
      " [-980.0, -299.0, -311.0, -1076.0, -29.0, -44.0, -193.0, -89.0, -44.0, -67.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.0, 0.1], [0.16, -0.1], [1.05, -0.59], [-0.02, 0.0], [0.05, 0.0], [-0.07, 0.11], [-0.39, 0.01], [-0.07, 0.06], [0.0, -0.0], [0.08, -0.06]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 148/400, Epsilon:0, Average reward: -60.24\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-73.0, -94.0, -31.0, -72.0, -97.0, -49.0, -44.0, -87.0, -114.0, -46.0] \n",
      " [-114.0, -202.0, -76.0, -207.0, -142.0, -126.0, -52.0, -68.0, -1716.0, -32.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.65, 0.07], [-0.05, -0.07], [-0.06, 0.39], [-0.12, 0.39], [0.13, -0.25], [0.21, -0.13], [-0.05, 0.03], [0.14, -0.05], [0.12, 0.63], [0.11, -0.25]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 149/400, Epsilon:0, Average reward: -71.52\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-102.0, -24.0, -63.0, -89.0, -71.0, -61.0, -146.0, -15.0, -122.0, -97.0] \n",
      " [-328.0, -160.0, -356.0, -343.0, -328.0, -928.0, -349.0, -2223.0, -379.0, -1244.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.11, -0.03], [0.23, -0.24], [0.34, 0.01], [-0.24, 0.02], [-0.06, 0.07], [0.14, 0.11], [-0.35, 0.38], [-0.0, -0.0], [-0.09, 0.01], [0.2, -0.4]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 150/400, Epsilon:0, Average reward: -54.6\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-103.0, -105.0, -36.0, -66.0, -118.0, -74.0, -73.0, -103.0, -42.0, -67.0] \n",
      " [-226.0, -328.0, -740.0, -357.0, -65.0, -62.0, -305.0, -226.0, -42.0, -128.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.18, -0.25], [0.12, 0.08], [0.14, -0.2], [0.04, -0.13], [0.16, -0.1], [0.33, -0.31], [0.29, -0.16], [0.18, -0.25], [-0.05, 0.1], [0.13, -0.08]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Saving architecture, weights, optimizer state for agent-0\n",
      "Dumping agent-0 memory into pickle file\n",
      "Dumping Training Results into pickle file.\n",
      "Dumping Loss Results into pickle file.\n",
      "Saved Partial results at the end of episode 150.\n",
      "Episode: 151/400, Epsilon:0, Average reward: -48.01\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-57.0, -98.0, -99.0, -152.0, -8.0, -34.0, -182.0, -77.0, -12.0, -36.0] \n",
      " [-476.0, -525.0, -786.0, -184.0, -32.0, -25.0, -358.0, -109.0, -28.0, -379.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.19, -0.03], [0.18, -0.32], [-0.16, -0.0], [-0.36, 0.06], [0.01, 0.01], [0.0, -0.02], [-0.11, -0.12], [0.33, -0.4], [-0.1, 0.2], [0.08, -0.09]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 152/400, Epsilon:0, Average reward: -49.22\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-97.0, -51.0, -39.0, -107.0, -30.0, -46.0, -100.0, -97.0, -78.0, -87.0] \n",
      " [-334.0, -104.0, -777.0, -696.0, -121.0, -164.0, -545.0, -158.0, -58.0, -282.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.01, -0.09], [-0.0, -0.09], [-0.08, 0.09], [0.2, 0.39], [-0.03, -0.02], [-0.1, 0.09], [-0.01, -0.05], [0.12, -0.08], [-0.17, 0.22], [0.13, -0.22]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 153/400, Epsilon:0, Average reward: -48.1\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-100.0, -76.0, -132.0, -71.0, -9.0, -68.0, -31.0, -153.0, -67.0, -109.0] \n",
      " [-145.0, -83.0, -419.0, -681.0, -63.0, -839.0, -178.0, -418.0, -71.0, -240.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.04, -0.24], [-0.14, 0.07], [0.28, -0.13], [0.07, -0.18], [0.25, -0.21], [-0.09, -0.21], [-0.03, -0.15], [0.63, -0.35], [0.2, -0.15], [0.32, -0.33]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 154/400, Epsilon:0, Average reward: -49.77\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-36.0, -142.0, -103.0, -71.0, -130.0, -52.0, -31.0, -132.0, -2.0, -8.0] \n",
      " [-281.0, -256.0, -323.0, -1258.0, -910.0, -804.0, -45.0, -409.0, -38.0, -71.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.03, 0.09], [-0.41, 0.29], [-0.02, 0.18], [-0.06, -0.02], [0.33, 0.07], [0.5, 0.22], [-0.08, 0.05], [0.26, -0.04], [0.05, -0.06], [0.02, -0.03]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 155/400, Epsilon:0, Average reward: -67.95\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-115.0, -122.0, -64.0, -98.0, -60.0, -231.0, -2.0, -24.0, -212.0, -134.0] \n",
      " [-1281.0, -415.0, -352.0, -1041.0, -102.0, -249.0, -30.0, -36.0, -617.0, -487.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.37, -0.36], [-0.09, -0.09], [-0.04, -0.2], [0.13, 0.05], [-0.06, -0.02], [0.45, -0.56], [0.03, -0.03], [-0.06, 0.13], [-0.14, 0.14], [0.2, 0.12]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 156/400, Epsilon:0, Average reward: -54.58\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-77.0, -86.0, -163.0, -71.0, -90.0, -103.0, -169.0, -154.0, -124.0, -54.0] \n",
      " [-1046.0, -74.0, -268.0, -70.0, -506.0, -295.0, -209.0, -387.0, -98.0, -64.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.09, -0.01], [-0.01, -0.12], [0.3, -0.27], [-0.05, 0.03], [0.18, -0.06], [-0.06, -0.13], [0.27, -0.26], [-0.66, 0.31], [0.11, 0.22], [0.24, -0.09]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 157/400, Epsilon:0, Average reward: -45.38\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-175.0, -1.0, -123.0, -115.0, -92.0, -96.0, -86.0, -130.0, -86.0, -97.0] \n",
      " [-755.0, -40.0, -306.0, -374.0, -206.0, -202.0, -470.0, -307.0, -49.0, -531.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.73, -0.9], [0.0, 0.0], [-0.18, 0.22], [0.39, -0.52], [0.35, 0.08], [0.51, -0.27], [-0.37, 0.23], [0.53, -0.21], [0.16, -0.34], [0.0, 0.09]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 158/400, Epsilon:0, Average reward: -53.58\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-36.0, -0.0, -36.0, -22.0, -129.0, -20.0, -38.0, -92.0, -26.0, -31.0] \n",
      " [-179.0, -50.0, -61.0, -77.0, -431.0, -87.0, -38.0, -144.0, -115.0, -1282.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.08, -0.18], [-0.0, 0.0], [0.18, -0.13], [0.0, -0.05], [-0.13, -0.11], [0.04, -0.13], [0.08, 0.0], [-0.09, -0.06], [0.04, -0.01], [0.18, -0.0]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 159/400, Epsilon:0, Average reward: -64.01\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-18.0, -87.0, -140.0, -10.0, -119.0, -64.0, -117.0, -70.0, -67.0, -129.0] \n",
      " [-110.0, -145.0, -262.0, -456.0, -288.0, -125.0, -1315.0, -222.0, -73.0, -555.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.1, -0.11], [0.16, -0.1], [0.01, 0.07], [0.01, 0.05], [0.12, -0.07], [0.19, 0.04], [0.12, -0.39], [0.15, -0.21], [-0.14, -0.04], [0.57, -0.42]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 160/400, Epsilon:0, Average reward: -40.75\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-151.0, -105.0, -116.0, -93.0, -44.0, -18.0, -14.0, -86.0, -104.0, -24.0] \n",
      " [-195.0, -208.0, -305.0, -150.0, -176.0, -62.0, -37.0, -293.0, -576.0, -362.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.04, 0.09], [0.11, -0.03], [-0.13, -0.19], [0.01, -0.11], [0.19, 0.22], [0.05, -0.12], [0.07, -0.21], [0.1, -0.08], [-0.28, 0.03], [0.26, -0.19]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 161/400, Epsilon:0, Average reward: -67.82\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-89.0, -41.0, -56.0, -140.0, -57.0, -76.0, -215.0, -31.0, -68.0, -87.0] \n",
      " [-1296.0, -21.0, -34.0, -1225.0, -72.0, -544.0, -383.0, -370.0, -106.0, -1248.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.07, -0.16], [0.26, -0.24], [-0.09, -0.06], [0.25, 0.07], [0.11, 0.15], [0.48, -0.48], [0.77, -0.18], [-0.15, 0.01], [0.24, -0.28], [0.5, -0.45]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 162/400, Epsilon:0, Average reward: -60.13\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-37.0, -123.0, -128.0, -87.0, -105.0, -130.0, -1.0, -59.0, -3.0, -72.0] \n",
      " [-24.0, -84.0, -1161.0, -1405.0, -1364.0, -75.0, -14.0, -1057.0, -43.0, -66.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.15, 0.11], [0.2, -0.18], [-0.11, 0.03], [0.2, -0.4], [0.14, -0.26], [0.17, -0.25], [-0.0, 0.0], [-0.0, -0.01], [0.02, -0.01], [0.24, -0.19]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 163/400, Epsilon:0, Average reward: -42.75\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-164.0, -33.0, -82.0, -31.0, -75.0, -47.0, -42.0, -131.0, -178.0, -63.0] \n",
      " [-384.0, -436.0, -115.0, -176.0, -331.0, -35.0, -54.0, -485.0, -457.0, -81.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.07, -0.05], [0.07, -0.09], [0.28, -0.23], [-0.16, 0.01], [0.04, -0.09], [0.0, -0.11], [0.16, -0.35], [-0.1, 0.3], [0.38, -0.41], [0.15, 0.39]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 164/400, Epsilon:0, Average reward: -54.3\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-102.0, -6.0, -58.0, -58.0, -30.0, -112.0, -1.0, -67.0, -165.0, -1.0] \n",
      " [-40.0, -36.0, -91.0, -122.0, -79.0, -528.0, -32.0, -134.0, -337.0, -45.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.44, 0.27], [0.06, -0.0], [0.37, -0.23], [0.09, 0.1], [-0.07, 0.14], [0.11, 0.11], [0.0, 0.0], [0.0, -0.11], [0.48, -0.34], [-0.0, 0.0]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 165/400, Epsilon:0, Average reward: -41.19\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-38.0, -52.0, -196.0, -131.0, -175.0, -111.0, -88.0, -33.0, -176.0, -27.0] \n",
      " [-123.0, -408.0, -318.0, -366.0, -247.0, -388.0, -1062.0, -58.0, -534.0, -109.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.04, -0.04], [0.09, -0.18], [0.24, -0.22], [0.05, -0.16], [0.5, -0.07], [0.07, -0.27], [0.05, -0.25], [0.14, -0.23], [0.05, -0.12], [0.12, -0.01]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 166/400, Epsilon:0, Average reward: -43.69\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-86.0, -71.0, -242.0, -76.0, -92.0, -72.0, -45.0, -138.0, -44.0, -106.0] \n",
      " [-282.0, -360.0, -376.0, -661.0, -114.0, -104.0, -84.0, -185.0, -45.0, -462.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[0.16, -0.35], [0.14, -0.34], [0.59, -0.29], [0.0, 0.03], [0.17, -0.22], [0.15, -0.13], [0.03, -0.12], [0.08, -0.13], [-0.24, 0.06], [-0.52, 0.22]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 167/400, Epsilon:0, Average reward: -42.46\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-162.0, -75.0, -36.0, -117.0, -172.0, -132.0, -75.0, -77.0, -24.0, -89.0] \n",
      " [-379.0, -453.0, -34.0, -413.0, -235.0, -362.0, -98.0, -119.0, -523.0, -476.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.48, 0.49], [-0.09, 0.12], [0.01, -0.21], [0.42, -0.3], [-0.2, -0.2], [0.23, -0.27], [0.2, -0.06], [0.32, -0.43], [0.08, 0.02], [0.06, -0.1]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 168/400, Epsilon:0, Average reward: -71.71\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-145.0, -110.0, -47.0, -58.0, -68.0, -157.0, -4.0, -108.0, -70.0, -69.0] \n",
      " [-1967.0, -53.0, -31.0, -91.0, -162.0, -1409.0, -31.0, -107.0, -126.0, -1712.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.05, -0.11], [0.06, -0.22], [0.21, 0.27], [-0.02, 0.07], [-0.24, -0.22], [-0.02, 0.24], [0.03, -0.01], [0.21, -0.24], [0.1, -0.09], [-0.02, 0.02]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Episode: 169/400, Epsilon:0, Average reward: -47.06\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-71.0, -63.0, -141.0, -91.0, -78.0, -60.0, -55.0, -75.0, -14.0, -121.0] \n",
      " [-213.0, -66.0, -1736.0, -72.0, -1219.0, -629.0, -188.0, -516.0, -15.0, -952.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.23, -0.15], [-0.03, -0.1], [0.16, -0.03], [0.1, 0.45], [-0.03, -0.25], [0.31, -0.23], [0.47, -0.16], [0.05, -0.03], [0.22, -0.17], [0.04, 0.07]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 170/400, Epsilon:0, Average reward: -69.13\n",
      "Agent 0 : Predicted Values and True Return : \n",
      " [-199.0, -154.0, -165.0, -94.0, -79.0, -112.0, -90.0, -83.0, -128.0, -64.0] \n",
      " [-1280.0, -980.0, -1059.0, -1166.0, -210.0, -1084.0, -978.0, -1442.0, -1232.0, -1475.0]\n",
      "Agent 0 : Logits on those states : \n",
      " [[-0.24, -0.2], [0.25, 0.58], [-0.32, 0.2], [0.14, 0.22], [0.28, 0.05], [0.11, -0.06], [0.21, 0.13], [0.08, 0.1], [-0.07, -0.03], [0.06, -0.04]]\n",
      "Agent 0 : Logits on the 0 state : \n",
      " [-0.  0.]\n",
      "Failed load attempt 1/5. Re-attempting.\n",
      "Failed load attempt 2/5. Re-attempting.\n",
      "Failed load attempt 3/5. Re-attempting.\n",
      "Failed load attempt 4/5. Re-attempting.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Failed 5th loading attempt. Please restart program. TERMINATING NOW.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mcom_error\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\COMServer.py\u001b[0m in \u001b[0;36mCOMServerReload\u001b[1;34m(Vissim, model_name, vissim_working_directory, simulation_length, timesteps_per_second, delete_results)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mVissim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoadNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36mLoadNet\u001b[1;34m(self, NetPath, Additive)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36m_ApplyTypes_\u001b[1;34m(self, dispid, wFlags, retType, argTypes, user, resultCLSID, *args)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_ApplyTypes_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdispid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwFlags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margTypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresultCLSID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_oleobj_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvokeTypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLCID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwFlags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margTypes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_good_object_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresultCLSID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mcom_error\u001b[0m: (-2147023174, 'Le serveur RPC nest pas disponible.', None, None)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-bc44a88d308e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;33m!=\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mrunflag\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m                 Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n\u001b[1;32m---> 91\u001b[1;33m                                                       simulation_length, timesteps_per_second, delete_results = True)\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MLforFlowOptimisationOrigine\\Vissim\\COMServer.py\u001b[0m in \u001b[0;36mCOMServerReload\u001b[1;34m(Vissim, model_name, vissim_working_directory, simulation_length, timesteps_per_second, delete_results)\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Failed load attempt \"\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m\"/5. Re-attempting.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Failed 5th loading attempt. Please restart program. TERMINATING NOW.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m                 \u001b[0mquit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Failed 5th loading attempt. Please restart program. TERMINATING NOW."
     ]
    }
   ],
   "source": [
    "# Have to find a way to reduce entropy over time entropy = exploration\n",
    "\n",
    "## Converging network\n",
    "# - reward queue, state queue\n",
    "# converging with updates every steps and entropy = 0.00001 and 1 core layer of 42\n",
    "# converging well with updates every steps and entropy = 0.00001 and no core\n",
    "\n",
    "# - reward queue state queues + sig\n",
    "# converging well with updates every steps and entropy = 0.00001 and no core\n",
    "\n",
    "# 64 is a good number\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize storage\n",
    "    reward_storage = []\n",
    "    best_agent_weights = []\n",
    "    best_agent_memory = []\n",
    "    reward_plot = np.zeros([episodes,])\n",
    "    loss_plot = np.zeros([episodes,])\n",
    "\n",
    "    # Initialize simulation\n",
    "    Vissim, Simulation, Network, cache_flag = COMServerDispatch(model_name, vissim_working_directory,\\\n",
    "                                                                simulation_length, timesteps_per_second,\\\n",
    "                                                                delete_results = True, verbose = True)\n",
    "    SF.Select_Vissim_Mode(Vissim,mode)\n",
    "    \n",
    "    runflag = True\n",
    "    # Setting Random Seed\n",
    "    Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "    print ('Random seed set in simulator. Random Seed = '+str(Random_Seed))\n",
    "\n",
    "    # Deploy Network Parser (crawl network)\n",
    "    npa = NetworkParser(Vissim)\n",
    "    print('NetworkParser has succesfully crawled the model network.')\n",
    "    \n",
    "    # Initialize agents\n",
    "    if agent_type in ['AC'] :\n",
    "        Agents = [ACAgent(state_size, action_size, ID, state_type, npa, n_step_size, gamma, alpha, entropy, value, Vissim) for ID in npa.signal_controllers_ids] \n",
    "        for agent in Agents:\n",
    "            # to initialise the computational graph ot the model (I am sure there is a better way to to this)\n",
    "            agent.test()\n",
    "        agents_deployed = True\n",
    "    else:\n",
    "        print(\"Incorrect Agent Class selected. Deployment could not be completed.\")\n",
    "        quit()\n",
    "    if agents_deployed:\n",
    "        print(\"Deployed {} agent(s) of the Class {}.\".format(len(Agents), agent_type))\n",
    "    \n",
    "    ## EXECUTION OF A DEMONSTRATION RUN (slow, choice of best available agent)\n",
    "    if mode == \"demo\" or mode == \"populate\" or mode == \"debug\" or mode == \"test\":\n",
    "        # If mode or debug, set slow simulation\n",
    "        if mode == \"demo\" or mode ==\"debug\":\n",
    "            timesteps_per_second = 10\n",
    "            Vissim.Simulation.SetAttValue('SimRes', timesteps_per_second)\n",
    "            \n",
    "        # If memory population or test mode, set quick simulation\n",
    "        elif mode == \"populate\" or mode == \"test\":\n",
    "            SF.Set_Quickmode(Vissim, timesteps_per_second)\n",
    "            \n",
    "        # If on a test or a demo, load the best available agent and set exploration to zero\n",
    "        if mode == \"demo\" or mode == \"test\":\n",
    "            Agents , reward_storage = SF.load_agents(vissim_working_directory, model_name, Agents,\\\n",
    "                                    Session_ID, best = best)\n",
    "            for agent in Agents:\n",
    "                agent.epsilon = 0\n",
    "                \n",
    "    # Run the episode\n",
    "        if mode == \"demo\" or mode == \"debug\":\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length,\\\n",
    "                                      timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                                      demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "        elif mode == \"test\":\n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length,\\\n",
    "                                      timesteps_per_second, seconds_per_green, seconds_per_yellow,\\\n",
    "                                      demand_list, demand_change_timesteps, mode, PER_activated)\n",
    "        \n",
    "        \n",
    "        Vissim = None\n",
    "    \n",
    "    ## EXECUTION OF THE NORMAL TRAINING LOOP\n",
    "    elif mode == \"training\" or mode == \"retraining\":\n",
    "        print(\"Training\")\n",
    "        \n",
    "            \n",
    "        # Iterations of the simulation\n",
    "        for episode in log_progress(range(episodes), every=1):\n",
    "        \n",
    "            # Reload map if it has already been run (previous episode or prepopulation)\n",
    "            if episode !=0 or runflag == True:\n",
    "                Simulation, Network = COMServerReload(Vissim, model_name, vissim_working_directory,\\\n",
    "                                                      simulation_length, timesteps_per_second, delete_results = True)\n",
    "\n",
    "                \n",
    "\n",
    "            # Change the random seed\n",
    "            Random_Seed += 1\n",
    "            Vissim.Simulation.SetAttValue('RandSeed', Random_Seed)\n",
    "        \n",
    "            # Run Episode at maximum speed\n",
    "            \n",
    "            SF.Select_Vissim_Mode(Vissim, mode)\n",
    "            \n",
    "            SF.run_simulation_episode(Agents, Vissim, state_type, reward_type, state_size, simulation_length, timesteps_per_second,\\\n",
    "                                      seconds_per_green, seconds_per_yellow, demand_list, demand_change_timesteps, mode,\\\n",
    "                                      PER_activated,Surtrac = Surtrac)\n",
    "        \n",
    "            # Calculate episode average reward\n",
    "            reward_storage, average_reward = SF.average_reward(reward_storage, Agents, episode, episodes)\n",
    "            best_agent_weights, best_agent_memory = SF.best_agent(reward_storage, average_reward,\\\n",
    "                                                                  best_agent_weights, best_agent_memory,\\\n",
    "                                                                  vissim_working_directory, model_name, Agents, Session_ID)\n",
    "            \n",
    "            for index, agent in enumerate(Agents):\n",
    "                predicted_values, true_values, logit0, logits = agent.value_check(horizon, n_sample)\n",
    "                print (\"Agent {} : Predicted Values and True Return : \\n {} \\n {}\" .format(index, predicted_values, true_values))\n",
    "                print (\"Agent {} : Logits on those states : \\n {}\" .format(index, logits))\n",
    "                print (\"Agent {} : Logits on the 0 state : \\n {}\" .format(index, logit0))\n",
    "               \n",
    "        \n",
    "            \n",
    "            # Security save for long trainings\n",
    "            if SaveResultsAgent:\n",
    "                if (episode+1)%partial_save_at == 0:\n",
    "                    SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "                    print('Saved Partial results at the end of episode {}.'.format(episode+1))\n",
    "            \n",
    "            # line to reduce the entropy of the actor_critic.\n",
    "            if reduce_entropy:\n",
    "                pass\n",
    "            \n",
    "        #Saving agents memory, weights and optimizer\n",
    "        if SaveResultsAgent:\n",
    "            SF.save_agents(vissim_working_directory, model_name, Agents, Session_ID, reward_storage)\n",
    "            print(\"Model, architecture, weights, optimizer, memory and training results succesfully saved.\\\n",
    "            Succesfully Terminated.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"ERROR: Mode selected not recognized. TERMINATING.\")\n",
    "    # Close Vissim\n",
    "    Vissim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training progress\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(reward_storage)+1)\n",
    "fit = np.polyfit(x_series,reward_storage,1)\n",
    "fit_fn = np.poly1d(fit) \n",
    "plt.plot(x_series,reward_storage, '-b', x_series, fit_fn(x_series), '--r')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average agent reward in episode')\n",
    "plt.title('Training evolution and trend')\n",
    "plt.gca().legend(('Episode Reward','Linear Trend'))\n",
    "plt.show()\n",
    "\n",
    "# Plotting training loss\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].loss)+1)\n",
    "plt.plot(x_series,Agents[0].loss, '-b')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.gca().legend(('Loss'))\n",
    "plt.show()\n",
    "\n",
    "print(reward_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting test progress:\n",
    "West_queue = list()\n",
    "South_queue= list()\n",
    "East_queue = list()\n",
    "North_queue= list()\n",
    "# Queue Lengths\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].queues_over_time)+1)\n",
    "for i in range(len(Agents[0].queues_over_time)):\n",
    "    West_queue.append(Agents[0].queues_over_time[i][0])\n",
    "    South_queue.append(Agents[0].queues_over_time[i][1])\n",
    "    East_queue.append(Agents[0].queues_over_time[i][2])\n",
    "    North_queue.append(Agents[0].queues_over_time[i][3])\n",
    "plt.plot(x_series, West_queue, '-b',\\\n",
    "         x_series, South_queue, '-r',\\\n",
    "         x_series, East_queue, '-g',\\\n",
    "         x_series, North_queue, '-y')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Queue Length')\n",
    "plt.title('Training evolution and trend')\n",
    "plt.gca().legend(('West Queue','South Queue', 'East Queue', 'North Queue'))\n",
    "plt.show()\n",
    "\n",
    "# Accumulated delay over time\n",
    "plt.figure(figsize=(8,4.5))\n",
    "x_series = range(1,len(Agents[0].accumulated_delay)+1)\n",
    "plt.plot(x_series,Agents[0].accumulated_delay, '-b')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Accumulated global Delay')\n",
    "plt.title('Global Delay')\n",
    "plt.gca().legend('GLlobal accumulated delay')\n",
    "plt.show()\n",
    "\n",
    "average_queue_length = np.mean(Agents[0].queues_over_time)\n",
    "print(\"Average queue size is {}\".format(np.round(average_queue_length,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
